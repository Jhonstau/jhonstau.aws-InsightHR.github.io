[{"uri":"https://thienluhoan.github.io/workshop-template/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Đặng Nguyễn Minh Duy\nPhone Number: 0985515251\nEmail: dangnguyenminhduy11b08@gmail.com\nUniversity: FPT University of Ho Chi Minh City\nMajor: Artificial Intelligence\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 09/08/2025 to 11/30/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":"InsightHR Platform Overview InsightHR is a comprehensive serverless HR automation platform that demonstrates modern cloud-native application development on AWS. This workshop will guide you through building a production-ready application from scratch.\nProject Scope The InsightHR platform provides:\nEmployee Management System: Complete CRUD operations for employee records with advanced filtering by department, position, and status Performance Tracking: Quarterly performance scores with automatic calculation based on KPIs, completed tasks, and 360-degree feedback Attendance Management: Real-time check-in/check-out system with historical tracking and status monitoring AI-Powered Chatbot: Natural language query interface using AWS Bedrock (Claude 3 Haiku) for intelligent data insights Analytics Dashboard: Interactive visualizations with charts, tables, and export capabilities Role-Based Access Control: Three-tier access system (Admin, Manager, Employee) with appropriate data filtering Secure Authentication: Email/password and Google OAuth integration via AWS Cognito Architecture Overview ┌─────────────────────────────────────────────────────────────────┐ │ User Browser │ └────────────────────────┬────────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────────┐ │ CloudFront CDN │ │ Custom Domain: insight-hr.io.vn │ └────────────────────────┬────────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────────┐ │ S3 Static Website │ │ React SPA (Vite + TypeScript) │ └─────────────────────────────────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────────┐ │ API Gateway (REST) │ │ Cognito Authorizer │ └────────────────────────┬────────────────────────────────────────┘ │ ┌───────────────┼───────────────┐ ▼ ▼ ▼ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ │ Lambda │ │ Lambda │ │ Lambda │ │ Auth │ │ Employees │ │ Chatbot │ └──────┬───────┘ └──────┬───────┘ └──────┬───────┘ │ │ │ ▼ ▼ ▼ ┌─────────────────────────────────────────────────────────────────┐ │ DynamoDB │ │ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐ │ │ │ Users │ │Employees │ │ Scores │ │Attendance│ │ │ └──────────┘ └──────────┘ └──────────┘ └──────────┘ │ └─────────────────────────────────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────────┐ │ AWS Bedrock │ │ (Claude 3 Haiku Model) │ └─────────────────────────────────────────────────────────────────┘ Technology Stack Frontend:\nReact 18 with TypeScript Vite 7.2 (build tool) Tailwind CSS 3.4 (Frutiger Aero theme) Zustand 5.0 (state management) React Hook Form + Zod (form validation) Recharts 3.4 (data visualization) Backend:\nPython 3.11 (Lambda runtime) AWS Lambda (serverless compute) AWS API Gateway (REST API) AWS DynamoDB (NoSQL database) AWS Cognito (authentication) AWS Bedrock (AI/ML) Infrastructure:\nAWS S3 (static hosting) AWS CloudFront (CDN) AWS Route53 (DNS) AWS CloudWatch (monitoring) AWS IAM (security) Key Features 1. Fully Serverless Architecture No EC2 instances to manage Automatic scaling based on demand Pay-per-use pricing model High availability built-in 2. Modern Development Stack TypeScript for type safety React for responsive UI Python for backend logic Infrastructure as Code principles 3. Production-Ready Custom domain with SSL CloudWatch monitoring Synthetic canaries for testing Role-based access control 4. Cost-Effective DynamoDB on-demand pricing Lambda free tier eligible Minimal monthly costs (~$2-5) No idle resource charges Workshop Structure This workshop is divided into 11 modules:\nWorkshop Overview (Current) - Understanding the project scope Prerequisites - Setting up your environment Project Architecture - Deep dive into system design Setup AWS Environment - Configuring AWS account and credentials Database Setup - Creating and populating DynamoDB tables Authentication Service - Implementing Cognito and auth Lambda functions Backend Services - Building employee, performance, and chatbot APIs Frontend Development - Creating the React application Deployment - Deploying to S3 and CloudFront Testing \u0026amp; Monitoring - Setting up CloudWatch and canaries Cleanup - Removing resources to avoid charges Learning Objectives By the end of this workshop, you will be able to:\nDesign and implement serverless architectures on AWS Build RESTful APIs using Lambda and API Gateway Model data effectively in DynamoDB Implement authentication with AWS Cognito Integrate AI capabilities using AWS Bedrock Deploy static websites with S3 and CloudFront Monitor applications with CloudWatch Apply security best practices with IAM Optimize costs for serverless applications Prerequisites Check Before proceeding, ensure you have:\n✅ AWS Account with admin access ✅ AWS CLI installed and configured ✅ Node.js 18+ and npm installed ✅ Python 3.11+ installed ✅ Basic understanding of React and TypeScript ✅ Familiarity with REST APIs ✅ Text editor or IDE (VS Code recommended) Estimated Costs Running this workshop will incur minimal costs:\nService Estimated Cost DynamoDB $0.50/month Lambda Free tier S3 + CloudFront $1-2/month API Gateway $0.10/month Bedrock $0.0004/query Total $2-5/month Remember to complete the cleanup module at the end to avoid ongoing charges.\nNext Steps Ready to begin? Let\u0026rsquo;s move to the Prerequisites section to set up your development environment.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.1-event1/","title":"AWS First Cloud Journey Community Day","tags":[],"description":"","content":"Event Objectives Cultivate and strengthen the AWS community network among technical professionals. Present tangible examples of Generative AI being used in major sectors: Enterprise, Financial Services, and Education. Investigate sophisticated cloud architectures, including Retrieval-Augmented Generation (RAG) and coordinated Multi-Agent Systems. Highlight the deployment of highly scalable AI solutions using serverless infrastructure patterns. Speakers Mr. Nguyen Gia Hung – Head of Solutions Architect, AWS Mr. Pham Tien Thuan Phat, Mr. Le Minh Nghia, Mr. Tran Doan Cong Ly – Enterprise Software Mr. Dinh Le Hoang Anh, Mr. Nguyen Tai Minh Huy – Academic Sector Mr. Kiet Lam, Ms. Nguyen Ngoc Quynh Mai – Banking / Internal IT Ms. Le Pham Ngoc Uyen, Ms. Phan Thi Thanh Thao, Mr. Ho Dien Dang Khoa, Mr. Nguyen Quang Nhat Linh – Banking / Process Automation Mr. Viet Ly – AWS Partner / Cloud \u0026amp; AI Key Highlights 1. Fostering Community and Vision Inauguration: The day was opened by Mr. Nguyen Gia Hung, who outlined the strategic direction for the AWS community and emphasized the importance of sharing practical, real-world cloud expertise. 2. Enterprise Solutions for Contextual AI Context-Aware Chatbots: A presentation detailed how context is effectively managed using the Model Context Protocol (MCP) on AWS. The speakers demonstrated building enterprise-level chatbots capable of processing and retaining complex contextual information, sharing key learning points from their implementation experience. 3. Applied GenAI Case Studies Personalized Recommendations: An academic team showcased a system that provides personalized Kitchen Recipe Recommendations using GenAI. The session elaborated on the specific AWS workflow utilized to customize suggestions based on user input. Scalable Knowledge Bot: A deep-dive focused on creating an internal FAQ and knowledge retrieval bot leveraging a Retrieval-Augmented Generation (RAG) architecture. A critical takeaway was the use of a fully serverless implementation to ensure both cost efficiency and horizontal scalability. 4. Advanced Automation in the Banking Sector Workflow Automation with Agents: A highlighted session demonstrated the practical application of GenAI Multi-Agent Systems to automate intricate banking processes. Case studies illustrated how a team of specialized agents can collaborate to handle multi-step financial tasks using AWS serverless workflows. 5. Integration Tools and Control GenAI Orchestration: The discussion explored deploying GenAI systems in both Research \u0026amp; Development and Production environments. The focus was on workflow orchestration tools, featuring a live demonstration of a multi-agent system managed on AWS. Key Takeaways Architectural Evolution Shift to Agentic Systems: The trend is clearly moving past simple question-answering bots toward highly coordinated Multi-Agent Systems that are capable of planning and executing sophisticated workflows, particularly crucial for regulated industries. Serverless Mandate: The successful deployment of modern RAG and Agent systems strongly advocates for Serverless AWS infrastructure as the standard for achieving optimized cost, elasticity, and operational agility. Practical Deployment Lessons Context is Paramount: For enterprise success, techniques like RAG or MCP are non-negotiable for guaranteeing the AI\u0026rsquo;s relevance and accuracy in dealing with proprietary information. Managing Complexity: The need for specialized orchestration tools is becoming apparent to effectively control and monitor the complexity inherent in multi-step AI workflows. Applying to Work Evaluate Agent Strategy: I will investigate implementing a multi-agent approach for specific complex internal automation requirements, moving beyond relying on a single large language model. Prototype Serverless RAG: I plan to review our internal knowledge bases and develop a proof-of-concept for a Serverless RAG solution to enhance the speed and accuracy of information retrieval. Research MCP: I will conduct research into the Model Context Protocol (MCP) to determine its potential for improving context retention in our existing conversational AI applications. Event Experience The event at Bitexco Tower was a highly engaging and technically rich morning session.\nNetworking Success: The dedicated \u0026ldquo;Welcome Coffee\u0026rdquo; and break sessions provided invaluable opportunities to network directly with AWS Solution Architects and other industry experts. Tangible Learning: Watching live demonstrations of complex systems, such as Multi-Agent automation in banking, was instrumental in translating the theoretical principles of GenAI into practical, deployable product concepts. Some event photos "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Implementing granular failover in multi-Region Amazon EKS by Dumlu Timuralp and Pratik R. Mankad | on 18 SEP 2025 | in Amazon Elastic Kubernetes Service, Amazon Route 53, Technical How-to | Permalink | Share\nEnterprises across industries operate tens of millions of Amazon Elastic Kubernetes Service (Amazon EKS) clusters annually, supporting use cases from web/mobile applications to data processing, machine learning (ML), generative AI, gaming, and Internet of Things (IoT). As organizations increasingly adopt multi-tenant platform models where multiple application teams share an EKS cluster, they need more granular control over application availability in their multi-Region architectures—particularly for serving global users and meeting strict regulatory requirements.\nAlthough multi-tenant models optimize resource usage and reduce operational overhead, they present challenges when applications need individualized Recovery Point Objective (RPO) and Recovery Time Objective (RTO) targets across multiple AWS Regions. Traditional approaches force uniform failover policies across all applications where a single application’s failure creates an unnecessary compromise between operational efficiency and application-specific resilience.\nIn this post, we demonstrate how to configure Amazon Route 53 to enable unique failover behavior for each application within your multi-tenant Amazon EKS environment across AWS Regions, which allows you to maintain the cost benefits of shared infrastructure while meeting diverse availability requirements.\nTypical architecture Before diving into the solution, we demonstrate how a typical multi-Region multi-tenant Amazon EKS architecture is structured, and the key components that enable Regional traffic routing. Figure 1 consists of the following key components:\nRoute 53 Routes traffic to the Application Load Balancer (ALB) in the respective Region Makes sure of high availability and resiliency during failure events AWS Load Balancer Controller Exposes applications running on the EKS cluster through ALB Uses IP target type to register application Pods to the ALB The Route 53 configuration details are as follows:\nRegional Routing Each AWS Region uses a Route 53 alias record to route traffic to its Region-specific ALB. Example: app1.example.com points to ALB in Region 1 Example: app2.example.com points to ALB in Region 2 Health monitoring “Evaluate target health” feature in alias records: Continuously monitor ALB health Automatically remove unhealthy ALBs from DNS responses Request routing Provide routing policies for active-active architecture Figure 1: Multi-Region, multi-tenant Amazon EKS platform architecture\nThe all-or-nothing health check problem The Route 53 Evaluate target health attribute in alias records continuously monitors the health of the specified ALB target. An ALB is considered healthy only if each of its configured target groups contains at least one healthy target. If any target group becomes unhealthy, then Route 53 marks the entire ALB as unhealthy and removes it from DNS responses, redirecting users to the remaining healthy Region(s).\nThis reflects the cascading failure effect in multi-tenant environments:\nWhen one application fails, Route 53 marks the entire ALB as unhealthy All traffic redirects to other AWS Region(s), even for healthy applications Healthy applications unnecessarily experience increased latency Consider this example: app1.example.com fails in Region 1 app2.example.com remains healthy in Region 1 Route 53 redirects all traffic to Region 2 Users of healthy app2 experience unnecessary latency Healthy applications are impacted by unrelated service failures This behavior forces users to choose between two sub-optimal options:\nAccept complete Regional failure when any single microservice fails Deploy separate ALBs for each microservice, thus increasing both operational complexity and costs In the following sections, we present a solution that addresses these architectural challenges and provides a more resilient multi-Region deployment model.\nSolution overview To address the challenge, we propose a solution that provides granular failover control at the application level. Figure 2 shows the solution architecture. The solution consists of two key configurations:\nApplication-specific health checks\nConfigure dedicated Route 53 health checks for each application in each AWS Region Associate each health check with the respective Route 53 alias record Set Evaluate Target Health to No in alias records Granular failover control\nRoute 53 monitors application health independently During failures, affected application’s traffic is redirected Healthy applications continue serving traffic from their original AWS Region. Figure 2: Solution architecture\nThis architecture provides the following key benefits:\nEnhanced control and monitoring: Provides granular monitoring at the application level (target group) Eliminates the limitation of monitoring only at the ALB level Intelligent failover management: Selective failover for individual applications Protects healthy applications when a single service fails Maintains optimal routing for unaffected services Organizations can do the following: Maintain multi-tenant cost benefits Achieve application-specific resilience Meet diverse availability requirements Avoid unnecessary cross-Region routing\nPrerequisites he following prerequisites are necessary to complete the solution:\nAn AWS account An existing public hosted zone in Route 53, or you can register a new domain with Route 53 or use it for an existing domain AWS Command Line Interface (AWS CLI) eksctl kubectl kubectx (optional) AWS CloudShell (optional)\nWalkthrough The following steps walk you through this solution.\nConfigure environment variables Replace the values in the following example with your own values and create the variables.\nexport ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) export Region1=eu-west-1 export Region2=eu-central-1 export ALBName=demoalb export Region1ClusterName=cluster1 export Region2ClusterName=cluster2 export app1hostname=app1.example.com export app2hostname=app2.example.com export HostedZoneName=example.com export HostedZoneId=$(aws route53 list-hosted-zones-by-name --dns-name $HostedZoneName --query \u0026#39;HostedZones[0].Id\u0026#39; --output text | cut -d\u0026#39;/\u0026#39; -f3) Create EKS clusters in Region 1 and Region 2 Create an EKS cluster in Region 1 eksctl create cluster --name=$Region1ClusterName --enable-auto-mode --region=$Region1 Wait for the cluster state to become Active. This takes approximately 12 minutes. aws eks describe-cluster --name $Region1ClusterName --region $Region1 --output json --query \u0026#39;cluster.status\u0026#39; Create an EKS cluster in Region 2. eksctl create cluster --name=$Region2ClusterName --enable-auto-mode --region=$Region2 Wait for the cluster state to become Active. This takes approximately 12 minutes. aws eks describe-cluster --name $Region2ClusterName --region $Region2 --output json --query \u0026#39;cluster.status\u0026#39; eksctl updates your kubeconfig file as it completes the creation of each cluster. Use kubectl to list the nodes in each cluster. Use kubectx or manually switch between the clusters. kubectl get nodes Create IngressClass configuration Run the following command to create the IngressClassParams and IngressClass on the cluster in Region 1. curl -s https://raw.githubusercontent.com/aws-samples/multi-region-ingress/refs/heads/main/ingressclassconfiguration.yaml | kubectl apply -f - Verify that the resources are created. kubectl get ingressclass,ingressclassparams Repeat Steps 1 and 2 for the cluster in Region 2. Deploy app1 and app2 on the EKS clusters We use a manifest to deploy the applications on the cluster in Region 1. The manifest creates the following: a deployment, with a single replica, for each application a service for each deployment an ingress that forwards http requests to the services curl -s https://github.com/aws-samples/multi-region-ingress/blob/main/multitenant.yaml | envsubst | kubectl apply -f - Verify that the resources are created. You should see two deployments, two services, and an ingress in the output. Confirm that the pod status is running kubectl get all,ingress Repeat Steps 1 and 2 for the cluster in Region 2. The applications that we deployed are simple web servers that display an HTML page that includes the application name, pod name, AWS Region, and AWS Availability Zone (AZ). We show this as a sample in Figure 3 and Figure 4. We test the access when we define the Route 53 records in the next few steps. Figure 3: App1 web page\nFigure 4: App2 web page\nConfigure Route 53 health checks for app1 and app2 Define environment variables for the ALBs created previously. export Region1ALB=$(aws elbv2 describe-load-balancers --region $Region1 --query \u0026#34;LoadBalancers[?contains(LoadBalancerName, \u0026#39;$ALBName\u0026#39;)].DNSName | [0]\u0026#34; --output text) export Region2ALB=$(aws elbv2 describe-load-balancers --region $Region2 --query \u0026#34;LoadBalancers[?contains(LoadBalancerName, \u0026#39;$ALBName\u0026#39;)].DNSName | [0]\u0026#34; --output text) Configure the health checks for app1 in Region 1 and Region 2. aws route53 create-health-check --caller-reference \u0026#34;Region1App1-$(date +%s)\u0026#34; \\ --health-check-config \u0026#34;{\\\u0026#34;Port\\\u0026#34;:80,\\\u0026#34;Type\\\u0026#34;:\\\u0026#34;HTTP\\\u0026#34;,\\\u0026#34;ResourcePath\\\u0026#34;:\\\u0026#34;/app1health\\\u0026#34;,\\\u0026#34;FullyQualifiedDomainName\\\u0026#34;:\\\u0026#34;$Region1ALB\\\u0026#34;,\\\u0026#34;RequestInterval\\\u0026#34;:30,\\\u0026#34;FailureThreshold\\\u0026#34;:3,\\\u0026#34;MeasureLatency\\\u0026#34;:true,\\\u0026#34;Inverted\\\u0026#34;:false,\\\u0026#34;Disabled\\\u0026#34;:false,\\\u0026#34;EnableSNI\\\u0026#34;:false}\u0026#34; aws route53 create-health-check --caller-reference \u0026#34;Region2App1-$(date +%s)\u0026#34; \\ --health-check-config \u0026#34;{\\\u0026#34;Port\\\u0026#34;:80,\\\u0026#34;Type\\\u0026#34;:\\\u0026#34;HTTP\\\u0026#34;,\\\u0026#34;ResourcePath\\\u0026#34;:\\\u0026#34;/app1health\\\u0026#34;,\\\u0026#34;FullyQualifiedDomainName\\\u0026#34;:\\\u0026#34;$Region2ALB\\\u0026#34;,\\\u0026#34;RequestInterval\\\u0026#34;:30,\\\u0026#34;FailureThreshold\\\u0026#34;:3,\\\u0026#34;MeasureLatency\\\u0026#34;:true,\\\u0026#34;Inverted\\\u0026#34;:false,\\\u0026#34;Disabled\\\u0026#34;:false,\\\u0026#34;EnableSNI\\\u0026#34;:false}\u0026#34; Configure the health checks for app2 in Region 1 and Region 2. aws route53 create-health-check --caller-reference \u0026#34;Region1App1-$(date +%s)\u0026#34; \\ --health-check-config \u0026#34;{\\\u0026#34;Port\\\u0026#34;:80,\\\u0026#34;Type\\\u0026#34;:\\\u0026#34;HTTP\\\u0026#34;,\\\u0026#34;ResourcePath\\\u0026#34;:\\\u0026#34;/app2health\\\u0026#34;,\\\u0026#34;FullyQualifiedDomainName\\\u0026#34;:\\\u0026#34;$Region1ALB\\\u0026#34;,\\\u0026#34;RequestInterval\\\u0026#34;:30,\\\u0026#34;FailureThreshold\\\u0026#34;:3,\\\u0026#34;MeasureLatency\\\u0026#34;:true,\\\u0026#34;Inverted\\\u0026#34;:false,\\\u0026#34;Disabled\\\u0026#34;:false,\\\u0026#34;EnableSNI\\\u0026#34;:false}\u0026#34; aws route53 create-health-check --caller-reference \u0026#34;Region2App1-$(date +%s)\u0026#34; \\ --health-check-config \u0026#34;{\\\u0026#34;Port\\\u0026#34;:80,\\\u0026#34;Type\\\u0026#34;:\\\u0026#34;HTTP\\\u0026#34;,\\\u0026#34;ResourcePath\\\u0026#34;:\\\u0026#34;/app2health\\\u0026#34;,\\\u0026#34;FullyQualifiedDomainName\\\u0026#34;:\\\u0026#34;$Region2ALB\\\u0026#34;,\\\u0026#34;RequestInterval\\\u0026#34;:30,\\\u0026#34;FailureThreshold\\\u0026#34;:3,\\\u0026#34;MeasureLatency\\\u0026#34;:true,\\\u0026#34;Inverted\\\u0026#34;:false,\\\u0026#34;Disabled\\\u0026#34;:false,\\\u0026#34;EnableSNI\\\u0026#34;:false}\u0026#34; Configure Route 53 alias records for app1 and app2 Define environment variables for the health checks created earlier. export HealthCheckRegion1App1Id=$(aws route53 list-health-checks \\ --query \u0026#34;HealthChecks[?HealthCheckConfig.FullyQualifiedDomainName==\u0026#39;$Region1ALB\u0026#39; \u0026amp;\u0026amp; HealthCheckConfig.ResourcePath==\u0026#39;/app1health\u0026#39;] | [0].Id\u0026#34; \\ --output text) export HealthCheckRegion1App2Id=$(aws route53 list-health-checks \\ --query \u0026#34;HealthChecks[?HealthCheckConfig.FullyQualifiedDomainName==\u0026#39;$Region1ALB\u0026#39; \u0026amp;\u0026amp; HealthCheckConfig.ResourcePath==\u0026#39;/app2health\u0026#39;] | [0].Id\u0026#34; \\ --output text) export HealthCheckRegion2App1Id=$(aws route53 list-health-checks \\ --query \u0026#34;HealthChecks[?HealthCheckConfig.FullyQualifiedDomainName==\u0026#39;$Region2ALB\u0026#39; \u0026amp;\u0026amp; HealthCheckConfig.ResourcePath==\u0026#39;/app1health\u0026#39;] | [0].Id\u0026#34; \\ --output text) export HealthCheckRegion2App2Id=$(aws route53 list-health-checks \\ --query \u0026#34;HealthChecks[?HealthCheckConfig.FullyQualifiedDomainName==\u0026#39;$Region2ALB\u0026#39; \u0026amp;\u0026amp; HealthCheckConfig.ResourcePath==\u0026#39;/app2health\u0026#39;] | [0].Id\u0026#34; \\ --output text) Define environment variables for the ALBs’ CanonicalHostedZoneId. We need to use it when creating the alias records in the next steps. export Region1ALBHostedZoneId=$(aws elbv2 describe-load-balancers --region $Region1 --query \u0026#34;LoadBalancers[?DNSName==\u0026#39;$Region1ALB\u0026#39;].CanonicalHostedZoneId\u0026#34; --output text) export Region2ALBHostedZoneId=$(aws elbv2 describe-load-balancers --region $Region2 --query \u0026#34;LoadBalancers[?DNSName==\u0026#39;$Region2ALB\u0026#39;].CanonicalHostedZoneId\u0026#34; --output text) Create the alias records for app1. In this example we use latency based routing policy in each record. aws route53 change-resource-record-sets --hosted-zone-id $HostedZoneId \\ --change-batch \u0026#34;{\\\u0026#34;Changes\\\u0026#34;:[{\\\u0026#34;Action\\\u0026#34;:\\\u0026#34;CREATE\\\u0026#34;,\\\u0026#34;ResourceRecordSet\\\u0026#34;:{\\\u0026#34;Name\\\u0026#34;:\\\u0026#34;$app1hostname\\\u0026#34;,\\\u0026#34;Type\\\u0026#34;:\\\u0026#34;A\\\u0026#34;,\\\u0026#34;SetIdentifier\\\u0026#34;:\\\u0026#34;$Region1\\\u0026#34;,\\\u0026#34;Region\\\u0026#34;:\\\u0026#34;$Region1\\\u0026#34;,\\\u0026#34;HealthCheckId\\\u0026#34;:\\\u0026#34;$HealthCheckRegion1App1Id\\\u0026#34;,\\\u0026#34;AliasTarget\\\u0026#34;:{\\\u0026#34;HostedZoneId\\\u0026#34;:\\\u0026#34;$Region1ALBHostedZoneId\\\u0026#34;,\\\u0026#34;DNSName\\\u0026#34;:\\\u0026#34;$Region1ALB\\\u0026#34;,\\\u0026#34;EvaluateTargetHealth\\\u0026#34;:false}}}]}\u0026#34; aws route53 change-resource-record-sets --hosted-zone-id $HostedZoneId \\ --change-batch \u0026#34;{\\\u0026#34;Changes\\\u0026#34;:[{\\\u0026#34;Action\\\u0026#34;:\\\u0026#34;CREATE\\\u0026#34;,\\\u0026#34;ResourceRecordSet\\\u0026#34;:{\\\u0026#34;Name\\\u0026#34;:\\\u0026#34;$app1hostname\\\u0026#34;,\\\u0026#34;Type\\\u0026#34;:\\\u0026#34;A\\\u0026#34;,\\\u0026#34;SetIdentifier\\\u0026#34;:\\\u0026#34;$Region2\\\u0026#34;,\\\u0026#34;Region\\\u0026#34;:\\\u0026#34;$Region2\\\u0026#34;,\\\u0026#34;HealthCheckId\\\u0026#34;:\\\u0026#34;$HealthCheckRegion2App1Id\\\u0026#34;,\\\u0026#34;AliasTarget\\\u0026#34;:{\\\u0026#34;HostedZoneId\\\u0026#34;:\\\u0026#34;$Region2ALBHostedZoneId\\\u0026#34;,\\\u0026#34;DNSName\\\u0026#34;:\\\u0026#34;$Region2ALB\\\u0026#34;,\\\u0026#34;EvaluateTargetHealth\\\u0026#34;:false}}}]}\u0026#34; Verify that the alias records for app1 are successfully created in the Route 53 hosted zone. aws route53 list-resource-record-sets --hosted-zone-id $HostedZoneId --query \u0026#34;ResourceRecordSets[?Name==\u0026#39;$app1hostname.\u0026#39;]\u0026#34; Create the alias records for app2. In this example we use a latency based routing policy in each record. aws route53 change-resource-record-sets --hosted-zone-id $HostedZoneId \\ --change-batch \u0026#34;{\\\u0026#34;Changes\\\u0026#34;:[{\\\u0026#34;Action\\\u0026#34;:\\\u0026#34;CREATE\\\u0026#34;,\\\u0026#34;ResourceRecordSet\\\u0026#34;:{\\\u0026#34;Name\\\u0026#34;:\\\u0026#34;$app2hostname\\\u0026#34;,\\\u0026#34;Type\\\u0026#34;:\\\u0026#34;A\\\u0026#34;,\\\u0026#34;SetIdentifier\\\u0026#34;:\\\u0026#34;$Region1\\\u0026#34;,\\\u0026#34;Region\\\u0026#34;:\\\u0026#34;$Region1\\\u0026#34;,\\\u0026#34;HealthCheckId\\\u0026#34;:\\\u0026#34;$HealthCheckRegion1App2Id\\\u0026#34;,\\\u0026#34;AliasTarget\\\u0026#34;:{\\\u0026#34;HostedZoneId\\\u0026#34;:\\\u0026#34;$Region1ALBHostedZoneId\\\u0026#34;,\\\u0026#34;DNSName\\\u0026#34;:\\\u0026#34;$Region1ALB\\\u0026#34;,\\\u0026#34;EvaluateTargetHealth\\\u0026#34;:false}}}]}\u0026#34; aws route53 change-resource-record-sets --hosted-zone-id $HostedZoneId \\ --change-batch \u0026#34;{\\\u0026#34;Changes\\\u0026#34;:[{\\\u0026#34;Action\\\u0026#34;:\\\u0026#34;CREATE\\\u0026#34;,\\\u0026#34;ResourceRecordSet\\\u0026#34;:{\\\u0026#34;Name\\\u0026#34;:\\\u0026#34;$app2hostname\\\u0026#34;,\\\u0026#34;Type\\\u0026#34;:\\\u0026#34;A\\\u0026#34;,\\\u0026#34;SetIdentifier\\\u0026#34;:\\\u0026#34;$Region2\\\u0026#34;,\\\u0026#34;Region\\\u0026#34;:\\\u0026#34;$Region2\\\u0026#34;,\\\u0026#34;HealthCheckId\\\u0026#34;:\\\u0026#34;$HealthCheckRegion2App2Id\\\u0026#34;,\\\u0026#34;AliasTarget\\\u0026#34;:{\\\u0026#34;HostedZoneId\\\u0026#34;:\\\u0026#34;$Region2ALBHostedZoneId\\\u0026#34;,\\\u0026#34;DNSName\\\u0026#34;:\\\u0026#34;$Region2ALB\\\u0026#34;,\\\u0026#34;EvaluateTargetHealth\\\u0026#34;:false}}}]}\u0026#34; Verify that the alias records for app2 are successfully created in the Route 53 hosted zone. aws route53 list-resource-record-sets --hosted-zone-id $HostedZoneId --query \u0026#34;ResourceRecordSets[?Name==\u0026#39;$app2hostname.\u0026#39;]\u0026#34; It may take a while for the DNS to propagate. Therefore, you may not have immediate access to the application domain names (app1.example.com and app2.example.com).\nTest application failure With the configuration now in place, we can examine how the system responds to an individual application failure. Figure 5 illustrates the sequence of events during such a scenario. Under normal conditions, users located in Region 1 are always directed to applications in Region 1 because we created Route 53 alias records that use latency-based routing policy. Let’s assume app1 fails. In that case, users in Region 1 accessing app1 are redirected to Region 2, while users in Region 1 sending requests to app2 continue to be served by app2 in Region 1.\nFigure 5: Application failure scenario\nThe following sequence of events is shown in Figure 5.\nBoth health checks are healthy App1 workloads fail in Region 1 Route 53 health check (for app1 in Region 1) becomes unhealthy Route 53 withdraws the Region 1 ALB from the DNS responses All users are redirected to app1 in Region 2 Validate the behavior shown in Figure 5 by scaling down the deployment for app1 to zero replicas, which simulates an application failure. Use the following command:\nkubectl scale deployment demoapp1 --replicas=0 We used the default timers in the Route 53 health checks in this scenario. Therefore, it takes several minutes for Route 53 to start redirecting user requests for app1 to Region 2. You can configure more aggressive timers for your use case to provide much quicker failover.\nThe key differentiator of this approach becomes immediately apparent: the Route 53 application-specific health checks detect only app1’s failure and redirect app1 traffic exclusively to Region 2. Meanwhile, app2 continues serving users from Region 1, maintaining optimal latency for healthy applications.\nThis selective failover behavior contrasts sharply with traditional ALB-level monitoring, where a single application failure forces all applications to fail over to another Region, unnecessarily impacting performance for healthy services. Our solution makes sure that failure isolation occurs at the application level, not the infrastructure level, providing true granular control in multi-tenant environments.\nThings to know Each application consumes one rule for the application endpoint and one rule for the healthcheck endpoint on the ALB. Consider the maximum number of rules allowed per ALB when planning. When creating an alias record using Route 53 console, Evaluate target health is automatically set to Yes. When using AWS CLI or other tools, you must explicitly set it. You must configure Route 53 health checks for each application. Be mindful of the Route 53 health check pricing. Route 53 health checkers that are based on HTTPS do not validate SSL/TLS certificates. Therefore, checks do not fail due to invalid or expired certificates. Route 53 health checkers can’t monitor endpoints with IP addresses in local, private, non-routable, or multicast ranges. If your application runs in a private subnet, you can implement custom Route 53 health checks using CloudWatch alarms. A custom health check implementation using a Lambda function is demonstrated in this AWS post: Performing Route 53 health checks on private resources in a VPC with AWS Lambda and Amazon CloudWatch. The ALB HTTP keepalive duration is crucial for clients to quickly detect failures in multi-tenant environments. For more information, go to Application Load Balancer enables configuring HTTP client keepalive duration. This post is not applicable to path based application segregation on the same domain name (for example when app1 is example.com/app1 and app2 is example.com/app2). Refer to Configuring DNS Failover to understand how health checks, alias records, and failover work in various scenarios. Amazon Application Recovery Controller (ARC) is another AWS service to achieve Regional resiliency. However, ARC is out of scope for this post. For more details, consult the ARC Developer Guide. Cleaning up You continue to incur costs until deleting the infrastructure that you created for this post. Use the following commands to delete those resources.\nDelete the EKS clusters using the following commands. eksctl automatically deletes all ELB resources (target groups, ALB, etc.), which are provisioned by ALB Controller as a result of the Kubernetes ingress requests.\neksctl delete cluster --name=$Region1ClusterName --region=$Region1 eksctl delete cluster --name=$Region2ClusterName --region=$Region2 Delete the Route 53 alias records.\naws route53 change-resource-record-sets --hosted-zone-id $HostedZoneId \\ --change-batch \u0026#34;{\\\u0026#34;Changes\\\u0026#34;:[{\\\u0026#34;Action\\\u0026#34;:\\\u0026#34;DELETE\\\u0026#34;,\\\u0026#34;ResourceRecordSet\\\u0026#34;:{\\\u0026#34;Name\\\u0026#34;:\\\u0026#34;$app1hostname\\\u0026#34;,\\\u0026#34;Type\\\u0026#34;:\\\u0026#34;A\\\u0026#34;,\\\u0026#34;SetIdentifier\\\u0026#34;:\\\u0026#34;$Region1\\\u0026#34;,\\\u0026#34;Region\\\u0026#34;:\\\u0026#34;$Region1\\\u0026#34;,\\\u0026#34;HealthCheckId\\\u0026#34;:\\\u0026#34;$HealthCheckRegion1App1Id\\\u0026#34;,\\\u0026#34;AliasTarget\\\u0026#34;:{\\\u0026#34;HostedZoneId\\\u0026#34;:\\\u0026#34;$Region1ALBHostedZoneId\\\u0026#34;,\\\u0026#34;DNSName\\\u0026#34;:\\\u0026#34;$Region1ALB\\\u0026#34;,\\\u0026#34;EvaluateTargetHealth\\\u0026#34;:false}}}]}\u0026#34; aws route53 change-resource-record-sets --hosted-zone-id $HostedZoneId \\ --change-batch \u0026#34;{\\\u0026#34;Changes\\\u0026#34;:[{\\\u0026#34;Action\\\u0026#34;:\\\u0026#34;DELETE\\\u0026#34;,\\\u0026#34;ResourceRecordSet\\\u0026#34;:{\\\u0026#34;Name\\\u0026#34;:\\\u0026#34;$app2hostname\\\u0026#34;,\\\u0026#34;Type\\\u0026#34;:\\\u0026#34;A\\\u0026#34;,\\\u0026#34;SetIdentifier\\\u0026#34;:\\\u0026#34;$Region1\\\u0026#34;,\\\u0026#34;Region\\\u0026#34;:\\\u0026#34;$Region1\\\u0026#34;,\\\u0026#34;HealthCheckId\\\u0026#34;:\\\u0026#34;$HealthCheckRegion1App2Id\\\u0026#34;,\\\u0026#34;AliasTarget\\\u0026#34;:{\\\u0026#34;HostedZoneId\\\u0026#34;:\\\u0026#34;$Region1ALBHostedZoneId\\\u0026#34;,\\\u0026#34;DNSName\\\u0026#34;:\\\u0026#34;$Region1ALB\\\u0026#34;,\\\u0026#34;EvaluateTargetHealth\\\u0026#34;:false}}}]}\u0026#34; aws route53 change-resource-record-sets --hosted-zone-id $HostedZoneId \\ --change-batch \u0026#34;{\\\u0026#34;Changes\\\u0026#34;:[{\\\u0026#34;Action\\\u0026#34;:\\\u0026#34;DELETE\\\u0026#34;,\\\u0026#34;ResourceRecordSet\\\u0026#34;:{\\\u0026#34;Name\\\u0026#34;:\\\u0026#34;$app1hostname\\\u0026#34;,\\\u0026#34;Type\\\u0026#34;:\\\u0026#34;A\\\u0026#34;,\\\u0026#34;SetIdentifier\\\u0026#34;:\\\u0026#34;$Region2\\\u0026#34;,\\\u0026#34;Region\\\u0026#34;:\\\u0026#34;$Region2\\\u0026#34;,\\\u0026#34;HealthCheckId\\\u0026#34;:\\\u0026#34;$HealthCheckRegion2App1Id\\\u0026#34;,\\\u0026#34;AliasTarget\\\u0026#34;:{\\\u0026#34;HostedZoneId\\\u0026#34;:\\\u0026#34;$Region2ALBHostedZoneId\\\u0026#34;,\\\u0026#34;DNSName\\\u0026#34;:\\\u0026#34;$Region2ALB\\\u0026#34;,\\\u0026#34;EvaluateTargetHealth\\\u0026#34;:false}}}]}\u0026#34; aws route53 change-resource-record-sets --hosted-zone-id $HostedZoneId \\ --change-batch \u0026#34;{\\\u0026#34;Changes\\\u0026#34;:[{\\\u0026#34;Action\\\u0026#34;:\\\u0026#34;DELETE\\\u0026#34;,\\\u0026#34;ResourceRecordSet\\\u0026#34;:{\\\u0026#34;Name\\\u0026#34;:\\\u0026#34;$app2hostname\\\u0026#34;,\\\u0026#34;Type\\\u0026#34;:\\\u0026#34;A\\\u0026#34;,\\\u0026#34;SetIdentifier\\\u0026#34;:\\\u0026#34;$Region2\\\u0026#34;,\\\u0026#34;Region\\\u0026#34;:\\\u0026#34;$Region2\\\u0026#34;,\\\u0026#34;HealthCheckId\\\u0026#34;:\\\u0026#34;$HealthCheckRegion2App2Id\\\u0026#34;,\\\u0026#34;AliasTarget\\\u0026#34;:{\\\u0026#34;HostedZoneId\\\u0026#34;:\\\u0026#34;$Region2ALBHostedZoneId\\\u0026#34;,\\\u0026#34;DNSName\\\u0026#34;:\\\u0026#34;$Region2ALB\\\u0026#34;,\\\u0026#34;EvaluateTargetHealth\\\u0026#34;:false}}}]}\u0026#34; Delete the Route 53 health checks.\naws route53 delete-health-check --health-check-id $HealthCheckRegion1App1Id aws route53 delete-health-check --health-check-id $HealthCheckRegion1App2Id aws route53 delete-health-check --health-check-id $HealthCheckRegion2App1Id aws route53 delete-health-check --health-check-id $HealthCheckRegion2App2Id Clean up any other resources that you created as part of the prerequisites if they are no longer needed.\nConclusion In this post, we demonstrated how to overcome a critical limitation in multi-Region, multi-tenant Amazon EKS architectures: the all-or-nothing failover behavior that impacts healthy applications when a single service fails. Implementing application-specific Amazon Route 53 health checks granted surgical precision in your failover strategy, redirecting only affected applications while maintaining optimal performance for healthy services.\nThis solution allows you to maintain the cost benefits of shared infrastructure while meeting diverse availability requirements. This eliminated unnecessary cross-Region latency for healthy applications during partial failures.\nReady to enhance your multi-Region resilience further? Consider these next steps:\nExtend health checks beyond basic HTTP responses to monitor business-critical dependencies. Integrate with Amazon Application Recovery Controller (ARC) for automated disaster recovery orchestration. Start by identifying applications in your current setup that benefit most from independent failover behavior—particularly those with different SLA requirements or varying Regional user bases.\nFor more information, see the following references:\nAmazon EKS Best Practices Guide Best Practices for Amazon Route 53 Designing for high availability and resiliency in Amazon EKS applications\nAbout the authors Dumlu Timuralp Dumlu Timuralp is a Senior Solutions Architect with AWS based in the United Kingdom. In this role he provides architecture guidance on cloud migration, application modernization and cloud native patterns. He loves working with users and meet their business needs with technology.\nPratik Mankad Pratik Mankad is a Network Solutions Architect at AWS. He is passionate about network technologies and loves to innovate to help solve user problems. He enjoys architecting solutions and providing technical guidance to help users and partners achieve their technical and business objectives.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Tailor Amazon SageMaker Unified Studio project environments to your needs using custom blueprints by Aditya Challa and Khushbu Agarwal on 17 SEP 2025 in Amazon SageMaker Unified Studio, Announcements, Intermediate (200), Technical How-to |Permalink | Comments | Share\nAmazon SageMaker Unified Studio is a single data and AI development environment that brings together data preparation, analytics, machine learning (ML), and generative AI development in one place. By unifying these workflows, it saves teams from managing multiple tools and makes it straightforward for data scientists, analysts, and developers to build, train, and deploy ML models and AI applications while collaborating seamlessly.\nIn SageMaker Unified Studio, a project is a boundary where you can collaborate with other users to work on a business use case. A blueprint defines what AWS tools and services members of a project can use as they work with their data. Blueprints are defined by an administrator and are powered by AWS CloudFormation. Instead of manually piecing together project structures or workflow configurations, teams can rapidly spin up secure, compliant, and consistent analytics and AI environments. This streamlined approach significantly reduces setup time and provides standardized workspaces across the organization. Out of the box, SageMaker Unified Studio comes with several default blueprints.\nWe recently launched the custom blueprints feature in SageMaker Unified Studio. Organizations can now incorporate their specific dependencies, security controls using their own managed AWS Identity and Access Management (IAM) policies, and best practices, making it straightforward for them to align with internal standards. Because they’re defined through infrastructure as code (IaC), blueprints are straightforward to version control, share across teams, and evolve over time. This speeds up onboarding and keeps projects consistent and governed, no matter how big or distributed your data organization becomes.\nFor enterprises, this means more time focusing on insights, models, and innovation. The custom blueprints feature is designed to help teams move faster and stay consistent while maintaining their organization’s security controls and best practices. In this post, we show how to get started with custom blueprints in SageMaker Unified Studio.\nSolution overview We provide a CloudFormation template to implement a custom blueprint in SageMaker Unified Studio. The template deploys the following resources in the project environment:\nAWS Glue database Amazon Redshift Serverless namespace and workgroup AWS Lake Formation permissions for the newly created project to access the AWS Glue database Custom managed policies for AWS Glue and Amazon Redshift\nPrerequisites The post assumes you have a preexisting SageMaker Unified Studio domain. If you don’t have one, refer to Create a Amazon SageMaker Unified Studio domain – quick setup for instructions to create one.\nDefine reserved environment parameters The CloudFormation template uses parameters that are reserved to your SageMaker environment, such as datazoneEnvironmentEnvironmentId, datazoneEnvironmentProjectId, s3BucketArn, and privateSubnets. These parameters are automatically populated by SageMaker when creating the project. The parameters also help in retrieving other environment variables, such as SecurityGroupIds, as shown in the following snippets.\nThe following code illustrates defining reserved environment parameters:\n\u0026#34;Parameters\u0026#34;: { \u0026#34;datazoneEnvironmentEnvironmentId\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;EnvironmentId for which the resource will be created for.\u0026#34; }, \u0026#34;datazoneEnvironmentProjectId\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;DZ projectId for which project the resource will be created for.\u0026#34; }, \u0026#34;s3BucketArn\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Project S3 Bucket ARN\u0026#34; }, \u0026#34;privateSubnets\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Project Private Subnets\u0026#34; } } The following code illustrates using reserved environment parameters to import other necessary values: \u0026#34;SecurityGroupIds\u0026#34;: [ { \u0026#34;Fn::ImportValue\u0026#34;: { \u0026#34;Fn::Join\u0026#34;: [ \u0026#34;\u0026#34;, [ \u0026#34;securityGroup-\u0026#34;, { \u0026#34;Ref\u0026#34;: \u0026#34;datazoneEnvironmentProjectId\u0026#34; }, \u0026#34;-dev\u0026#34; ] ] } } ] Attach custom IAM policies to project role By default, SageMaker Unified Studio creates a project role and attaches several managed policies to the role. These managed policies are defined in the tooling blueprint. With custom blueprints, you can configure and attach your own IAM policies, in addition to the default policies, to the project role. To do this, include the IAM policies in your CloudFormation template and use the Export feature in the Outputs section, as shown in the following code. SageMaker Unified Studio gathers the policy information and adds it to the project role.\n\u0026#34;GlueAccessManagedPolicy\u0026#34;: { \u0026#34;Description\u0026#34;: \u0026#34;ARN of the created managed policy\u0026#34;, \u0026#34;Value\u0026#34;: { \u0026#34;Ref\u0026#34;: \u0026#34;GlueAccessManagedPolicy\u0026#34; }, \u0026#34;Export\u0026#34;: { \u0026#34;Name\u0026#34;: { \u0026#34;Fn::Sub\u0026#34;: \u0026#34;datazone-managed-policy-glue-${glueDbName}-${datazoneEnvironmentEnvironmentId}\u0026#34; } } }, \u0026#34;RedshiftAccessManagedPolicy\u0026#34;: { \u0026#34;Description\u0026#34;: \u0026#34;ARN of the created Redshift managed policy\u0026#34;, \u0026#34;Value\u0026#34;: { \u0026#34;Ref\u0026#34;: \u0026#34;RedshiftAccessManagedPolicy\u0026#34; }, \u0026#34;Export\u0026#34;: { \u0026#34;Name\u0026#34;: { \u0026#34;Fn::Sub\u0026#34;: \u0026#34;datazone-managed-policy-redshift-${redshiftWorkgroupName}-${datazoneEnvironmentEnvironmentId}\u0026#34; } } } Create custom blueprint Complete the following steps to create a custom blueprint using the CloudFormation template:\nOn the Amazon SageMaker console, open the domain where you want to create a custom blueprint. On the Blueprints tab, choose Create. Under Name and description, enter a name and optional description. Under Upload CloudFormation template, select Upload a template file and upload the provided template. Choose Next. SageMaker will automatically detect the reserved parameters defined in the template, as shown in the following screenshot. For Editable parameters, edit the Value column if necessary, and specify whether the values can be editable at the time of project creation. Choose Next. As shown in the following screenshot, the reserved parameters described earlier are not shown on this page. Select Enable blueprint. Choose the provisioning role to be used by SageMaker to provision the environment resources. Choose the domain units authorized to use the blueprint. Choose Next. Review the blueprint information and choose Create blueprint. Create project profile Complete the following steps to create a custom project profile that includes the custom blueprint created in the previous section:\nOn the SageMaker console, open your domain. On the Project profiles tab, choose Create. Enter the project profile name and optional description. Select Custom create. Choose the blueprints to be included in the project profile, including the custom blueprint you created in the previous section. Choose the account and AWS Region to be used. Choose the authorized users. Select Enable project profile on creation. Choose Create project profile. Create project Complete the following steps to create a new project that is based on the custom project profile and custom blueprint created in the previous sections:\nIn the SageMaker Unified Studio environment, choose Create project. Enter a project name and optional description. For Project profile, choose the profile created in the previous section. Choose Continue. On the Customize blueprint parameters page, review the parameters, modify as necessary, and choose Continue. Review your selections and choose Create project. SageMaker Unified Studio will create the project environments with the resources defined in your custom blueprint. It will also attach the custom IAM policies defined and add them to the project role, as shown in the following screenshot. Clean up To avoid incurring additional costs, complete the following steps:\nDelete the project you created in SageMaker Unified Studio. Delete the custom project profile and custom blueprint you created. Delete the CloudFormation template. Conclusion In this post, we discussed custom blueprints, a new option during administrator setup in SageMaker Unified Studio. We showed how to create new custom blueprints and create custom project profiles that include the newly created custom blueprints. We also demonstrated how to create projects that implement custom blueprints.\nCustom blueprints in SageMaker Unified Studio are intended to streamline and standardize data, analytics and AI workflows. By helping organizations create templated environments with preconfigured resources, security controls, and best practices, custom blueprints can reduce setup time while providing consistency and compliance across projects.\nOrganizations can now enforce their specific security standards and access controls at the project level using the ability to incorporate custom IAM policies directly into these blueprints. This granular control over permissions helps organizations create projects that adhere to corporate security policies right from inception. Custom blueprints can help you scale analytics and AI/ML operations securely, by including tooling designed to version control these templates, share them across teams, and automatically apply custom IAM policies.\nTo learn more about custom blueprints in SageMaker Unified Studio, refer to Custom blueprints.\nAbout the Authors Aditya Challa Aditya is a Senior Solutions Architect at Amazon Web Services with over a decade of experience architecting and implementing cloud-based solutions. Specializing in data, analytics, and machine learning, he has helped numerous enterprises transform their data infrastructure and build scalable AI/ML solutions on AWS. As a trusted advisor to clients across industries, Aditya is passionate about helping organizations navigate their cloud transformation journeys and unlock business value through data-driven innovation. Beyond his technical pursuits, Aditya is an avid traveler and history enthusiast who finds inspiration in engineering marvels across cultures and eras. He maintains a growth mindset and believes in continuous learning—a philosophy that drives both his professional development and his approach to helping customers achieve their technology goals through AWS.\nKhushbu Agarwal Khushbu is a Senior Product Manager at AWS. She is focused on improving the customer onboarding and platform capabilities within Amazon SageMaker Unified Studio and making it the best-in-class choice for AWS analytics, generative AI, and ML services.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Enhance TLS inspection with SNI session holding in AWS Network Firewall by Amit Gaur, Olu Adeleke, Srivalsan Mannoor Sudhagar, and Vikram Saurabh on 17 SEP 2025 in Advanced (300), AWS Network Firewall, Security, Identity, \u0026amp; Compliance, Technical How-to | Permalink Comments | Share\nAWS Network Firewall is a managed firewall service that filters and controls network traffic in Amazon Virtual Private Cloud (Amazon VPC). Unlike traditional network controls such as security groups or network access control lists (NACLs), Network Firewall can inspect and make decisions based on information from higher layers of the OSI model, including the Transport through Application layers. Furthermore, you can use the TLS inspection capability of Network Firewall to create firewall rules that match the content of encrypted TLS traffic. Network Firewall decrypts the traffic using your configured certificate and matches the decrypted payload against the rules in the firewall policy.\nThis post introduces Server Name Indication (SNI) session holding, which enhances TLS inspection by stopping TCP or TLS establishment packets from reaching the destination server until TLS inspection rules for SNI have been applied. When SNI is enabled, Network Firewall will not initiate an outbound TCP connection to the target until it has received the client hello and matched its domain information sent through SNI against firewall rules. The TCP session between the firewall and the upstream server is only initiated after the firewall validates traffic to that domain. This offers you additional security controls on outbound traffic with minimal latency and performance overheads, helping protect against malicious targets.\nNetwork Firewall TLS inspection prior to SNI session holding When TLS inspection is enabled, Network Firewall acts as an intermediary between the client and server, maintaining separate connections with each endpoint. Throughout this process, Network Firewall evaluates outbound traffic against configured rules to determine whether the traffic should be allowed to exit the firewall.As shown in Figure 1, the steps prior to availability of SNI session holding were:\nThe client creates a TCP connection, and Network Firewall evaluates the stateless rules to determine if the traffic is allowed. If not, the connection is terminated. Network Firewall creates a TCP Connection to the destination server. The client sends a ClientHello message, including SNI information, to Network Firewall. The firewall validates that the SNI is valid, otherwise the connection is terminated. Network Firewall forwards the ClientHello message to the destination server. The destination server responds with a ServerHello message and its certificate. Network Firewall validates the certificates downloaded from the destination server. At this point, the server name indication is validated against the certificate subject name. Network Firewall forwards the server’s certificate to the client and completes the TLS connection with the client. The client encrypts the application payload using the session keys it negotiated during TLS handshake and sends it to Network Firewall. Network Firewall decrypts the traffic, uses its stateful engine to evaluate rules against the traffic, and determines if it is allowed. If traffic is allowed, Network Firewall re-encrypts the application layer payload with the destination server’s session keys and forwards it to the destination server. The destination server sends back response data to Network Firewall. The Network Firewall stateful engine analyzes the destination server’s response. Network Firewall forwards the server response to the client. The communication continues until the client or destination server terminates the connection. Figure 1: Steps prior to availability of SNI session holding\nWith the current sequence of traffic inspection, the TCP connection is established before the TLS SNI field is evaluated, which could lead to a server learning about a connection before the firewall inspects the SNI.\nFor example, when customers configure rules to reject traffic based on TLS SNI fields (such as example.com), they expect these connections to be blocked before opening a connection to the destination server and before data transmission occurs. However, because of the inherent protocol sequence, TCP connections are briefly established before SNI rule validation takes place. This processing order creates a narrow window where sophisticated threat actors could potentially attempt to circumvent data exfiltration prevention controls, even with properly configured SNI-based blocking rules.\nSession holding addresses this concern so that the traffic originating from within VPCs cannot connect to destination servers until Network Firewall verifies the TLS SNI.\nHow TLS inspection works with session holding SNI session holding implements a two-step validation process. First, the firewall examines the TLS layer and validates the SNI when the client sends the TLS client hello message. After the message is approved, Network Firewall allows the connection to the destination server, permitting encrypted upper-layer protocols like HTTP or SMTP to initiate their negotiations. This approach creates a distinct separation between TLS validation and protocol inspection, where protocol examination only occurs after successful TLS handshake authorization.As shown in Figure 2, the steps in this scenario with SNI session holding are:\nNote: Steps 2–5 are part of SNI session holding.\nThe client creates a TCP connection, and Network Firewall evaluates the stateless rules to determine if the traffic is allowed. If not, the connection is terminated. The Client sends a ClientHello message including SNI information to Network Firewall. Network Firewall performs validation of the SNI. The firewall evaluates the TLS inspection rules, including the SNI rules, to determine if the traffic is allowed. If not, the connection is terminated. Network Firewall creates a TCP connection to the destination server. Network Firewall forwards the ClientHello message to the destination server. The destination server responds with a ServerHello message and its certificate. Network Firewall validates the certificates downloaded from the destination server. Network Firewall forwards the server’s certificate to the client and completes the TLS connection with the client. The client encrypts the application payload using the session keys it negotiated during TLS handshake and sends it to Network Firewall. Network Firewall decrypts the traffic, uses its stateful engine to evaluate rules against the traffic, and determines if it is allowed. If traffic is allowed, Network Firewall re-encrypts the application layer payload with the destination server’s session keys and forwards it to destination server. The destination server sends back response data to Network Firewall. Network Firewall stateful engine analyzes the destination server response. Network Firewall forwards the server response to the client. The communication continues until the client, or the destination server terminates the connection. Figure 2: Steps after session holding\nGetting started Session holding can be enabled while creating a TLS inspection configuration directly within a Network Firewall policy using the AWS Management Console, AWS Command Line Interface (AWS CLI), or AWS SDK.\nPrerequisites To get started setting up a Network Firewall policy with session holding, visit the Network Firewall console or see the AWS Network Firewall Developers Guide. Session holding is supported in AWS Regions where Network Firewall is available today, including the AWS GovCloud (US) Regions and China Regions.\nIf this is your first time using Network Firewall, make sure to complete the following prerequisites. If you already have a firewall and TLS inspection configuration, you can skip this section.\nCreate a firewall Create a TLS inspection configuration Enable session holding To enable session holding, follow the steps to create a firewall policy. On the step to Add TLS Inspection configuration, you will have an option to enable session holding by selecting the box as shown in Figure 3. Figure 3: Enable session holding\nAfter adding the TLS inspection configuration and selecting the box to enable session holding, continue to create the new firewall policy and then associate this policy to your firewall.\nIf you have an existing policy that is attached to a TLS inspection configuration, choose Manage TLS Inspection Configuration on your firewall policy. Figure 4: TLS inspection configuration\nThis will provide the option to enable session holding as shown in figure 3.\nPricing SNI session holding is included in the cost of TLS advanced inspection. For TLS advanced inspection pricing, see AWS Network Firewall pricing.\nConsiderations When enabling the session holding, note the following considerations:\nKeywords: Session holding is only applicable to Suricata rules using the TLS.SNI keyword. It does not apply to rules using other TLS application keywords, such as TLS.CERT or TLS.VERSION. Performance: Because TCP connection establishment packets are held until the SNI validation is complete, session holding might introduce latency in the TCP connection establishment. You’ll notice the impact only when there is a surge in new TCP connections being inspected by Network Firewall with TLS inspection enabled. Compatibility: TLS.SNI takes priority over http.host rules when session holding is enabled. When disabled, the traffic can match rules based on the http.host keyword and tls.sni keyword simultaneously, resulting in an outcome defined by the combination of the actions in these two types of rules. However, when this session holding is enabled, this traffic can only match the rule with TLS.SNI keyword and the rule with http.host keyword is applied only when the decrypted traffic has not matched other TLS.SNI-based pass rules. Conclusion As a preventive measure, this session holding helps make sure that SNI validation happens before a connection is established with the destination server, avoiding even initial contact with potentially malicious endpoints. For more information, see What is AWS Network Firewall?\nIf you have feedback about this post, submit comments in the Comments section below.\nGiới thiệu về tác giả Amit Gaur Amit, a Cloud Infrastructure Architect at AWS, brings his passion for technology and knowledge-sharing to the networking community. Specializing in network architecture design, he helps customers build highly scalable and resilient environments on AWS. Through technical guidance and architectural expertise, Amit enables customers to accelerate their cloud adoption journey while making sure their systems are built for scale and reliability.\nSrivalsan Mannoor Sudhagar Srivalsan is a Sr. Cloud Infrastructure Architect at Amazon Web Services Professional Services who brings expertise in Cloud Infrastructure and MLOps solutions. He is passionate about networking, container technologies and loves to innovate to help solve customer problems. He enjoys architecting solutions and providing technical guidance to help customers and partners achieve their technical and business objectives.\nVikram Saurabh Vikram is an experienced engineering leader with 20 years of experience in software engineering, primarily in building firewall products and services. He currently leads the AWS Network Firewall engineering team and has previously led the engineering team of Route53 DNS Firewall. Outside of work, Vikram enjoys playing cricket, hiking, and solving math puzzles.\nOlu Adeleke Olu is a Senior Software Engineer with over 10 years of experience in software development and computer networks. Olu has been the technical lead for many initiatives and features of AWS Network Firewall and has a Ph.D. in computer science. Outside of work, Olu enjoys playing soccer, landscape painting, and hanging out with family and friends.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Make connections with the members of First Cloud Journey. Familiarize with the AWS services and additional softwares/tools. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Began learning the basics of Cloud Computing and core AWS services (Module 1 Introduction) - Read initial documentations for VPC and Networking basics 09/08/2025 09/08/2025 Study Group\u0026rsquo;s Facebook Page: https://cloudjourney.awsstudygroup.com/ Bootcamp’s Tutorial: https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i AWS Documentations: https://docs.aws.amazon.com/ Tuesday - Created AWS Free Tier account #1 and accidentally burned through $10 due to forgetting to stop a resource - Studied Module 01: VPC and Subnets #1 - Practice: Contacted AWS Support and set up an AWS Budget Creation 09/09/2025 09/09/2025 Bootcamp’s Tutorial: https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i AWS Documentations: https://docs.aws.amazon.com/ Wednesday - Created AWS Free Tier account #2 - Studied Module 01: VPC and Subnets #2 - Practice: Created a VPC, Subnets, a basic Route Table, and Security Groups 09/10/2025 09/10/2025 Bootcamp’s Tutorial: https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i AWS Documentations: https://docs.aws.amazon.com/ Thursday - Began studying Module 02: Networking and EC2 #1 - Learned about Gateways (Internet, NAT) and Network ACLs - Practice: Launched an EC2 Instance and learned the connection process 09/11/2025 09/11/2025 Bootcamp’s Tutorial: https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i AWS Documentations: https://docs.aws.amazon.com/ YouTube Tutorial on EC2 Setup: http://www.youtube.com/watch?v=YH_DVenJHII Friday - Continued studying Module 02: EC2 and S3 #2 - Focused on EC2 components and S3 introduction - Practice: Got familiar with the EC2 dashboard and setting up a basic S3 bucket 09/12/2025 09/12/2025 Bootcamp’s Tutorial: https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i AWS Documentations: https://docs.aws.amazon.com/ Saturday - Began studying Module 03: Web Deployment and DNS #1 - Practice: Built a simple Web page and tested its accessibility - Began learning about Route 53 and Hybrid DNS concepts 09/13/2025 09/13/2025 Bootcamp’s Tutorial: https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i AWS Documentations: https://docs.aws.amazon.com/ Week 1 Achievements: Have a general understanding of Cloud Computing, AWS, and initial core services: VPC, Subnets, EC2, and basic S3. Successfully created 2 AWS Free Tier accounts and learned a valuable lesson after losing $10 for forgetting to turn off an EC2 instance. Knows the basics of the AWS Management Console. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Continue studying the Bootcamp up to Module 4 (Security). Familiarize with serverless and CDN services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Continued studying Module 03: Web Deployment and DNS #2 - Focused on CloudFront and its benefits for reducing latency - Began configuring CloudFront with the S3 bucket 09/15/2025 09/15/2025 Bootcamp’s Tutorial: https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i AWS Documentations: https://docs.aws.amazon.com/ Tuesday - Studied Module 03: Web Deployment and DNS #3 - Deep dived into Route 53 concepts, including Routing Policies and Health Checks - Practice: Set up basic A and CNAME records in Route 53 09/16/2025 09/16/2025 Bootcamp’s Tutorial: https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i AWS Documentations: https://docs.aws.amazon.com/ Wednesday - Began studying Module 04: Security and Compliance #1 - Focused on IAM basics: users, groups, policies, and roles - Practice: Configured basic IAM users and attached managed policies 09/17/2025 09/17/2025 Bootcamp’s Tutorial: https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i AWS Documentations: https://docs.aws.amazon.com/ Thursday - Studied Module 04: Security and Compliance #2 - Introduced to key security services: KMS (Key Management Service) and Cognito - Learned about the importance of encryption at rest and in transit 09/18/2025 09/18/2025 Bootcamp’s Tutorial: https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i AWS Documentations: https://docs.aws.amazon.com/ Friday - Studied Module 04: Security and Compliance #3 - Introduced to CloudTrail for logging API activity - Practice: Created a CloudTrail trail and confirmed log delivery to S3 09/19/2025 09/19/2025 Bootcamp’s Tutorial: https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i AWS Documentations: https://docs.aws.amazon.com/ Saturday - Practice: Configured an IAM role to be assumed by an EC2 instance 09/19/2025 09/19/2025 Bootcamp’s Tutorial: https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i AWS Documentations: https://docs.aws.amazon.com/ Week 2 Achievements: Familiarized with web deployment services: CloudFront and Route 53. Gained foundational knowledge of key security services: IAM, KMS, and CloudTrail. Completed the study of Bootcamp Modules 3 and 4. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Revise and master the basics of AWS services and structures. Complete the study of the core Bootcamp material (Module 5). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday Take a break for burnout and rest. 09/22/2025 09/22/2025 N/A Tuesday - Began studying Module 05: Serverless Computing #1 - Focused on Lambda functions and serverless use cases - Relearned the services and structures of AWS #1 (Focus on VPC and Subnets) 09/23/2025 09/23/2025 Bootcamp’s Tutorial: https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i AWS Documentations: https://docs.aws.amazon.com/ Wednesday - Practice: Created and tested a basic Lambda function that reads/writes to S3 - Relearned the services and structures of AWS #2 (Focus on EC2, S3, and Security Groups) 09/24/2025 09/24/2025 Bootcamp’s Tutorial: https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i AWS Documentations: https://docs.aws.amazon.com/ Thursday - Studied Module 05: Serverless Computing #2 (Deep dive into API Gateway integration) - Relearned the services and structures of AWS #3 (Focus on Route 53, Gateways, and IAM) 09/25/2025 09/25/2025 Bootcamp’s Tutorial: https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i AWS Documentations: https://docs.aws.amazon.com/ Friday - Completed study of all Module 01-05 core concepts and principles 09/26/2025 09/26/2025 Bootcamp’s Tutorial: https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i AWS Documentations: https://docs.aws.amazon.com/ Saturday - Relearned the services and structures of AWS #4 (Focus on Lambda, CloudFront, and KMS) 09/27/2025 09/27/2025 Bootcamp’s Tutorial: https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i AWS Documentations: https://docs.aws.amazon.com/ Week 3 Achievements: Completed the entirety of the Bootcamps covered so far (Modules 1 through 5). Achieved a better understanding of AWS core services and infrastructure concepts. Successfully identified and fixed minor mistakes made during the past weeks\u0026rsquo; practical exercises (e.g., proper resource termination). "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Complete and Master the Bootcamp\u0026rsquo;s Contents. Begin Team Project\u0026rsquo;s Planning Phase. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Revise and master the bootcamp #1 - Focused on the principles of the AWS Well-Architected Framework (Operational Excellence and Security Pillars) 09/29/2025 09/29/2025 Bootcamp’s Tutorial: https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i AWS Documentations: https://docs.aws.amazon.com/ Tuesday - Revise and master the bootcamp #2 (Focus on Reliability and Performance Efficiency Pillars) - Began Team Project\u0026rsquo;s Concept Development #1 by brainstorming potential project ideas and defining the scope 09/30/2025 09/30/2025 Bootcamp’s Tutorial: https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i AWS Documentations: https://docs.aws.amazon.com/ Wednesday - Continued Team Project\u0026rsquo;s Concept Development #2 - Defined initial requirements, use cases, and drafted a high-level cloud architecture based on M1-M5 services 10/01/2025 10/01/2025 AWS Documentations: https://docs.aws.amazon.com/ Thursday - Concluded Team Project\u0026rsquo;s Concept Development #3 - Finalized the tech stack (e.g., Lambda, DynamoDB, S3) and planned for the data processing pipeline 10/02/2025 10/02/2025 AWS Documentations: https://docs.aws.amazon.com/ Friday - Began Team Project\u0026rsquo;s Analysis and Data Gathering 10/03/2025 10/03/2025 AWS Documentations: https://docs.aws.amazon.com/ Saturday Break 10/04/2025 10/04/2025 Week 4 Achievements: Mastered the basics of AWS Services covered in the Bootcamp. Gained initial knowledge of the AWS Well-Architected Framework. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Solve the data problem of the project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Began Data generation #1 using AI tools and specialized scripts to simulate realistic project data 10/06/2025 10/06/2025 Tuesday Break. 10/07/2025 10/07/2025 N/A Wednesday - Continued Data generation #2 to increase dataset size and diversity - Began to theorize about the project\u0026rsquo;s scoring formula and metrics for success 10/08/2025 10/08/2025 Thursday - Completed Data generation #3 - Concluded the design of the scoring formula and began setting up for basic model training 10/09/2025 10/09/2025 Friday - Started the initial phase of Data sanitization #1 - Focused on removing outliers, handling missing values, and ensuring data consistency 10/10/2025 10/10/2025 Saturday Break 10/11/2025 10/11/2025 Week 5 Achievements: Successfully generated AI data to simulate a real-life work environment for the project. Developed the preliminary scoring formula and started basic model training setup. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Finish data processing and refinement. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday Too sick for work. 10/13/2025 10/13/2025 N/A Tuesday Too sick for work. 10/14/2025 10/14/2025 N/A Wednesday - Resumed work and focused on Data sanitization #2 - Performed feature engineering and data normalization to prepare for model input 10/15/2025 10/15/2025 Wednesday - Attended (Workshop: Data Science on AWS) - Needed to regenerate data due to mandatory changes in the project\u0026rsquo;s aligned goal and scope 10/16/2025 10/16/2025 AWS Documentations: https://docs.aws.amazon.com/ Thursday - Began Data generation #1 for the new project goal 10/17/2025 10/17/2025 Friday Break 10/18/2025 10/18/2025 Saturday Break 10/19/2025 10/19/2025 Week 6 Achievements: Successfully prepared data for model use. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Finalize data quality and prepare for mid-term exams. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Continued Data generation #2 to finalize the required dataset for the revised project scope 10/20/2025 10/20/2025 AWS Documentations: https://docs.aws.amazon.com/ Tuesday - Performed Data sanitization #3 (Final Pass) - Ensured all data aligns with the scoring formula and model requirements 10/21/2025 10/21/2025 AWS Documentations: https://docs.aws.amazon.com/ Wednesday - Completed Data confirmation - Executed final testing with model\u0026rsquo;s accuracy to benchmark the dataset quality 10/22/2025 10/22/2025 Thursday - Conducted additional data generation (small-scale) to fill minor gaps identified during accuracy testing 10/23/2025 10/23/2025 AWS Documentations: https://docs.aws.amazon.com/ Friday Break to study for mid-term exam. 10/24/2025 10/24/2025 Saturday Break to study for mid-term exam. 10/25/2025 10/25/2025 Week 7 Achievements: Data quality confirmed and initial accuracy testing completed. Mid-term exam preparation successfully started. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Successfully complete the mid-term exam. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday Break to study for mid-term exam. 10/27/2025 10/27/2025 Tuesday Break to study for mid-term exam. 10/28/2025 10/28/2025 Wednesday Mid term exam. 10/29/2025 10/29/2025 Thursday Break. 10/30/2025 10/30/2025 Friday Break. 10/31/2025 10/31/2025 Saturday Break 11/1/2025 11/1/2025 Week 8 Achievements: Successfully completed the mid-term exam. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Set up the core project infrastructure including database and monitoring. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Configured Basic DynamoDB setups (tables and initial read/write capacity) - Implemented initial Cloudwatch Setups for monitoring EC2/Lambda activity 11/03/2025 11/03/2025 AWS Documentations: https://docs.aws.amazon.com/ Tuesday - Set up Cloudwatch Canaries for synthetic monitoring of the application endpoints - Configured SNS (Simple Notification Service) topics and subscriptions for alerts 11/04/2025 11/04/2025 Wednesday Break. 11/05/2025 11/05/2025 Thursday - Updated Cloudwatch Config to match the project\u0026rsquo;s additional features and new metrics requirements 11/06/2025 11/06/2025 AWS Documentations: https://docs.aws.amazon.com/ Friday - Performed DynamoDB re-config to optimize the key schema for the most frequent query patterns 11/07/2025 11/07/2025 AWS Documentations: https://docs.aws.amazon.com/ Saturday Break 11/08/2025 11/08/2025 Week 9 Achievements: Basic infrastructure for persistent data and monitoring established using DynamoDB and CloudWatch. Set up advanced monitoring with CloudWatch Canaries and notifications with SNS. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Warming up to AWS and its\u0026rsquo; services\nWeek 2: Learning And Completing Bootcamp #1\nWeek 3: Learning And Completing Bootcamp #2\nWeek 4: Learning And Completing Bootcamp #3 + Team Project\u0026rsquo;s Planning Phase\nWeek 5: Team Project\u0026rsquo;s Development Phase 1 #1 : Setting Up The Foundation #1\nWeek 6: Team Project\u0026rsquo;s Development Phase 1 #2 : Setting Up The Foundation #2\nWeek 7: Team Project\u0026rsquo;s Development Phase 1 #3 : Setting Up The Foundation #3\nWeek 8: Team Project\u0026rsquo;s Development Phase 1 #4 : Finalize The Foundation\nWeek 9: Team Project\u0026rsquo;s Development Phase 2 #5 : Basic Project Functions\nWeek 10: Team Project\u0026rsquo;s Development Phase 2 #6 : Demo \u0026amp; Testing\nWeek 11: Team Project\u0026rsquo;s Development Phase 3 : Additional Functions\nWeek 12: Team Project\u0026rsquo;s Development Phase 4 : Integration \u0026amp; Handover\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.2-prerequisite/","title":"Prerequisites","tags":[],"description":"","content":"Prerequisites for InsightHR Workshop Before starting this workshop, ensure you have the following tools and accounts set up.\n1. AWS Account You\u0026rsquo;ll need an AWS account with appropriate permissions to create and manage resources.\nRequired AWS Services Access:\nIAM (Identity and Access Management) DynamoDB Lambda API Gateway S3 CloudFront Cognito Bedrock CloudWatch Route53 (optional, for custom domain) Estimated Costs: $2-5/month during development\nIf you\u0026rsquo;re using AWS Free Tier, many services in this workshop are covered. However, some services like Bedrock may incur small charges.\n2. AWS CLI Install and configure the AWS Command Line Interface.\nInstallation:\nWindows:\nmsiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi macOS:\ncurl \u0026#34;https://awscli.amazonaws.com/AWSCLI2.pkg\u0026#34; -o \u0026#34;AWSCLIV2.pkg\u0026#34; sudo installer -pkg AWSCLIV2.pkg -target / Linux:\ncurl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Configuration:\naws configure Enter your:\nAWS Access Key ID AWS Secret Access Key Default region (e.g., ap-southeast-1) Default output format (e.g., json) Verify Installation:\naws --version # Expected output: aws-cli/2.x.x Python/3.x.x ... 3. Node.js and npm Required for frontend development.\nMinimum Version: Node.js 18+\nInstallation:\nDownload from nodejs.org or use a version manager:\nUsing nvm (recommended):\n# Install nvm curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash # Install Node.js nvm install 18 nvm use 18 Verify Installation:\nnode --version # Expected: v18.x.x or higher npm --version # Expected: 9.x.x or higher 4. Python Required for Lambda function development.\nMinimum Version: Python 3.11+\nInstallation:\nDownload from python.org or use your system\u0026rsquo;s package manager.\nVerify Installation:\npython --version # or python3 --version # Expected: Python 3.11.x or higher Install pip (if not included):\npython -m ensurepip --upgrade 5. Text Editor / IDE Choose your preferred development environment:\nRecommended: Visual Studio Code\nDownload from code.visualstudio.com Install recommended extensions: AWS Toolkit Python ESLint Prettier Tailwind CSS IntelliSense Alternatives:\nPyCharm Sublime Text Atom WebStorm 6. Git (Optional but Recommended) For version control and accessing code repositories.\nInstallation:\nDownload from git-scm.com\nVerify Installation:\ngit --version # Expected: git version 2.x.x 7. Required Knowledge JavaScript/TypeScript:\nBasic syntax and ES6+ features Async/await and Promises React fundamentals (components, hooks, state) Python:\nBasic syntax and data structures Functions and error handling Working with JSON AWS Concepts:\nBasic understanding of cloud computing Familiarity with AWS Console Understanding of serverless architecture (helpful but not required) Web Development:\nHTML/CSS basics REST API concepts HTTP methods (GET, POST, PUT, DELETE) 8. AWS Account Setup Create IAM User For security best practices, create an IAM user instead of using root credentials:\nSign in to AWS Console\nNavigate to IAM service\nClick \u0026ldquo;Users\u0026rdquo; → \u0026ldquo;Add users\u0026rdquo;\nEnter username (e.g., insighthr-admin)\nSelect \u0026ldquo;Programmatic access\u0026rdquo; and \u0026ldquo;AWS Management Console access\u0026rdquo;\nAttach policies:\nAdministratorAccess (for workshop purposes) Or create a custom policy with required permissions Download credentials (Access Key ID and Secret Access Key)\nConfigure AWS CLI with these credentials\nEnable Required Services Ensure the following services are available in your region:\n✅ AWS Lambda ✅ Amazon DynamoDB ✅ Amazon API Gateway ✅ Amazon S3 ✅ Amazon CloudFront ✅ Amazon Cognito ✅ Amazon Bedrock (check regional availability) ✅ Amazon CloudWatch Recommended Region: ap-southeast-1 (Singapore) - All services are available and latency is good for Southeast Asia.\n9. Bedrock Model Access AWS Bedrock requires explicit model access request.\nSteps to Enable:\nGo to AWS Console → Amazon Bedrock Navigate to \u0026ldquo;Model access\u0026rdquo; in the left sidebar Click \u0026ldquo;Manage model access\u0026rdquo; Find \u0026ldquo;Claude 3 Haiku\u0026rdquo; by Anthropic Check the box and click \u0026ldquo;Request model access\u0026rdquo; Wait for approval (usually instant for Haiku) Without Bedrock access, the AI chatbot feature won\u0026rsquo;t work. However, you can still complete the rest of the workshop.\n10. Optional: Domain Name If you want to use a custom domain (like insight-hr.io.vn):\nPurchase a domain from a registrar (e.g., Route53, GoDaddy, Namecheap) Have access to DNS management Budget for SSL certificate (free with AWS Certificate Manager) Pre-Workshop Checklist Before proceeding to the next section, verify you have:\nAWS account with admin access AWS CLI installed and configured Node.js 18+ and npm installed Python 3.11+ installed Text editor/IDE set up Git installed (optional) Basic knowledge of JavaScript/TypeScript and Python Understanding of REST APIs AWS Bedrock model access requested Familiarity with AWS Console Troubleshooting AWS CLI Configuration Issues:\n# Check current configuration aws configure list # Test AWS access aws sts get-caller-identity Node.js Version Issues:\n# Check installed versions nvm list # Switch to correct version nvm use 18 Python Version Issues:\n# Check Python path which python3 # Create virtual environment python3 -m venv venv source venv/bin/activate # On Windows: venv\\Scripts\\activate Next Steps Once you\u0026rsquo;ve completed all prerequisites, proceed to Project Architecture to understand the system design.\nAdditional Resources AWS CLI Documentation Node.js Documentation Python Documentation AWS Free Tier AWS Bedrock Documentation "},{"uri":"https://thienluhoan.github.io/workshop-template/2-proposal/","title":"Proposal","tags":[],"description":"","content":"AWS First Cloud AI Journey – Project Plan [SKYLINE2] – [FPTU] – [INSIGHTHR]\n[9-Dec-2025]\nClick to Download Proposal\nTable of Contents BACKGROUND and motivation\n1.1 Executive Summary 1.2 Project Success Criteria 1.3 Assumptions SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM\n2.1 Technical Architecture Diagram 2.2 Technical Plan 2.3 Project Plan 2.4 Security Considerations Activities AND Deliverables\n3.1 Activities and deliverables 3.2 Out of Scope 3.3 Path to Production EXPECTED AWS COST BREAKDOWN BY SERVICES\nTEAM\nRESOURCES \u0026amp; COST ESTIMATES\nACCEPTANCE\nBACKGROUND AND MOTIVATION 1.1 Executive Summary Organization faces HR evaluation inefficiencies due to manual data handling, lack of transparency in evaluation processes and metrics tracking.\nInsightHR delivers HR automation through flexible evaluation management, automated scoring, and AI insights. AWS provides serverless scalability, cost efficiency, security for sensitive data, AI Chatbot via Bedrock, and rapid deployment.\nCustom KPI, automated performance scoring, multi-level dashboards, AI assistant for natural-language queries, automated notifications, role-based access (Admin/Manager/Employee), multi-tenant support.\nEnd-to-end delivery including Well-Architected design, serverless backend (Lambda, DynamoDB, API Gateway), frontend (S3 + CloudFront), authentication/security (Cognito, IAM), KPI/formula builder, AI chatbot (Bedrock + Lambda query data from info tables), notifications (SNS, SES), CI/CD, monitoring, and knowledge transfer.\n1.2 Project Success Criteria Success is defined by demonstrating a functional MVP that proves the platform\u0026rsquo;s capability to automate HR evaluations and deliver measurable business value.\n1. Functional Criteria:\nAuthentication with role-based access (Admin/HR, Manager, Employee) HR creates custom KPIs without technical support CSV upload triggers automated Lambda scoring Dashboard displays individual/team performance with charts AI chatbot answers natural language queries from DynamoDB data SES sends automated email notifications 2. Technical Criteria:\n99.9%+ uptime \u0026lt;300ms API latency (95th percentile) 95%+ scoring accuracy vs manual calculations 90%+ AI response relevance Zero critical security vulnerabilities 3. Performance \u0026amp; Cost:\n~$33.14/month AWS cost End-to-end workflow (upload → score → visualize) completes in \u0026lt;5 minutes 4. Business Impact:\nDemonstrates 60%+ HR time reduction potential Non-technical users operate KPI builder and chatbot independently 5. Delivery:\nWeek 8: MVP (authentication, KPI/formula management, scoring, basic dashboard) Week 12: Full features (chatbot, notifications, advanced dashboard) 1.3 Assumptions 1. Assumptions:\nThe current AWS cost estimate of approximately $33.14/month is accurate for the projected initial load and usage. The required data format and mapping logic for employee performance data can be clearly defined and provided by the HR team for the automated scoring engine. The Large Language Model provided by Amazon Bedrock to support HR. The automated scoring system is trained locally. The technical evaluation files of each team are assessed according to the companies\u0026rsquo; criteria and must follow the format provided by the customer. 2. Constraints:\nThe project delivery must adhere to the 12-week timeline utilizing the Agile Scrum framework. The solution must be built entirely on serverless AWS services to meet the objectives of scalability, cost efficiency, and reduced operational overhead. The final production AWS cost must remain around the ~$33.14/month target. 3. Risks:\nData Security/Compliance: Failure to fully understand or implement all of the customer\u0026rsquo;s specific regulatory control validation requirements could impact the project\u0026rsquo;s ability to meet security objectives. Feature Creep: Requests for features identified as \u0026ldquo;Out of Scope\u0026rdquo; could derail the 12-week MVP delivery timeline. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 Technical Architecture Diagram The InsightHR platform is built on a serverless architecture using AWS services, providing scalability, cost-effectiveness, and high availability. The architecture includes:\n1. Frontend \u0026amp; Content Delivery:\nAmazon S3: Hosts the static website and stores user-uploaded files (CSV, AI models). S3 Vector to store vectors (embeddings for text-data), S3 Standard for storing raw documents. Amazon CloudFront: Distributes static and dynamic content globally with low latency. 2. Backend \u0026amp; Compute:\nAWS Lambda: Executes all business logic, including authentication, custom scoring, and chatbot functions. Amazon API Gateway: Manages APIs as the communication gateway between frontend and backend. 3. Data Storage:\nAmazon DynamoDB: Stores structured data such as user/employee information, company KPIs, scoring formulas, and performance evaluation results. 4. AI \u0026amp; Machine Learning:\nAmazon Bedrock: Provides Large Language Models (LLMs) for the HR assistant chatbot. The ML Model system is trained locally for scoring function. 5. Security \u0026amp; Identity:\nAmazon Cognito: Manages user authentication, registration, and identity workflows. AWS IAM: Manages access control and permissions for AWS services. AWS KMS: Encrypts sensitive data in DynamoDB and S3. 6. Monitoring \u0026amp; Notifications:\nAmazon CloudWatch \u0026amp; CloudWatch Logs: Monitors Lambda functions, API Gateway, and database access. Amazon SNS: Sends notifications (e.g., reminders, result notifications) to employees. 7. Architecture Benefits:\nServerless: No server management and automatic scaling. Cost-Effective: Mostly pay-as-you-go services. High Availability: Built-in redundancy across AWS regions. Scalable: Can handle growth from small teams to large enterprises. Flexible: Easy to modify and extend functionality. 8. Proposed Architecture Diagram:\n9. Tools Proposed for This Project:\nAmazon CloudFront: For global content delivery and caching of static and dynamic web content. Amazon S3: To host static web assets and store documents, vector embeddings, and other files processed by the system. Amazon API Gateway: To provide a secure RESTful interface, acting as the communication layer between frontend clients and backend services. AWS Lambda: To run backend business logic including user dashboard, auto scoring, HR assistant, and data management workflows. Amazon DynamoDB: To store application data such as user information, HR records, scoring results, and vector metadata with low-latency performance. Amazon Cognito: To manage user authentication, authorization, registration, MFA, and secure access to APIs and frontend applications. AWS Identity and Access Management (IAM): To define fine-grained access policies and control permissions between services and users. AWS Key Management Service (KMS): To manage encryption keys used for securing sensitive data stored in S3, DynamoDB, and logs. Amazon ECR: To store containerized model assets and application dependencies in a secure and version-controlled repository. Amazon Bedrock / Large Language Model (LLM): To provide AI capabilities for chat, data extraction, summarisation, and intelligent HR workflows. Amazon Simple Email Service (SES): To send automated email notifications such as onboarding alerts and HR communications. Amazon Simple Notification Service (SNS): To publish notifications and trigger downstream processes; integrates with email, SMS, and microservices. Amazon CloudWatch \u0026amp; CloudWatch Logs: For monitoring performance, logging, tracing, and operational troubleshooting across Lambda, API Gateway, and AI components. 2.2 Technical Plan The partner will develop automated deployment scripts using AWS CloudFormation and Infrastructure as Code (IaC) practices.\nThis will allow for quick and repeatable deployments into AWS accounts. Some additional configurations such as WAF rules on CloudFront for enhanced security may require approval and will follow standard DevOps change management processes.\nApplication Feature Implementation:\n1. Authentication \u0026amp; Security Module\nUser Management: Cognito manages user lifecycle Registration, login, password reset workflows Access Control: IAM and RBAC enforce role-based permissions Admin/HR, Manager, and Employee access levels API Security: API Gateway implements JWT-protected endpoints Token validation before Lambda processing 2. Administration Module (HR Panel)\nKPI Management: HR creates, edits, and deletes custom metrics Examples: Tasks Completed, Code Quality, Customer Satisfaction Definitions stored in DynamoDB Auto scoring by employee\u0026rsquo;s technical score for each team with ML model. 3. Core User Functions\nData Upload \u0026amp; Mapping: Upload performance data files (CSV) to DynamoDB Scoring Engine: Lambda triggered on upload Retrieves active formula from DynamoDB Calculates employee scores Stores results in DynamoDB Flow: Upload → Validate → Map → Calculate → Store Dashboard: Visualize individual and department performance Line graphs, bar charts, trend analysis AI Chatbot: Bedrock (LLM) integration Natural language queries (e.g., \u0026ldquo;Summarize Team A Q4 performance\u0026rdquo;) Queries and summarizes DynamoDB data Notifications: SES sends automated alerts Performance milestones, review reminders, custom triggers 2.3 Project Plan The partner will adopt the Agile Scrum framework over 12 one-week sprints totaling a 12-week delivery timeline.\n1. Team Responsibilities\nProduct Owner: Prioritizes backlog (KPIs, formulas, analytics) Final authority on feature acceptance Development Team: Implements Cognito authentication Builds admin portal and formula builder Develops scoring engine and dashboard Integrates Bedrock chatbot and SNS with SES notifications via Email. QA Personnel: Conducts functional, performance, and security testing Facilitates UAT Ensures compliance and quality standards 2. Communication Cadences\nDaily Standups (30 min - 1 hr): Progress review and blocker identification Retrospectives (Weekly, 1 hr): Process improvement and delivery optimization Executive Updates (Weekly): Written reports on progress, risks, KPIs, roadmap Leadership decisions required 3. Knowledge Transfer\nSessions conducted by the development team covering AWS serverless fundamentals KPI and formula configuration Data workflows and column-mapping Dashboard navigation and analytics System monitoring (CloudWatch, Cognito, DynamoDB) 2.4 Security Considerations The partner will implement AWS security best practices based on the Well-Architected Framework, prioritizing protection of sensitive HR data while ensuring high operational availability. Security implementation covers five key categories:\n1. Access Control\nCognito manages user identities Enforces strong password policies and MFA support IAM implements RBAC Admin/HR access Admin Panel and KPI/Formula configurations Employees view only their own performance data API Gateway validates JWT tokens Cognito-issued tokens verified before Lambda processing 2. Infrastructure Security\nServerless architecture reduces attack surface No OS or server patching required Lambda functions communicate via private AWS networks Only necessary endpoints exposed through API Gateway 3. Data Protection\nKMS encrypts data at rest DynamoDB and S3 encrypted Data unusable without decryption keys TLS/SSL (HTTPS) encrypts data in transit All frontend-backend communication secured 4. Detection \u0026amp; Monitoring\nCloudWatch Logs captures execution details Lambda and API Gateway activity logged Real-time monitoring and anomaly detection enabled AWS Config tracks configuration changes Ensures resource compliance with security objectives 5. Incident Management\nCloudWatch Alarms trigger automated alerts via SES Failed login threshold breaches Lambda resource anomalies Security Hub provides consolidated security view Unified compliance findings across AWS environment Simplifies incident identification and response AWS CloudTrail and AWS Config will be configured for continuous monitoring of activities and compliance status of resources. The customer will share their regulatory control validation requirements as inputs for the partner to ensure all security objectives are met.\nACTIVITIES AND DELIVERABLES 3.1 Activities and Deliverables NOTE: Some Project Phases overlap each other.\nProject Phase Timeline Activities Deliverables/Milestones Total man-day Phase 1: Foundation \u0026amp; Scoring Model Week 1-8 • Personal infrastructure architecture research • Data generation for local model training • Scoring model build • Finalized personal architecture diagram • Ready dataset for Local Model training • Scoring Model MVP (Minimum Viable Product) 80 Phase 2: Project Setup \u0026amp; Dashboard Week 9-10 • Project Setup with basic functions: IAM Role, CRUD function, Static web • Web UI Demo • Implement Dashboard • Fix Model • Basic IAM Roles configured • Operational CRUD functions • Static website deployed (S3/CloudFront) • Web UI Demo completed • Dashboard displaying data implemented 40 Phase 3: AI Agent \u0026amp; Absence Mgmt Week 11 • Building Bedrock Agent • Implement Absent managing • Bedrock Agent built • Operational Absence tracking workflow implemented 15 Phase 4: Integration, Testing \u0026amp; Handover Week 12 • Implement Chatbot into App • Testing and set up Monitoring • Chatbot integrated into the application • Functional, Performance, and Security Testing completed • Monitoring (CloudWatch) configured and operational • Project Completion Report \u0026amp; Post-implementation support plan delivered 15 3.2 Out Of Scope 1. AI Enhancements\nAI-Powered Insights:\nWhen sufficient data is available, develop AI models capable of: Chatbot accesses database directly and retrieves prompt window -\u0026gt; Price a lot of tokens and high lock, future can be optimized by other ways Identifying performance patterns across teams and departments Predicting HR risks (e.g., turnover likelihood, burnout indicators) Recommending personalized development plans Detecting anomalies in performance data Suggesting optimal team compositions Machine Learning Features:\nPredictive analytics for workforce planning Sentiment analysis from employee feedback Automated skill gap analysis Performance trend forecasting 2. Public API Development\nAPI Ecosystem:\nBuild a comprehensive API set allowing other internal business systems to automatically push performance data into InsightHR. Integration Targets:\nProject management tools (Jira, Asana, Monday.com) CRM systems (Salesforce, HubSpot) Time tracking software (Toggl, Harvest) Communication platforms (Slack, Microsoft Teams) Code repositories (GitHub, GitLab, Bitbucket) Benefits:\nTransform InsightHR into a central HR data processing hub Create a synchronized and comprehensive management ecosystem Eliminate manual data entry Real-time performance tracking 3. Advanced Features\nMobile Applications:\niOS and Android native apps Push notifications Offline capabilities Mobile-optimized dashboards DynamoDB back up Advanced Analytics:\nPredictive modeling Benchmarking across industries Custom report builder Data export and API for third-party tools Collaboration Features:\nPeer review systems 360-degree feedback Goal setting and tracking Performance improvement plans Compliance \u0026amp; Governance:\nAudit trails Compliance reporting Data retention policies Advanced access controls 3.3 Path To Production This document outlines the current production architecture and operational status for the InsightHR platform deployment. The platform is fully live in the ap-southeast-1 (Singapore) region.\n1. Platform Architecture and Access\nPublic URL: https://insight-hr.io.vn AWS Region: ap-southeast-1 (Singapore) Frontend: React application hosted on S3 (insighthr-web-app-sg) with CloudFront HTTPS distribution. Backend: 8 Lambda function groups accessed via API Gateway REST API. Database: DynamoDB tables configured with On-Demand capacity. 6 tables for each team Employee information table History score table Absent table Account managing tables Authentication: Cognito User Pool. AI/Chatbot: Amazon Bedrock (Claude 3 Haiku) integration for conversation history and knowledge base to enhance models. 2. Live Production Features\nThe following core features have been successfully deployed and are operational:\nAuthentication: Full support for email/password login, password reset workflows. User Management: Complete CRUD functionality, including bulk import and role-based access. Employee Management: Full support for 300+ employees and bulk operations. Performance Score Management: Management of 900+ quarterly scores and calendar-based viewing. Attendance Management: Processing of 9,300+ records, including check-in/check-out kiosk functionality and auto-absence marking. Performance Dashboard: Live charts, trend analysis, live clock, and CSV export capabilities. AI Chatbot: Bedrock integration with conversation history enabled. 3. Deployment and Verification Process\nThe standard, repeatable deployment workflow ensures rapid and verifiable updates to the production site:\nBuild: npm run build creates the optimized production asset bundle. Test: npm run preview validates the built bundle locally prior to deployment. Deploy: aws s3 sync dist/ s3://insighthr-web-app-sg --region ap-southeast-1 pushes assets to the S3 bucket. Invalidate: aws cloudfront create-invalidation --distribution-id E3MHW5VALWTOCI --paths \u0026quot;/*\u0026quot; clears the CloudFront CDN cache. Verify: Full feature testing is performed on the live public URL. 4. Remaining Production Enhancements\nThe platform is in the final phases of enhancement before full stabilization, with key items planned or in progress:\nPage Integration (In Progress)\nConsolidate all administrative page navigation. Verify all features are accessible from the main menu. Test role-based routing across all pages. Fix any integration bugs. Polish and Final Deployment (Planned)\nImplement comprehensive error handling and input validation. Refine responsive design for full mobile compatibility. Conduct dedicated Security testing (penetration testing, vulnerability scanning). Execute Load testing for scalability validation. Develop user documentation and training materials. Perform final production hardening procedures. Monitoring and Scalability Strategy\nActive Monitoring: CloudWatch Logs are enabled for all Lambda functions and API Gateway endpoints, along with CloudWatch Metrics for performance tracking. Planned Alarms: CloudWatch Alarms and SNS notifications are planned for critical error rates and latency. Scalability: Achieved via Serverless Architecture (DynamoDB On-Demand, Lambda, CloudFront CDN). Disaster Recovery: DynamoDB Point-in-Time Recovery and S3 Versioning are planned to be enabled for critical data/assets. Lambda code is stored in version control for rapid redeployment. EXPECTED AWS COST BREAKDOWN BY SERVICES AWS Service Monthly Estimated Cost (USD) Amazon Bedrock $21.61 AWS Lambda $3.75 Amazon Simple Email Service (SES) $2.25 Amazon DynamoDB $1.52 Amazon Simple Storage Service $0.46 Amazon CloudWatch $0.80 Amazon API Gateway $0.06 Amazon CloudFront $0.00 Amazon Cognito $0.00 Amazon EventBridge $0.00 Amazon IAM $1.60 Amazon KMS $1.03 TOTAL MONTHLY COST $33.14 TOTAL YEARLY COST $397.79 TEAM Name Task Role Email / Contact Info Bùi Tấn Phát Dashboard, Manage Employee, Support, Content check Leader btfat3103@gmail.com Nguyễn Ngọc Long CRUD, Config Network / API Gateway, Test function, Slide Member nguyenngoclong216@gmail.com Đặng Nguyễn Minh Duy Database, CloudWatch / CloudLogs, Paper, Slide Member dangnguyenminhduy11b08@gmail.com Đỗ Đăng Khoa Log In/ Registration / Forget Password, UI / UX - Static Web, Paper Member khoado7577@gmail.com Nguyễn Huỳnh Thiên Quang Auto Scoring, AI Assistant, Slide Member quangkootenhatvutru@gmail.com RESOURCES \u0026amp; COST ESTIMATES Resource Responsibility Rate (USD) / Hour Full-Stack Developers [2] React frontend, Python Lambda backend, API integration $66 Cloud Engineers [3] AWS infrastructure setup, deployment automation, monitoring $66 Other (Please specify) Estimated platform consumption (Lambda, DynamoDB, Bedrock). Paper and present material $0.01 NOTE: Project Phase durations overlap each other.\nProject Phase Duration Man-Days Other (Please specify) Estimated Cost Phase 1: Foundation \u0026amp; Scoring Model 8 Weeks 80 - $42,246.40 (80 x $528.08) Phase 2: Project Setup \u0026amp; Dashboard 2 Weeks 40 - $21,123.20 (40 x $528.08) Phase 3: AI Agent \u0026amp; Absence Mgmt 1 Week 15 - $7,921.20 (15 x $528.08) Phase 4: Integration, Testing \u0026amp; Handover 1 Week 15 - $7,921.20 (15 x $528.08) Total Hours 12 Weeks 150 Man-Days $79,212.00 Cost Contribution distribution between Partner, Customer, AWS.\nParty Contribution (USD) % Contribution of Total Customer 0 0 Partner 0 0 AWS 200 100 ACCEPTANCE 1. Project Acceptance Criteria\nThe InsightHR platform will be considered complete and accepted when the following criteria are met.\n2. Completed Deliverables\nAll major features implemented and deployed to production User and employee management with bulk operations Performance score management with calendar view Attendance system with auto-absence marking Interactive dashboard with live clock AI chatbot with Bedrock integration 3. Key Metrics Achieved\n300+ user accounts 300 employee records across 5 departments 900+ performance scores tracked 9,300+ attendance records AWS monthly cost: ~$33.14 System uptime: 99.9%+ Zero critical security vulnerabilities 4. Acceptance Status\nCurrent Status: Application deployed in cloudfront Production URL: https://d2z6tht6rq32uy.cloudfront.net 5. Next Steps\nMinor bug fixing and feature updates Conduct user acceptance testing Provide knowledge transfer and training "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.2-event2/","title":"Kick-off AWS First Cloud Journey Workforce OJT FALL 2025","tags":[],"description":"","content":"Event Objectives Formally inaugurate the AWS First Cloud Journey (FCJ) Workforce OJT program for the Fall 2025 intake. Establish connections between new interns and influential industry leaders from AWS and its partners. Deliver strategic career orientation covering specialized fields like Cloud Computing, DevOps, and GenAI. Inspire participants through successful alumni stories and promote the initiative for diversity in technical roles (\u0026ldquo;She in Tech\u0026rdquo;). Speakers Mr. Nguyen Tran Phuoc Bao – Head of Enterprise Relations (School Representative) Mr. Nguyen Gia Hung – Head of Solutions Architect, AWS Vietnam Mr. Do Huy Thang – DevOps Lead, VNG Mr. Danh Hoang Hieu Nghi – GenAI Engineer, Renova Ms. Bui Ho Linh Nhi – AI Engineer, SoftwareOne Mr. Pham Nguyen Hai Anh – Cloud Engineer, G-Asia Pacific Mr. Nguyen Dong Thanh Hiep – Principal Cloud Engineer, G-Asia Pacific Key Highlights 1. Program Launch and Strategic Alignment Academic Collaboration: Mr. Nguyen Tran Phuoc Bao highlighted the crucial partnership between educational institutions and tech companies aimed at closing the industry skill gap. Cloud Vision: Mr. Nguyen Gia Hung presented the \u0026ldquo;First Cloud Journey\u0026rdquo; as a crucial stepping stone for developing the next generation of cloud professionals in Vietnam. 2. Career Insights: Specialization and Progression DevOps Realities: Mr. Do Huy Thang provided an unvarnished perspective on the DevOps career path, detailing the competencies required to excel in a large technology corporation. FCJ Success Stories: Alumni shared their fast-track experiences, demonstrating how the FCJ foundational training led them directly into specialized AI and GenAI engineering roles. 3. Culture and Professional Life Inclusivity in Tech: Ms. Bui Ho Linh Nhi\u0026rsquo;s segment championed the increasing presence of women in technology, encouraging broader participation in Cloud and AI careers. Engineer Day-to-Day: Speakers from G-Asia Pacific offered transparency into the daily operations and responsibilities across various levels of a Cloud Engineer\u0026rsquo;s career. Key Takeaways Career Strategy Diverse Trajectories: Success in the cloud industry is possible across multiple roles, from pure infrastructure engineering to specialized automation (DevOps) and cutting-edge GenAI development. Strong Core Foundation: Success in specialized fields is built upon a solid mastery of fundamental Cloud Computing concepts acquired during the OJT program. Essential Mindset Adaptability is Crucial: Moving from an academic setting to an enterprise requires a definite shift toward a proactive, continuous, and self-directed learning approach. Network as Support: The community of mentors and alumni provides a vital support structure for overcoming challenges and accelerating career development. Applying to Work Establish Performance Metrics: Based on the insights from industry speakers, define clear, measurable technical goals for the internship (e.g., specific certifications, mastering core services). Mentor Engagement: Commit to actively networking and building relationships with the experts and mentors introduced at the kick-off event. Prioritize GenAI Exploration: Allocate dedicated time within the OJT to explore and implement Generative AI services on AWS, aligning with the successful paths of the alumni speakers. Event Experience The kick-off at the Bitexco Financial Tower created an atmosphere that was both highly professional and deeply inspiring.\nEnergy and Connection: The event fostered a strong sense of collective purpose among the new cohort, the alumni, and the enterprise leaders. Inspiration Achieved: Hearing concrete success stories from peers who progressed \u0026ldquo;From FCJ to GenAI Engineer\u0026rdquo; made ambitious career objectives feel immediate and wholly achievable. Some event photos "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Optimize infrastructure cost and finalize data/storage configuration. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday Break. 11/10/2025 11/10/2025 Tuesday - Reviewed AWS Cost Explorer data to implement Cost optimization for the infrastructure - Focused on adjusting DynamoDB provisioned capacity and S3 storage tiers 11/11/2025 11/11/2025 Wednesday Break. 11/12/2025 11/12/2025 Thursday - Made slight alterations to data within S3 buckets - Re-configured S3 lifecycle policies and performed final re-config of DynamoDB tables 11/13/2025 11/13/2025 Friday - Established and subsequently demolished Cloudwatch DynamoDB alarms due to cost overruns 11/14/2025 11/14/2025 AWS Documentations: https://docs.aws.amazon.com/ Saturday Break 11/15/2025 11/15/2025 Week 10 Achievements: Implemented initial cost optimization strategies across the infrastructure. Finalized data storage configuration in S3 and DynamoDB. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Collaborate with the team to finalize project implementation and testing. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Attended (AWS Cloud Mastery Series #2) - Team meeting to finish project #1 11/17/2025 11/17/2025 AWS Documentations: https://docs.aws.amazon.com/ Tuesday - Team meeting to finish project #2 11/18/2025 11/18/2025 Wednesday Break. 11/19/2025 11/19/2025 Thursday - Team meeting to finish project #3 11/20/2025 11/20/2025 Friday - Team meeting to finish project #4 11/21/2025 11/21/2025 Saturday Break 11/22/2025 11/22/2025 Week 11 Achievements: Successfully collaborated with the team to finalize the project implementation phase. Integrated all project components and completed comprehensive testing. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Consolidate AWS knowledge and perform final polish on the team project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Dedicated time to wrap up on AWS knowledge - Reviewed all Bootcamp materials, notes, and practical scenarios 11/24/2025 11/24/2025 Tuesday - Wrap up on team\u0026rsquo;s project - Reviewed the final code, architecture, and deployment pipeline - Attended (AWS Cloud Mastery Series #3) 11/25/2025 11/25/2025 Wednesday Break. 11/26/2025 11/26/2025 Thursday - Executed final polish to team\u0026rsquo;s project - Prepared demo environment, finalized presentation slides, and reviewed the project README 11/27/2025 11/27/2025 Friday Break. 11/28/2025 11/28/2025 Saturday Break 11/29/2025 11/29/2025 Week 12 Achievements: Consolidated knowledge of all AWS services covered during the internship. Successfully wrapped up the team\u0026rsquo;s project and prepared all materials for final submission/presentation. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-architecture/","title":"Project Architecture","tags":[],"description":"","content":"InsightHR Architecture Deep Dive This section provides a detailed look at the InsightHR platform architecture, explaining how different AWS services work together to create a scalable, secure, and cost-effective serverless application.\nHigh-Level Architecture ┌─────────────────────────────────────────────────────────────────┐ │ User Browser │ │ (React + TypeScript) │ └────────────────────────┬────────────────────────────────────────┘ │ HTTPS ▼ ┌─────────────────────────────────────────────────────────────────┐ │ CloudFront CDN │ │ - Global edge locations │ │ - SSL/TLS termination │ │ - Caching static assets │ └────────────────────────┬────────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────────┐ │ S3 Static Website │ │ - React SPA hosting │ │ - Static assets (JS, CSS, images) │ └─────────────────────────────────────────────────────────────────┘ │ REST API calls ▼ ┌─────────────────────────────────────────────────────────────────┐ │ API Gateway (REST) │ │ - Request routing │ │ - Cognito authorization │ │ - Request/response transformation │ └────────────────────────┬────────────────────────────────────────┘ │ ┌───────────────┼───────────────┬───────────────┐ ▼ ▼ ▼ ▼ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ │ Lambda │ │ Lambda │ │ Lambda │ │ Lambda │ │ Auth │ │ Employees │ │ Performance │ │ Chatbot │ │ │ │ │ │ Scores │ │ │ └──────┬───────┘ └──────┬───────┘ └──────┬───────┘ └──────┬───────┘ │ │ │ │ ▼ ▼ ▼ ▼ ┌─────────────────────────────────────────────────────────────────┐ │ DynamoDB │ │ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐ │ │ │ Users │ │Employees │ │ Scores │ │Attendance│ │ │ └──────────┘ └──────────┘ └──────────┘ └──────────┘ │ └─────────────────────────────────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────────┐ │ AWS Bedrock │ │ (Claude 3 Haiku Model) │ │ - Natural language processing │ │ - Context-aware responses │ └─────────────────────────────────────────────────────────────────┘ Component Breakdown 1. Frontend Layer Amazon S3 + CloudFront\nS3 Bucket: Hosts the React SPA as static files\nConfigured for static website hosting Stores HTML, JavaScript, CSS, and image assets Versioning enabled for rollback capability CloudFront Distribution: Global CDN for fast content delivery\nEdge locations worldwide for low latency SSL/TLS certificate from ACM Custom domain support (insight-hr.io.vn) Caching policies for optimal performance Origin Access Identity (OAI) for S3 security React Application\nSingle Page Application (SPA) architecture Client-side routing with React Router State management with Zustand TypeScript for type safety Tailwind CSS for styling 2. API Layer Amazon API Gateway (REST)\nEndpoints: RESTful API design\n/auth/* - Authentication endpoints /employees/* - Employee management /performance-scores/* - Performance tracking /attendance/* - Attendance management /chatbot/* - AI chatbot queries Features:\nRequest validation CORS configuration Rate limiting and throttling Request/response transformation API keys and usage plans Authorization: Cognito User Pool Authorizer\nJWT token validation Role-based access control Automatic token refresh 3. Compute Layer AWS Lambda Functions\nAuthentication Service (auth-*-handler)\nLogin with Cognito User registration Google OAuth integration Password reset workflow Token management Employee Service (employees-*-handler)\nCRUD operations for employees Department and position filtering Bulk import from CSV Search functionality Performance Scores Service (performance-scores-handler)\nScore calculation and management Quarterly performance tracking KPI aggregation Role-based data access Chatbot Service (chatbot-handler)\nNatural language query processing Context building from DynamoDB Bedrock API integration Response formatting Attendance Service (attendance-handler)\nCheck-in/check-out operations Attendance history tracking Status management Bulk operations Lambda Configuration:\nRuntime: Python 3.11 Memory: 256-512 MB Timeout: 30-60 seconds Environment variables for configuration IAM roles with least privilege 4. Data Layer Amazon DynamoDB\nTables Structure:\ninsighthr-users-dev\nPrimary Key: userId GSI: email-index Purpose: User authentication and profiles Attributes: email, name, role, employeeId, department insighthr-employees-dev\nPrimary Key: employeeId GSI: department-index Purpose: Employee master data Attributes: name, email, department, position, status insighthr-performance-scores-dev\nPrimary Key: employeeId Sort Key: period GSI: department-period-index Purpose: Quarterly performance scores Attributes: overallScore, kpiScores, calculatedAt insighthr-attendance-history-dev\nPrimary Key: employeeId Sort Key: date GSI: date-index, department-date-index Purpose: Check-in/check-out history Attributes: checkInTime, checkOutTime, status DynamoDB Features:\nOn-demand billing mode Point-in-time recovery Encryption at rest Global secondary indexes for efficient queries TTL for automatic data expiration (if needed) 5. Authentication Layer Amazon Cognito User Pools\nUser Management:\nEmail/password authentication Google OAuth integration User attributes (name, role, department) Password policies and MFA support Token Management:\nJWT tokens (ID, Access, Refresh) Token expiration and refresh Custom claims for roles Security Features:\nPassword complexity requirements Account recovery workflows User verification Brute force protection 6. AI/ML Layer Amazon Bedrock (Claude 3 Haiku)\nCapabilities:\nNatural language understanding Context-aware responses Data querying and analysis Conversational interface Integration:\nInvoked from Lambda function Context built from DynamoDB data Role-based data filtering Response formatting and validation Cost Optimization:\nHaiku model for cost-effectiveness Efficient prompt engineering Response caching where applicable 7. Monitoring Layer Amazon CloudWatch\nLogs:\nLambda function logs API Gateway access logs Error tracking and debugging Metrics:\nAPI request counts Lambda invocations and duration DynamoDB read/write capacity Error rates and latency Alarms:\nHigh error rate alerts Performance degradation Cost threshold warnings CloudWatch Synthetics Canaries:\nLogin flow testing Dashboard availability Chatbot functionality Performance score calculations Data Flow Examples User Login Flow 1. User enters credentials in React app 2. React app calls API Gateway /auth/login 3. API Gateway routes to auth-login-handler Lambda 4. Lambda authenticates with Cognito 5. Cognito returns JWT tokens 6. Lambda stores user session in DynamoDB 7. Tokens returned to React app 8. React app stores tokens in localStorage 9. Subsequent requests include JWT in Authorization header Employee Query Flow 1. User requests employee list in React app 2. React app calls API Gateway /employees with filters 3. API Gateway validates JWT token with Cognito 4. Request routed to employees-handler Lambda 5. Lambda queries DynamoDB employees table 6. Results filtered based on user role 7. Data returned to React app 8. React app displays employees in table Chatbot Query Flow 1. User types question in chatbot interface 2. React app calls API Gateway /chatbot/query 3. API Gateway validates JWT and routes to chatbot-handler 4. Lambda retrieves relevant data from DynamoDB 5. Lambda builds context and calls Bedrock API 6. Bedrock processes query with Claude 3 Haiku 7. Response formatted and returned to React app 8. React app displays answer in chat interface Security Architecture Defense in Depth:\nNetwork Security:\nHTTPS only (enforced by CloudFront) API Gateway with AWS WAF (optional) VPC endpoints for private connectivity (optional) Authentication \u0026amp; Authorization:\nCognito for user authentication JWT tokens for API authorization Role-based access control (RBAC) Least privilege IAM roles Data Security:\nEncryption at rest (DynamoDB, S3) Encryption in transit (TLS 1.2+) Secure credential management No hardcoded secrets Application Security:\nInput validation SQL injection prevention (NoSQL) XSS protection CORS configuration Scalability \u0026amp; Performance Auto-Scaling:\nLambda: Automatic concurrent execution scaling DynamoDB: On-demand capacity mode CloudFront: Global edge network API Gateway: Automatic request handling Performance Optimization:\nCloudFront caching for static assets DynamoDB GSIs for efficient queries Lambda function optimization API response caching High Availability:\nMulti-AZ deployment (automatic) CloudFront global distribution DynamoDB replication Lambda fault tolerance Cost Optimization Strategies:\nOn-demand pricing for variable workloads Lambda free tier utilization CloudFront caching to reduce origin requests DynamoDB query optimization Bedrock Haiku model for cost-effectiveness Estimated Monthly Costs (Development):\nDynamoDB: $0.50 Lambda: Free tier S3 + CloudFront: $1-2 API Gateway: $0.10 Bedrock: $0.0004 per query Total: $2-5/month Next Steps Now that you understand the architecture, let\u0026rsquo;s proceed to Setup AWS Environment to start building the platform.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.3-event3/","title":"Workshop: Data Science on AWS","tags":[],"description":"","content":"Event Objectives Provide a thorough overview of the lifecycle of a modern Data Science solution, from conceptualization to deployment. Detail the structure of the AWS Data Science Pipeline, covering data persistence, transformation, and model deployment stages. Facilitate practical, hands-on learning using a real dataset (IMDb) to construct an application (Sentiment Analysis). Compare the operational and financial advantages and disadvantages of Cloud-based versus On-premise infrastructures. Speakers Mr. Van Hoang Kha – Cloud Solutions Architect, AWS Community Builder Mr. Bach Doan Vuong – Cloud DevOps Engineer, AWS Community Builder Key Highlights 1. Cloud in Data Science \u0026amp; Pipeline Overview Cloud Necessity: Explored the reasons modern data science necessitates the cloud: required agility, scalability, and seamless integration, which static on-premise setups cannot offer. Core Pipeline Components: Data Lake: Emphasized Amazon S3 as the essential, scalable storage layer. Serverless ETL: Introduced AWS Glue for automated, serverless data transformation and cleaning. ML Environment: Focused on Amazon SageMaker as the unified platform for the entire machine learning model lifecycle. The broader AWS AI/ML stack, including specialized AI services, was also reviewed. 2. Practical Demos Demo 1: Data Preparation: Illustrated the process of cleaning and transforming the IMDb dataset using AWS Glue. Techniques for feature engineering were shown, contrasting low-code options like SageMaker Canvas with traditional, code-driven methods using libraries like Numpy/Pandas. Demo 2: Model Deployment: Demonstrated the complete ML workflow for a Sentiment Analysis model. This included the \u0026ldquo;Train, Tune, Deploy\u0026rdquo; sequence in SageMaker Studio. The presenters also covered the flexibility of deploying custom models via the Bring Your Own Model (BYOM) approach, compatible with popular frameworks like TensorFlow and PyTorch. 3. Strategic Discussions Platform Comparison: A focused discussion on optimizing for both performance and budget. It was stressed that the elasticity of the cloud makes it superior for experimenting with computationally heavy workloads compared to the prohibitive capital investment of fixed on-premise hardware. Follow-up Project: A suggested mini-project was introduced to help attendees solidify the hands-on knowledge acquired during the workshop. Key Takeaways Technical Workflow Technical Integration: A functional ML workflow demands seamless, automated integration across core services: data lake storage (S3), processing (Glue), and the modeling environment (SageMaker). Strategic Tooling: Competence involves knowing when to utilize high-level managed AI services (e.g., Comprehend, Textract) and when to develop custom models within SageMaker to maximize efficiency and ROI. Industry Application Operational Focus: The main challenge in moving from classroom concepts to enterprise reality is achieving automation and scalable operations. Cost-Benefit Balance: Successful data projects require a constant balance between achieving high model precision and managing the associated operational costs. Applying to Work Modernize ETL: Transition existing local data transformation and ETL scripts to leverage AWS Glue for its serverless automation capabilities, especially for processing large-scale data. Standardize Modeling: Adopt SageMaker Studio as the primary, professional environment for iterating on and deploying experimental models, thereby standardizing the ML lifecycle. Reinforce Learning: Commit to executing the suggested follow-up project to solidify the end-to-end knowledge gained from processing and analyzing real-world datasets. Event Experience The “Data Science on AWS” workshop effectively served as the vital connection point between theoretical academic study and the practical realities of enterprise deployment.\nReal-World Application: I gained direct exposure to the specific cloud technologies utilized by leading global tech organizations. Workflow Clarity: The live demonstration of cleaning the IMDb data and deploying a Sentiment Analysis model clarified the practical implementation steps for building cloud-based AI systems. Strategic Insight: My interaction with the AWS Community Builders provided valuable strategic context for the economic and operational arguments supporting cloud migration over the maintenance of traditional on-premise solutions. Some event photos "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Implementing granular failover in multi-Region Amazon EKS This blog introduces how to implement granular, active-passive failover for applications deployed on Amazon EKS (Elastic Kubernetes Service) across multiple AWS Regions. You will learn why multi-Region disaster recovery is crucial for achieving high availability and business continuity for mission-critical containerized workloads, how granular failover allows for selective traffic shifting at the service level (instead of failing over the entire cluster), and the role of Amazon Route 53 and AWS Gateway Load Balancer (GWLB) in this high-resilience architecture. The article also guides you through the steps to set up the EKS clusters in two regions, configure Route 53 health checks and failover routing, and implement the necessary network infrastructure to ensure seamless traffic management and resilience.\nBlog 2 - Tailor Amazon SageMaker Unified Studio project environments to your needs using custom blueprints This blog introduces how to tailor Amazon SageMaker Studio project environments using custom project templates, or blueprints, for machine learning (ML) workflows. You will learn why custom templates are essential for standardizing environments, integrating specific resources (like custom Amazon ECR images or specialized infrastructure), and ensuring compliance across MLOps projects, how these blueprints are built using AWS Service Catalog, and the role of Amazon CodeCommit and AWS CodePipeline in managing the lifecycle of these custom templates. The article also guides you through the steps to create and share your own custom project blueprints, manage resource provisioning, and provide data scientists with a pre-configured, consistent environment tailored to their exact needs.\nBlog 3 - Enhance TLS inspection with SNI session holding in AWS Network Firewall This blog introduces how to enhance TLS inspection and security posture using the new Server Name Indication (SNI) session holding feature in AWS Network Firewall. You will learn why Network Firewall is a critical layer for deep packet inspection and network segmentation in your VPCs, how the SNI session holding feature prevents security policy bypass by ensuring that subsequent connections from the same client reuse the initial TLS session, and the importance of this feature for security compliance and robust threat prevention. The article also guides you through the steps to configure and deploy Network Firewall with SNI session holding, demonstrate how it improves the accuracy of TLS inspection, and ensure consistent application of security rules.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-setup-aws/","title":"Setup AWS Environment","tags":[],"description":"","content":"Setting Up AWS Environment This section guides you through configuring your AWS environment for the InsightHR platform, including IAM roles, policies, and initial service configuration.\nOverview We\u0026rsquo;ll set up:\nIAM roles for Lambda functions IAM policies for service access AWS region configuration Service quotas verification Bedrock model access Step 1: Configure AWS Region Set your default region to Singapore (ap-southeast-1):\n# Configure AWS CLI aws configure set region ap-southeast-1 # Verify configuration aws configure get region Why Singapore?\nAll required services available Good latency for Southeast Asia Bedrock Claude 3 Haiku available Step 2: Create IAM Role for Lambda Lambda functions need an execution role to access AWS services.\nCreate Trust Policy lambda-trust-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Create the Role # Create IAM role aws iam create-role \\ --role-name insighthr-lambda-role \\ --assume-role-policy-document file://lambda-trust-policy.json \\ --description \u0026#34;Execution role for InsightHR Lambda functions\u0026#34; Step 3: Create IAM Policies DynamoDB Access Policy dynamodb-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34;, \u0026#34;dynamodb:BatchGetItem\u0026#34;, \u0026#34;dynamodb:BatchWriteItem\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:dynamodb:ap-southeast-1:*:table/insighthr-*\u0026#34; ] } ] } Create the policy:\naws iam create-policy \\ --policy-name insighthr-dynamodb-policy \\ --policy-document file://dynamodb-policy.json Cognito Access Policy cognito-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cognito-idp:AdminInitiateAuth\u0026#34;, \u0026#34;cognito-idp:AdminCreateUser\u0026#34;, \u0026#34;cognito-idp:AdminSetUserPassword\u0026#34;, \u0026#34;cognito-idp:AdminGetUser\u0026#34;, \u0026#34;cognito-idp:AdminUpdateUserAttributes\u0026#34;, \u0026#34;cognito-idp:ListUsers\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:cognito-idp:ap-southeast-1:*:userpool/*\u0026#34; } ] } Create the policy:\naws iam create-policy \\ --policy-name insighthr-cognito-policy \\ --policy-document file://cognito-policy.json Bedrock Access Policy bedrock-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;bedrock:InvokeModel\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:bedrock:ap-southeast-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0\u0026#34; } ] } Create the policy:\naws iam create-policy \\ --policy-name insighthr-bedrock-policy \\ --policy-document file://bedrock-policy.json Step 4: Attach Policies to Role # Get AWS account ID ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) # Attach AWS managed policy for Lambda basic execution aws iam attach-role-policy \\ --role-name insighthr-lambda-role \\ --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole # Attach custom policies aws iam attach-role-policy \\ --role-name insighthr-lambda-role \\ --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/insighthr-dynamodb-policy aws iam attach-role-policy \\ --role-name insighthr-lambda-role \\ --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/insighthr-cognito-policy aws iam attach-role-policy \\ --role-name insighthr-lambda-role \\ --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/insighthr-bedrock-policy Step 5: Create IAM Role for API Gateway API Gateway needs a role to write logs to CloudWatch.\napi-gateway-trust-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;apigateway.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Create the role:\naws iam create-role \\ --role-name insighthr-apigateway-role \\ --assume-role-policy-document file://api-gateway-trust-policy.json # Attach CloudWatch logs policy aws iam attach-role-policy \\ --role-name insighthr-apigateway-role \\ --policy-arn arn:aws:iam::aws:policy/service-role/AmazonAPIGatewayPushToCloudWatchLogs Step 6: Enable Bedrock Model Access Request access to Claude 3 Haiku model:\nUsing AWS Console:\nNavigate to Amazon Bedrock Click \u0026ldquo;Model access\u0026rdquo; in left sidebar Click \u0026ldquo;Manage model access\u0026rdquo; Find \u0026ldquo;Claude 3 Haiku\u0026rdquo; by Anthropic Check the box Click \u0026ldquo;Request model access\u0026rdquo; Wait for approval (usually instant) Verify Access:\naws bedrock list-foundation-models \\ --region ap-southeast-1 \\ --query \u0026#39;modelSummaries[?contains(modelId, `claude-3-haiku`)].modelId\u0026#39; Step 7: Verify Service Quotas Check service limits for your account:\n# Lambda concurrent executions aws service-quotas get-service-quota \\ --service-code lambda \\ --quota-code L-B99A9384 \\ --region ap-southeast-1 # DynamoDB tables aws service-quotas get-service-quota \\ --service-code dynamodb \\ --quota-code L-F98FE922 \\ --region ap-southeast-1 # API Gateway requests per second aws service-quotas get-service-quota \\ --service-code apigateway \\ --quota-code L-8A5B8E43 \\ --region ap-southeast-1 Step 8: Create S3 Bucket for Deployment Artifacts # Create bucket for Lambda deployment packages aws s3 mb s3://insighthr-deployment-artifacts-${ACCOUNT_ID} \\ --region ap-southeast-1 # Enable versioning aws s3api put-bucket-versioning \\ --bucket insighthr-deployment-artifacts-${ACCOUNT_ID} \\ --versioning-configuration Status=Enabled Step 9: Set Up CloudWatch Log Groups Pre-create log groups for Lambda functions:\n# Create log groups LOG_GROUPS=( \u0026#34;/aws/lambda/insighthr-auth-login-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-auth-register-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-auth-google-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-employees-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-performance-scores-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-chatbot-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-attendance-handler\u0026#34; ) for log_group in \u0026#34;${LOG_GROUPS[@]}\u0026#34;; do aws logs create-log-group \\ --log-group-name $log_group \\ --region ap-southeast-1 # Set retention to 7 days aws logs put-retention-policy \\ --log-group-name $log_group \\ --retention-in-days 7 \\ --region ap-southeast-1 done Step 10: Configure Environment Variables Create a configuration file for environment variables:\nconfig.env:\n# AWS Configuration export AWS_REGION=ap-southeast-1 export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) # DynamoDB Tables export USERS_TABLE=insighthr-users-dev export EMPLOYEES_TABLE=insighthr-employees-dev export PERFORMANCE_SCORES_TABLE=insighthr-performance-scores-dev export ATTENDANCE_HISTORY_TABLE=insighthr-attendance-history-dev # Cognito (will be set after Cognito setup) export USER_POOL_ID= export CLIENT_ID= # Bedrock export BEDROCK_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0 export BEDROCK_REGION=ap-southeast-1 # Lambda Role ARN export LAMBDA_ROLE_ARN=arn:aws:iam::${AWS_ACCOUNT_ID}:role/insighthr-lambda-role Load the configuration:\nsource config.env Verification Checklist Verify your AWS environment is ready:\n# Check IAM role exists aws iam get-role --role-name insighthr-lambda-role # Check policies are attached aws iam list-attached-role-policies --role-name insighthr-lambda-role # Check Bedrock access aws bedrock list-foundation-models \\ --region ap-southeast-1 \\ --query \u0026#39;modelSummaries[?contains(modelId, `claude`)].modelId\u0026#39; # Check S3 bucket exists aws s3 ls s3://insighthr-deployment-artifacts-${ACCOUNT_ID} # Check CloudWatch log groups aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/lambda/insighthr\u0026#34; \\ --region ap-southeast-1 IAM Setup Summary Resource Purpose Policies Attached insighthr-lambda-role Lambda execution AWSLambdaBasicExecutionRole, DynamoDB, Cognito, Bedrock insighthr-apigateway-role API Gateway logging AmazonAPIGatewayPushToCloudWatchLogs insighthr-dynamodb-policy DynamoDB access Custom policy for table operations insighthr-cognito-policy Cognito access Custom policy for user management insighthr-bedrock-policy Bedrock access Custom policy for model invocation Security Best Practices ✅ Least Privilege: Policies grant only necessary permissions ✅ Resource Restrictions: Policies limited to insighthr-* resources ✅ Separate Roles: Different roles for different services ✅ CloudWatch Logging: All Lambda functions log to CloudWatch ✅ Versioning: S3 bucket versioning enabled for artifacts\nTroubleshooting IAM Role Creation Fails:\nCheck IAM permissions Verify trust policy syntax Ensure role name is unique Policy Attachment Fails:\nVerify policy ARN is correct Check role exists Ensure you have iam:AttachRolePolicy permission Bedrock Access Denied:\nRequest model access in Bedrock console Wait for approval (usually instant for Haiku) Verify region supports Bedrock Service Quota Issues:\nRequest quota increase in Service Quotas console Use different region if needed Contact AWS support for urgent increases Next Steps With the AWS environment configured, proceed to Database Setup to create DynamoDB tables.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Event 1 Event Name: AWS First Cloud Journey Community Day\nDate \u0026amp; Time: 09:00, August 30, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ho Chi Minh City\nRole: Attendee\nDescription: Presentations detailed the implementation of Retrieval-Augmented Generation (RAG) systems, showcased Multi-Agent Systems for automated processes, and emphasized building scalable AI on serverless architecture.\nOutcome: I gained technical clarity on designing Serverless RAG solutions and grasped the importance of establishing robust workflow orchestration for enterprise AI deployment.\nEvent 2 Event Name: Kick-off AWS First Cloud Journey Workforce OJT FALL 2025\nDate \u0026amp; Time: 09:00, September 06, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ho Chi Minh City\nRole: Attendee\nDescription: The session featured industry leaders from major companies who provided strategic career guidance in Cloud Computing, DevOps, and GenAI. A segment promoting diversity (\u0026ldquo;She in Tech\u0026rdquo;) and inspiring alumni success stories were key components.\nOutcome: I internalized the need for a highly adaptable and continuous learning mindset in the tech sector, and the event facilitated valuable connections with mentors and experienced alumni.\nEvent 3 Event Name: Workshop: Data Science on AWS\nDate \u0026amp; Time: 09:30, October 16, 2025\nLocation: Hall Academic – FPT University, Ho Chi Minh City\nRole: Attendee\nDescription: The Workshop provided crucial knowledge about the data pipeline, covering ingestion into Amazon S3, processing with AWS Glue (serverless ETL), and advanced model deployment using Amazon SageMaker (demonstrated via Sentiment Analysis on IMDb data).\nOutcome: This session highlighted the necessity of a unified cloud pipeline and the crucial need to balance model performance with computational costs.\nEvent 4 Event Name: AWS AI/ML \u0026amp; GenAI Workshop (Cloud Mastery Series #1)\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: Bitexco Tower, 02 Hai Trieu Street, Ho Chi Minh City\nRole: Attendee\nDescription: Content included deep dives into Amazon SageMaker for traditional ML, hands-on practice with Amazon Bedrock (Foundation Models, Agents, and Guardrails), advanced Prompt Engineering, and practical implementation of the RAG Architecture.\nOutcome: I acquired essential skills in Prompt Engineering and building accurate RAG systems, underscoring the vital role of Guardrails for compliance in production AI applications.\nEvent 5 Event Name: AWS DevOps \u0026amp; Modern Operations (Cloud Mastery Series #2)\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: Bitexco Tower, 02 Hai Trieu Street, Ho Chi Minh City\nRole: Attendee\nDescription: Key topics included DORA metrics, setting up a complete CI/CD pipeline with AWS Developer Tools, managing infrastructure via Infrastructure as Code (IaC) using CloudFormation/CDK, and container deployment options (ECS/EKS).\nOutcome: I understood the importance of Observability (CloudWatch, X-Ray) in managing microservices.\nEvent 6 Event Name: AWS Security Specialty Workshop (Cloud Mastery Series #3)\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: Bitexco Tower, 02 Hai Trieu Street, Ho Chi Minh City\nRole: Attendee\nDescription: The session covered the five core security domains: Identity (IAM), Detection (GuardDuty), Infrastructure Protection, Data Protection (KMS, Secrets Manager), and Incident Response (IR) automation strategies.\nOutcome: I learned practical, actionable techniques such as auditing IAM policies, enabling GuardDuty for continuous threat detection, and implementing Secrets Manager.\nEvent 7 Event Name: CloudThinker: Agentic AI \u0026amp; Orchestration on AWS\nDate \u0026amp; Time: 09:00, December 05, 2025\nLocation: Bitexco Tower, 02 Hai Trieu Street, Ho Chi Minh City\nRole: Attendee\nDescription: This event provided an advanced look into AWS Bedrock Agent Core, showcasing production-ready Agentic Workflows and deep-diving into Agentic Orchestration and Context Optimization.\nOutcome: I gained the technical understanding of Agentic Orchestration and learned Context Optimization strategies necessary for scaling high-accuracy, cost-efficient AI solutions that execute actions on AWS.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.4-event4/","title":"AWS AI/ML &amp; GenAI Workshop","tags":[],"description":"","content":"Event Objectives Present a market update on the AI/ML adoption and current trends within the local region. Provide hands-on training for developing end-to-end ML models using Amazon SageMaker. Conduct a technical deep dive into Generative AI capabilities on Amazon Bedrock, including its Foundation Models, Agents, and Guardrails. Provide essential skills training in Prompt Engineering and the deployment of Retrieval-Augmented Generation (RAG) systems. Speakers AWS Experts Team Key Highlights Program Introduction \u0026amp; Networking Market Insight: Shared an update on the current state of Artificial Intelligence and Machine Learning adoption across the Vietnam market. AWS AI/ML Services: The SageMaker Platform Unified ML Workflow: Detailed the complete machine learning lifecycle managed on Amazon SageMaker, covering data preparation, labeling, model training, tuning, and integration with MLOps for automated deployment. Studio Walkthrough: A live demonstration provided a clear view of the SageMaker Studio interface and its professional data science features. Generative AI Deep Dive with Amazon Bedrock Model Selection: Offered criteria and comparisons for selecting the optimal Foundation Models, such as Claude, Llama, and Titan, based on specific use case requirements. Prompt Optimization: Covered advanced Prompt Engineering techniques, including Chain-of-Thought reasoning and Few-shot learning. RAG Implementation: Demonstrated the \u0026ldquo;Retrieval -\u0026gt; Augmentation -\u0026gt; Generation\u0026rdquo; architecture, focusing on how to integrate internal Knowledge Bases to significantly improve AI response accuracy. Advanced Controls: Introduced Bedrock Agents for managing complex, multi-step tasks and Guardrails for ensuring content safety and regulatory compliance. Live Build: Successfully built a functional GenAI Chatbot prototype using Amazon Bedrock during the session. Key Takeaways Platform Strategy SageMaker\u0026rsquo;s Role: The ideal, robust platform for managing traditional, iterative Machine Learning development cycles with governance. Bedrock\u0026rsquo;s Advantage: Provides a fast, simplified API-based route to deploying Generative AI applications without the overhead of managing complex underlying infrastructure. Advanced Implementation Beyond Simple Chat: RAG and Agents are critical technologies that enable GenAI to move past basic interactions and solve complex, real-world business challenges and workflows. Compliance and Safety: Guardrails are an obligatory component for any enterprise deployment, ensuring the AI operates within defined ethical, safety, and regulatory boundaries. Applying to Work Standardize MLOps: Apply the learned SageMaker standards to automate the model lifecycle, tracking, and governance in ongoing projects. Develop RAG PoC: Conduct experiments to integrate proprietary company documents into a Bedrock Knowledge Base to build a context-aware information retrieval assistant. Improve AI Quality: Implement Chain-of-Thought prompting techniques to enhance the logical consistency and output quality of existing AI applications. Strategic Selection: Utilize the learned criteria to make informed decisions when selecting Foundation Models, balancing performance requirements against cost considerations. Event Experience This workshop provided a highly valuable balance between fundamental Machine Learning principles and contemporary Generative AI practices.\nPractical Application The SageMaker Studio walkthrough offered a clear visualization of a professional ML workflow. The Bedrock Chatbot demonstration was a highlight, showcasing the rapid development possible for powerful GenAI applications. Market Relevance The analysis of the local AI market helped frame my technical work within the broader industry trends and identified future opportunities. Some event photos "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-database-setup/","title":"Database Setup (DynamoDB)","tags":[],"description":"","content":"DynamoDB Database Setup In this section, you\u0026rsquo;ll create and configure the DynamoDB tables required for the InsightHR platform.\nOverview InsightHR uses Amazon DynamoDB, a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. We\u0026rsquo;ll create 4 main tables:\nUsers Table - Authentication and user profiles Employees Table - Employee master data Performance Scores Table - Quarterly performance tracking Attendance History Table - Check-in/check-out records Table Design Principles DynamoDB Best Practices:\nUse partition keys for even data distribution Add sort keys for range queries Create GSIs for alternate query patterns Denormalize data for read performance Use on-demand billing for variable workloads Step 1: Create Users Table Table Configuration:\nTable Name: insighthr-users-dev Partition Key: userId (String) Billing Mode: On-demand Using AWS Console:\nNavigate to DynamoDB in AWS Console Click \u0026ldquo;Create table\u0026rdquo; Enter table name: insighthr-users-dev Partition key: userId (String) Table settings: Use default settings Billing mode: On-demand Click \u0026ldquo;Create table\u0026rdquo; Add Global Secondary Index:\nSelect the table Go to \u0026ldquo;Indexes\u0026rdquo; tab Click \u0026ldquo;Create index\u0026rdquo; Index name: email-index Partition key: email (String) Projected attributes: All Click \u0026ldquo;Create index\u0026rdquo; Using AWS CLI:\n# Create table aws dynamodb create-table \\ --table-name insighthr-users-dev \\ --attribute-definitions \\ AttributeName=userId,AttributeType=S \\ AttributeName=email,AttributeType=S \\ --key-schema \\ AttributeName=userId,KeyType=HASH \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 # Create GSI aws dynamodb update-table \\ --table-name insighthr-users-dev \\ --attribute-definitions \\ AttributeName=email,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;email-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;email\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 Sample Data Structure:\n{ \u0026#34;userId\u0026#34;: \u0026#34;user-123\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;admin@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Admin User\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;Admin\u0026#34;, \u0026#34;employeeId\u0026#34;: \u0026#34;DEV-001\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34;, \u0026#34;isActive\u0026#34;: true, \u0026#34;createdAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;, \u0026#34;updatedAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34; } Step 2: Create Employees Table Table Configuration:\nTable Name: insighthr-employees-dev Partition Key: employeeId (String) Billing Mode: On-demand Using AWS CLI:\n# Create table aws dynamodb create-table \\ --table-name insighthr-employees-dev \\ --attribute-definitions \\ AttributeName=employeeId,AttributeType=S \\ AttributeName=department,AttributeType=S \\ --key-schema \\ AttributeName=employeeId,KeyType=HASH \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 # Create GSI for department queries aws dynamodb update-table \\ --table-name insighthr-employees-dev \\ --attribute-definitions \\ AttributeName=department,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;department-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;department\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 Sample Data Structure:\n{ \u0026#34;employeeId\u0026#34;: \u0026#34;DEV-001\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34;, \u0026#34;position\u0026#34;: \u0026#34;Senior\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34;, \u0026#34;hireDate\u0026#34;: \u0026#34;2024-01-01\u0026#34;, \u0026#34;managerId\u0026#34;: \u0026#34;DEV-000\u0026#34;, \u0026#34;createdAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;, \u0026#34;updatedAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34; } Step 3: Create Performance Scores Table Table Configuration:\nTable Name: insighthr-performance-scores-dev Partition Key: employeeId (String) Sort Key: period (String) Billing Mode: On-demand Using AWS CLI:\n# Create table aws dynamodb create-table \\ --table-name insighthr-performance-scores-dev \\ --attribute-definitions \\ AttributeName=employeeId,AttributeType=S \\ AttributeName=period,AttributeType=S \\ AttributeName=department,AttributeType=S \\ --key-schema \\ AttributeName=employeeId,KeyType=HASH \\ AttributeName=period,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 # Create GSI for department-period queries aws dynamodb update-table \\ --table-name insighthr-performance-scores-dev \\ --attribute-definitions \\ AttributeName=department,AttributeType=S \\ AttributeName=period,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;department-period-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;department\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;},{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;period\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;RANGE\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 Sample Data Structure:\n{ \u0026#34;scoreId\u0026#34;: \u0026#34;score-123\u0026#34;, \u0026#34;employeeId\u0026#34;: \u0026#34;DEV-001\u0026#34;, \u0026#34;period\u0026#34;: \u0026#34;2025-Q1\u0026#34;, \u0026#34;employeeName\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34;, \u0026#34;position\u0026#34;: \u0026#34;Senior\u0026#34;, \u0026#34;overallScore\u0026#34;: 85.5, \u0026#34;kpiScores\u0026#34;: { \u0026#34;KPI\u0026#34;: 85.0, \u0026#34;completed_task\u0026#34;: 88.0, \u0026#34;feedback_360\u0026#34;: 83.5 }, \u0026#34;calculatedAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;, \u0026#34;createdAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;, \u0026#34;updatedAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34; } Step 4: Create Attendance History Table Table Configuration:\nTable Name: insighthr-attendance-history-dev Partition Key: employeeId (String) Sort Key: date (String) Billing Mode: On-demand Using AWS CLI:\n# Create table aws dynamodb create-table \\ --table-name insighthr-attendance-history-dev \\ --attribute-definitions \\ AttributeName=employeeId,AttributeType=S \\ AttributeName=date,AttributeType=S \\ AttributeName=department,AttributeType=S \\ --key-schema \\ AttributeName=employeeId,KeyType=HASH \\ AttributeName=date,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 # Create GSI for date queries aws dynamodb update-table \\ --table-name insighthr-attendance-history-dev \\ --attribute-definitions \\ AttributeName=date,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;date-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;date\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 # Create GSI for department-date queries aws dynamodb update-table \\ --table-name insighthr-attendance-history-dev \\ --attribute-definitions \\ AttributeName=department,AttributeType=S \\ AttributeName=date,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;department-date-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;department\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;},{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;date\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;RANGE\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 Sample Data Structure:\n{ \u0026#34;employeeId\u0026#34;: \u0026#34;DEV-001\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2025-12-09\u0026#34;, \u0026#34;employeeName\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34;, \u0026#34;checkInTime\u0026#34;: \u0026#34;09:00:00\u0026#34;, \u0026#34;checkOutTime\u0026#34;: \u0026#34;18:00:00\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;present\u0026#34;, \u0026#34;notes\u0026#34;: \u0026#34;On time\u0026#34;, \u0026#34;createdAt\u0026#34;: \u0026#34;2025-12-09T09:00:00Z\u0026#34;, \u0026#34;updatedAt\u0026#34;: \u0026#34;2025-12-09T18:00:00Z\u0026#34; } Step 5: Verify Table Creation Using AWS Console:\nNavigate to DynamoDB Check that all 4 tables are listed Verify each table status is \u0026ldquo;Active\u0026rdquo; Check GSIs are created and active Using AWS CLI:\n# List all tables aws dynamodb list-tables --region ap-southeast-1 # Describe specific table aws dynamodb describe-table \\ --table-name insighthr-users-dev \\ --region ap-southeast-1 Step 6: Load Sample Data (Optional) For testing purposes, you can load sample data into the tables.\nCreate sample data file (sample-users.json):\n{ \u0026#34;insighthr-users-dev\u0026#34;: [ { \u0026#34;PutRequest\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;userId\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;user-001\u0026#34;}, \u0026#34;email\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;admin@example.com\u0026#34;}, \u0026#34;name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Admin User\u0026#34;}, \u0026#34;role\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Admin\u0026#34;}, \u0026#34;employeeId\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DEV-001\u0026#34;}, \u0026#34;department\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DEV\u0026#34;}, \u0026#34;isActive\u0026#34;: {\u0026#34;BOOL\u0026#34;: true}, \u0026#34;createdAt\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;}, \u0026#34;updatedAt\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;} } } } ] } Load data:\naws dynamodb batch-write-item \\ --request-items file://sample-users.json \\ --region ap-southeast-1 Database Configuration Summary Table Partition Key Sort Key GSIs Purpose insighthr-users-dev userId - email-index User authentication insighthr-employees-dev employeeId - department-index Employee data insighthr-performance-scores-dev employeeId period department-period-index Performance tracking insighthr-attendance-history-dev employeeId date date-index, department-date-index Attendance records Best Practices Implemented ✅ Partition Key Design: Even distribution of data ✅ Sort Keys: Enable range queries for time-series data ✅ GSIs: Support alternate query patterns ✅ On-Demand Billing: Cost-effective for variable workloads ✅ Naming Convention: Consistent table naming with environment suffix\nTroubleshooting Table Creation Fails:\nCheck IAM permissions for DynamoDB Verify region is correct Ensure table name doesn\u0026rsquo;t already exist GSI Creation Fails:\nWait for table to be ACTIVE before creating GSI Check attribute definitions match Verify IAM permissions High Costs:\nUse on-demand billing for development Monitor read/write capacity units Optimize query patterns Next Steps With the database tables created, proceed to Authentication Service to set up Cognito and authentication Lambda functions.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/","title":"Workshop","tags":[],"description":"","content":"InsightHR - Serverless HR Automation Platform Workshop Overview InsightHR is a modern, fully serverless HR automation platform built on AWS that demonstrates best practices for cloud-native application development. This workshop guides you through building and deploying a complete production-ready application using AWS services.\nWhat You\u0026rsquo;ll Build A comprehensive HR management system featuring:\nEmployee Management: Full CRUD operations with advanced filtering Performance Tracking: Quarterly performance scores and KPI management Attendance System: Real-time check-in/check-out with history tracking AI Chatbot: Natural language queries powered by AWS Bedrock (Claude 3) Dashboard Analytics: Interactive performance visualization Role-Based Access Control: Admin, Manager, and Employee roles Authentication: Email/password and Google OAuth via AWS Cognito Architecture Highlights ✅ 100% Serverless - No EC2 instances to manage ✅ Scalable - Auto-scales with demand ✅ Cost-Effective - Pay only for what you use ✅ Secure - Built-in security with Cognito and IAM ✅ Modern Stack - React + TypeScript + Python ✅ Production-Ready - CloudWatch monitoring and custom domain AWS Services Used Frontend: S3 + CloudFront + Route53 Backend: Lambda + API Gateway + DynamoDB Authentication: Cognito User Pools AI/ML: Amazon Bedrock (Claude 3 Haiku) Monitoring: CloudWatch + Synthetics Canaries Security: IAM + ACM (SSL Certificates) Workshop Content Workshop Overview Prerequisites Project Architecture Setup AWS Environment Database Setup (DynamoDB) Authentication Service Backend Services Frontend Development Deployment Testing \u0026amp; Monitoring Cleanup Learning Outcomes By completing this workshop, you will learn:\nHow to design and implement serverless architectures Best practices for AWS Lambda and API Gateway DynamoDB data modeling and optimization AWS Cognito authentication flows Integration with AWS Bedrock for AI capabilities CloudFront CDN configuration Infrastructure as Code principles Production deployment strategies Cost optimization techniques Prerequisites AWS Account with appropriate permissions Basic knowledge of JavaScript/TypeScript and Python Familiarity with React framework Understanding of REST APIs AWS CLI installed and configured Estimated Time Full Workshop: 4-6 hours Core Features Only: 2-3 hours Cost Estimate Running this workshop will incur minimal AWS costs:\nDynamoDB: ~$0.50/month (on-demand pricing) Lambda: Free tier covers most usage S3 + CloudFront: ~$1-2/month API Gateway: ~$0.10/month Bedrock: ~$0.0004 per query Total: ~$2-5/month for development Remember to clean up resources after completing the workshop to avoid ongoing charges.\nSupport For questions or issues during the workshop:\nCheck the troubleshooting sections in each module Review AWS documentation links provided Refer to the GitHub repository for code samples Let\u0026rsquo;s get started! 🚀\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.5-event5/","title":"AWS DevOps &amp; Modern Operations","tags":[],"description":"","content":"Event Objectives Instill a comprehensive DevOps culture and master key performance indicators (DORA metrics). Build a functional Continuous Integration/Continuous Delivery (CI/CD) pipeline using native AWS Developer Tools. Modernize infrastructure management by utilizing Infrastructure as Code (IaC) tools like CloudFormation and CDK. Explore deploying containerized applications (Docker) across various AWS compute services (ECS, EKS, App Runner). Establish best practices for a complete Observability system for complex, distributed applications. Speakers AWS Experts Team Key Highlights DevOps Culture and Automation Efficiency Metrics: Defined and explained the relevance of DORA Metrics (Lead Time, Deployment Frequency, MTTR, Change Failure Rate) for measuring operational health. Pipeline Construction: Demonstrated creating a full CI/CD workflow, integrating CodeCommit, CodeBuild, CodeDeploy, and orchestrating them with CodePipeline. Safe Deployment: Reviewed crucial deployment techniques for minimizing risk, including Blue/Green, Canary, and Rolling updates. Infrastructure as Code (IaC) Mastery CloudFormation Fundamentals: Covered defining infrastructure resources using declarative templates and utilizing features like Drift Detection. AWS CDK Advantage: Showcased using familiar programming languages (TypeScript, Python, etc.) to define infrastructure via powerful, reusable \u0026ldquo;Constructs.\u0026rdquo; IaC Strategy: Discussed the criteria for selecting the appropriate IaC tool (e.g., CloudFormation vs. CDK vs. Terraform) based on project complexity and team proficiency. Containerization and Compute Options Deployment Spectrum: Reviewed the various AWS container options, ranging from image management (ECR) to orchestration choices like simplified ECS, standardized EKS (Kubernetes), and highly abstracted App Runner. Monitoring and Observability Full-Stack Visibility: Emphasized moving beyond basic monitoring to full Observability, integrating CloudWatch (Logs, Metrics) with X-Ray (Distributed Tracing) for deeper insights into microservice performance. Operational Best Practices: Guidance on creating effective monitoring dashboards and establishing reliable on-call processes. Key Takeaways Mandatory Automation CI/CD Foundation: Adopting CI/CD is an operational imperative that reduces manual error and significantly increases release velocity. IaC Standard: Infrastructure as Code is required for ensuring configuration consistency and repeatability across all development, testing, and production environments. Operational Excellence Principles Beyond Monitoring: Observability is essential for diagnosing failures and understanding complex behavior in modern microservices architectures. Zero-Downtime Strategy: Mastering advanced deployment techniques, such as Blue/Green, is key to achieving seamless, zero-downtime releases. Applying to Work Automate Manual Processes: Transition existing manual build and deployment steps to utilize AWS CodePipeline, incorporating automated testing stages. Adopt CDK for New Projects: Begin defining infrastructure for new services using AWS CDK to ensure resources are provisioned in a programmatic and repeatable manner. Containerize \u0026amp; Pilot: Dockerize suitable application components and pilot their deployment on simple platforms like AWS App Runner. Implement Tracing: Integrate AWS X-Ray into distributed applications to gain crucial visibility into latency and communication between microservices. Event Experience This full-day event offered a structured and highly interconnected view of the modern operations landscape.\nIntegrated Learning The live demonstration of the \u0026ldquo;Full CI/CD pipeline\u0026rdquo; effectively visualized the journey of code from a developer\u0026rsquo;s repository into the production environment. Gaining clarity on the distinct use cases and trade-offs between ECS and EKS was highly valuable for future solution architecture design. Practical Takeaways The focus on various Deployment Strategies (Canary deployments, feature flags) provided immediate, practical solutions to reduce deployment risk within the team. The session reinforced the necessary connection between Code, Infrastructure, and Monitoring for achieving true operational efficiency. Some event photos "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-authentication/","title":"Authentication Service","tags":[],"description":"","content":"Authentication Service Setup This section covers setting up AWS Cognito for user authentication and implementing authentication Lambda functions.\nOverview The authentication system includes:\nAWS Cognito User Pool - User management and authentication Login Handler - Email/password authentication Registration Handler - New user signup Google OAuth Handler - Social login integration Password Reset - Password recovery workflow Step 1: Create Cognito User Pool Using AWS Console Navigate to Amazon Cognito\nClick \u0026ldquo;Create user pool\u0026rdquo;\nConfigure sign-in experience:\nSign-in options: Email User name requirements: Allow email addresses Click \u0026ldquo;Next\u0026rdquo; Configure security requirements:\nPassword policy: Cognito defaults Multi-factor authentication: Optional User account recovery: Email only Click \u0026ldquo;Next\u0026rdquo; Configure sign-up experience:\nSelf-registration: Enabled Required attributes: name, email Click \u0026ldquo;Next\u0026rdquo; Configure message delivery:\nEmail provider: Send email with Cognito FROM email address: no-reply@verificationemail.com Click \u0026ldquo;Next\u0026rdquo; Integrate your app:\nUser pool name: insighthr-user-pool App client name: insighthr-web-client Client secret: Don\u0026rsquo;t generate Authentication flows: ALLOW_USER_PASSWORD_AUTH, ALLOW_REFRESH_TOKEN_AUTH Click \u0026ldquo;Next\u0026rdquo; Review and create\nUsing AWS CLI # Create user pool aws cognito-idp create-user-pool \\ --pool-name insighthr-user-pool \\ --policies \u0026#39;{ \u0026#34;PasswordPolicy\u0026#34;: { \u0026#34;MinimumLength\u0026#34;: 8, \u0026#34;RequireUppercase\u0026#34;: true, \u0026#34;RequireLowercase\u0026#34;: true, \u0026#34;RequireNumbers\u0026#34;: true, \u0026#34;RequireSymbols\u0026#34;: false } }\u0026#39; \\ --auto-verified-attributes email \\ --username-attributes email \\ --schema \u0026#39;[ { \u0026#34;Name\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;AttributeDataType\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;Required\u0026#34;: true, \u0026#34;Mutable\u0026#34;: true }, { \u0026#34;Name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;AttributeDataType\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;Required\u0026#34;: true, \u0026#34;Mutable\u0026#34;: true } ]\u0026#39; \\ --region ap-southeast-1 # Get user pool ID USER_POOL_ID=$(aws cognito-idp list-user-pools \\ --max-results 10 \\ --query \u0026#34;UserPools[?Name==\u0026#39;insighthr-user-pool\u0026#39;].Id\u0026#34; \\ --output text \\ --region ap-southeast-1) echo \u0026#34;User Pool ID: $USER_POOL_ID\u0026#34; Create App Client # Create app client aws cognito-idp create-user-pool-client \\ --user-pool-id $USER_POOL_ID \\ --client-name insighthr-web-client \\ --no-generate-secret \\ --explicit-auth-flows ALLOW_USER_PASSWORD_AUTH ALLOW_REFRESH_TOKEN_AUTH ALLOW_USER_SRP_AUTH \\ --region ap-southeast-1 # Get client ID CLIENT_ID=$(aws cognito-idp list-user-pool-clients \\ --user-pool-id $USER_POOL_ID \\ --query \u0026#34;UserPoolClients[?ClientName==\u0026#39;insighthr-web-client\u0026#39;].ClientId\u0026#34; \\ --output text \\ --region ap-southeast-1) echo \u0026#34;Client ID: $CLIENT_ID\u0026#34; Step 2: Configure Cognito for Development For development, disable email verification requirement:\n# Update user pool to auto-verify emails aws cognito-idp update-user-pool \\ --user-pool-id $USER_POOL_ID \\ --auto-verified-attributes email \\ --region ap-southeast-1 Step 3: Create Login Lambda Function Create Function Code auth_login_handler.py:\nimport json import boto3 import os from botocore.exceptions import ClientError cognito_client = boto3.client(\u0026#39;cognito-idp\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) USER_POOL_ID = os.environ[\u0026#39;USER_POOL_ID\u0026#39;] CLIENT_ID = os.environ[\u0026#39;CLIENT_ID\u0026#39;] USERS_TABLE = os.environ[\u0026#39;DYNAMODB_USERS_TABLE\u0026#39;] def lambda_handler(event, context): try: body = json.loads(event[\u0026#39;body\u0026#39;]) email = body[\u0026#39;email\u0026#39;] password = body[\u0026#39;password\u0026#39;] # Authenticate with Cognito response = cognito_client.admin_initiate_auth( UserPoolId=USER_POOL_ID, ClientId=CLIENT_ID, AuthFlow=\u0026#39;ADMIN_NO_SRP_AUTH\u0026#39;, AuthParameters={ \u0026#39;USERNAME\u0026#39;: email, \u0026#39;PASSWORD\u0026#39;: password } ) # Get user attributes user_response = cognito_client.admin_get_user( UserPoolId=USER_POOL_ID, Username=email ) # Extract user info user_attributes = {attr[\u0026#39;Name\u0026#39;]: attr[\u0026#39;Value\u0026#39;] for attr in user_response[\u0026#39;UserAttributes\u0026#39;]} # Get user from DynamoDB table = dynamodb.Table(USERS_TABLE) db_response = table.get_item( Key={\u0026#39;userId\u0026#39;: user_attributes[\u0026#39;sub\u0026#39;]} ) user_data = db_response.get(\u0026#39;Item\u0026#39;, {}) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;tokens\u0026#39;: { \u0026#39;accessToken\u0026#39;: response[\u0026#39;AuthenticationResult\u0026#39;][\u0026#39;AccessToken\u0026#39;], \u0026#39;idToken\u0026#39;: response[\u0026#39;AuthenticationResult\u0026#39;][\u0026#39;IdToken\u0026#39;], \u0026#39;refreshToken\u0026#39;: response[\u0026#39;AuthenticationResult\u0026#39;][\u0026#39;RefreshToken\u0026#39;] }, \u0026#39;user\u0026#39;: { \u0026#39;userId\u0026#39;: user_attributes[\u0026#39;sub\u0026#39;], \u0026#39;email\u0026#39;: user_attributes[\u0026#39;email\u0026#39;], \u0026#39;name\u0026#39;: user_attributes.get(\u0026#39;name\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;role\u0026#39;: user_data.get(\u0026#39;role\u0026#39;, \u0026#39;Employee\u0026#39;), \u0026#39;department\u0026#39;: user_data.get(\u0026#39;department\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;employeeId\u0026#39;: user_data.get(\u0026#39;employeeId\u0026#39;, \u0026#39;\u0026#39;) } }) } except ClientError as e: error_code = e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] if error_code == \u0026#39;NotAuthorizedException\u0026#39;: return { \u0026#39;statusCode\u0026#39;: 401, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;Invalid credentials\u0026#39;}) } elif error_code == \u0026#39;UserNotFoundException\u0026#39;: return { \u0026#39;statusCode\u0026#39;: 404, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;User not found\u0026#39;}) } else: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } except Exception as e: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } Create requirements.txt boto3\u0026gt;=1.26.0 Package and Deploy # Create deployment package mkdir -p lambda/auth/package cd lambda/auth pip install -r requirements.txt -t package/ cd package zip -r ../auth-login-handler.zip . cd .. zip -g auth-login-handler.zip auth_login_handler.py # Deploy Lambda function aws lambda create-function \\ --function-name insighthr-auth-login-handler \\ --runtime python3.11 \\ --role arn:aws:iam::${ACCOUNT_ID}:role/insighthr-lambda-role \\ --handler auth_login_handler.lambda_handler \\ --zip-file fileb://auth-login-handler.zip \\ --timeout 30 \\ --memory-size 256 \\ --environment Variables=\u0026#34;{ USER_POOL_ID=${USER_POOL_ID}, CLIENT_ID=${CLIENT_ID}, DYNAMODB_USERS_TABLE=insighthr-users-dev }\u0026#34; \\ --region ap-southeast-1 Step 4: Create Registration Lambda Function auth_register_handler.py:\nimport json import boto3 import os import uuid from datetime import datetime from botocore.exceptions import ClientError cognito_client = boto3.client(\u0026#39;cognito-idp\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) USER_POOL_ID = os.environ[\u0026#39;USER_POOL_ID\u0026#39;] CLIENT_ID = os.environ[\u0026#39;CLIENT_ID\u0026#39;] USERS_TABLE = os.environ[\u0026#39;DYNAMODB_USERS_TABLE\u0026#39;] def lambda_handler(event, context): try: body = json.loads(event[\u0026#39;body\u0026#39;]) email = body[\u0026#39;email\u0026#39;] password = body[\u0026#39;password\u0026#39;] name = body[\u0026#39;name\u0026#39;] role = body.get(\u0026#39;role\u0026#39;, \u0026#39;Employee\u0026#39;) department = body.get(\u0026#39;department\u0026#39;, \u0026#39;\u0026#39;) employee_id = body.get(\u0026#39;employeeId\u0026#39;, \u0026#39;\u0026#39;) # Create user in Cognito cognito_response = cognito_client.sign_up( ClientId=CLIENT_ID, Username=email, Password=password, UserAttributes=[ {\u0026#39;Name\u0026#39;: \u0026#39;email\u0026#39;, \u0026#39;Value\u0026#39;: email}, {\u0026#39;Name\u0026#39;: \u0026#39;name\u0026#39;, \u0026#39;Value\u0026#39;: name} ] ) user_id = cognito_response[\u0026#39;UserSub\u0026#39;] # Auto-confirm user for development cognito_client.admin_confirm_sign_up( UserPoolId=USER_POOL_ID, Username=email ) # Store user in DynamoDB table = dynamodb.Table(USERS_TABLE) timestamp = datetime.utcnow().isoformat() + \u0026#39;Z\u0026#39; table.put_item( Item={ \u0026#39;userId\u0026#39;: user_id, \u0026#39;email\u0026#39;: email, \u0026#39;name\u0026#39;: name, \u0026#39;role\u0026#39;: role, \u0026#39;department\u0026#39;: department, \u0026#39;employeeId\u0026#39;: employee_id, \u0026#39;isActive\u0026#39;: True, \u0026#39;createdAt\u0026#39;: timestamp, \u0026#39;updatedAt\u0026#39;: timestamp } ) return { \u0026#39;statusCode\u0026#39;: 201, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;message\u0026#39;: \u0026#39;User registered successfully\u0026#39;, \u0026#39;userId\u0026#39;: user_id }) } except ClientError as e: error_code = e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] if error_code == \u0026#39;UsernameExistsException\u0026#39;: return { \u0026#39;statusCode\u0026#39;: 409, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;User already exists\u0026#39;}) } else: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } except Exception as e: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } Deploy the registration handler:\nzip -g auth-register-handler.zip auth_register_handler.py aws lambda create-function \\ --function-name insighthr-auth-register-handler \\ --runtime python3.11 \\ --role arn:aws:iam::${ACCOUNT_ID}:role/insighthr-lambda-role \\ --handler auth_register_handler.lambda_handler \\ --zip-file fileb://auth-register-handler.zip \\ --timeout 30 \\ --memory-size 256 \\ --environment Variables=\u0026#34;{ USER_POOL_ID=${USER_POOL_ID}, CLIENT_ID=${CLIENT_ID}, DYNAMODB_USERS_TABLE=insighthr-users-dev }\u0026#34; \\ --region ap-southeast-1 Step 5: Create API Gateway Endpoints Create REST API # Create API API_ID=$(aws apigateway create-rest-api \\ --name \u0026#34;InsightHR API\u0026#34; \\ --description \u0026#34;InsightHR Backend API\u0026#34; \\ --region ap-southeast-1 \\ --query \u0026#39;id\u0026#39; \\ --output text) echo \u0026#34;API ID: $API_ID\u0026#34; Create Cognito Authorizer # Create authorizer AUTHORIZER_ID=$(aws apigateway create-authorizer \\ --rest-api-id $API_ID \\ --name insighthr-cognito-authorizer \\ --type COGNITO_USER_POOLS \\ --provider-arns arn:aws:cognito-idp:ap-southeast-1:${ACCOUNT_ID}:userpool/${USER_POOL_ID} \\ --identity-source method.request.header.Authorization \\ --region ap-southeast-1 \\ --query \u0026#39;id\u0026#39; \\ --output text) echo \u0026#34;Authorizer ID: $AUTHORIZER_ID\u0026#34; Create /auth Resource # Get root resource ID ROOT_ID=$(aws apigateway get-resources \\ --rest-api-id $API_ID \\ --query \u0026#34;items[?path==\u0026#39;/\u0026#39;].id\u0026#34; \\ --output text \\ --region ap-southeast-1) # Create /auth resource AUTH_ID=$(aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $ROOT_ID \\ --path-part auth \\ --query \u0026#39;id\u0026#39; \\ --output text \\ --region ap-southeast-1) # Create /auth/login resource LOGIN_ID=$(aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $AUTH_ID \\ --path-part login \\ --query \u0026#39;id\u0026#39; \\ --output text \\ --region ap-southeast-1) # Create POST method for /auth/login aws apigateway put-method \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method POST \\ --authorization-type NONE \\ --region ap-southeast-1 # Integrate with Lambda aws apigateway put-integration \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method POST \\ --type AWS_PROXY \\ --integration-http-method POST \\ --uri arn:aws:apigateway:ap-southeast-1:lambda:path/2015-03-31/functions/arn:aws:lambda:ap-southeast-1:${ACCOUNT_ID}:function:insighthr-auth-login-handler/invocations \\ --region ap-southeast-1 # Grant API Gateway permission to invoke Lambda aws lambda add-permission \\ --function-name insighthr-auth-login-handler \\ --statement-id apigateway-invoke \\ --action lambda:InvokeFunction \\ --principal apigateway.amazonaws.com \\ --source-arn \u0026#34;arn:aws:execute-api:ap-southeast-1:${ACCOUNT_ID}:${API_ID}/*/*\u0026#34; \\ --region ap-southeast-1 Step 6: Enable CORS # Enable CORS for /auth/login aws apigateway put-method \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method OPTIONS \\ --authorization-type NONE \\ --region ap-southeast-1 aws apigateway put-integration \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method OPTIONS \\ --type MOCK \\ --request-templates \u0026#39;{\u0026#34;application/json\u0026#34;: \u0026#34;{\\\u0026#34;statusCode\\\u0026#34;: 200}\u0026#34;}\u0026#39; \\ --region ap-southeast-1 aws apigateway put-method-response \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method OPTIONS \\ --status-code 200 \\ --response-parameters \u0026#39;{ \u0026#34;method.response.header.Access-Control-Allow-Headers\u0026#34;: true, \u0026#34;method.response.header.Access-Control-Allow-Methods\u0026#34;: true, \u0026#34;method.response.header.Access-Control-Allow-Origin\u0026#34;: true }\u0026#39; \\ --region ap-southeast-1 aws apigateway put-integration-response \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method OPTIONS \\ --status-code 200 \\ --response-parameters \u0026#39;{ \u0026#34;method.response.header.Access-Control-Allow-Headers\u0026#34;: \u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;, \u0026#34;method.response.header.Access-Control-Allow-Methods\u0026#34;: \u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;POST,OPTIONS\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;, \u0026#34;method.response.header.Access-Control-Allow-Origin\u0026#34;: \u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;*\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34; }\u0026#39; \\ --region ap-southeast-1 Step 7: Deploy API # Create deployment aws apigateway create-deployment \\ --rest-api-id $API_ID \\ --stage-name dev \\ --description \u0026#34;Initial deployment\u0026#34; \\ --region ap-southeast-1 # Get API endpoint echo \u0026#34;API Endpoint: https://${API_ID}.execute-api.ap-southeast-1.amazonaws.com/dev\u0026#34; Step 8: Test Authentication Test Registration curl -X POST \\ https://${API_ID}.execute-api.ap-southeast-1.amazonaws.com/dev/auth/register \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;test@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;Test123!\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Test User\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;Employee\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34; }\u0026#39; Test Login curl -X POST \\ https://${API_ID}.execute-api.ap-southeast-1.amazonaws.com/dev/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;test@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;Test123!\u0026#34; }\u0026#39; Authentication Flow Summary 1. User submits credentials → Frontend 2. Frontend calls /auth/login → API Gateway 3. API Gateway routes to Lambda → auth-login-handler 4. Lambda authenticates with Cognito → Cognito User Pool 5. Cognito returns JWT tokens → Lambda 6. Lambda retrieves user data → DynamoDB 7. Lambda returns tokens + user data → Frontend 8. Frontend stores tokens → localStorage 9. Subsequent requests include JWT → Authorization header Security Best Practices ✅ Password Policy: Strong password requirements ✅ JWT Tokens: Short-lived access tokens ✅ Refresh Tokens: Long-lived for token renewal ✅ HTTPS Only: All communication encrypted ✅ CORS: Properly configured for frontend domain ✅ No Secrets: Client doesn\u0026rsquo;t use client secret\nTroubleshooting Cognito Authentication Fails:\nVerify user pool ID and client ID Check password meets requirements Ensure user is confirmed Lambda Permission Denied:\nCheck IAM role has Cognito permissions Verify Lambda execution role attached CORS Errors:\nEnable CORS on API Gateway Check Access-Control-Allow-Origin header Verify OPTIONS method configured Next Steps With authentication configured, proceed to Backend Services to implement employee, performance, and chatbot APIs.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.6-event6/","title":"AWS Security Specialty Workshop","tags":[],"description":"","content":"Event Objectives Develop a thorough understanding of the Security Pillar within the AWS Well-Architected Framework. Gain mastery over the five security domains: Identity, Detection, Infrastructure Protection, Data Protection, and Incident Response. Receive an update on the most prevalent security threats facing the cloud market locally. Participate in practical exercises focusing on auditing access permissions (IAM) and creating structured Incident Response (IR) Playbooks. Speakers AWS Security Experts Team Key Highlights Foundation \u0026amp; Identity (Pillar 1) Core Security Principles: Reaffirmed the critical importance of applying Least Privilege, Zero Trust, and Defense in Depth methodologies. Modern Identity Management: Emphasized the shift from using long-term credentials (IAM Users) to utilizing IAM Roles and centralizing identity via AWS Identity Center (SSO). Governing Permissions: Learned to use Service Control Policies (SCPs) and Permission Boundaries to define and restrict maximum permission scopes in complex, multi-account environments. Practical Validation: Conducted a brief exercise on Validating IAM Policies to proactively identify and resolve potential security flaws. Detection \u0026amp; Infrastructure (Pillar 2 \u0026amp; 3) Continuous Threat Detection: Focused on enabling key services like CloudTrail (for organizational-level auditing), GuardDuty (for intelligent threat detection), and Security Hub (for consolidated compliance monitoring). Comprehensive Logging: Discussed best practices for capturing logs at every layer, including VPC Flow Logs (network traffic), ALB logs (application load balancing), and S3 access logs (data storage). Network Perimeter Defense: Strategies for VPC segmentation combined with Security Groups and NACLs. Integrated edge protection utilizing WAF, Shield, and Network Firewall was also covered. Data Protection \u0026amp; Incident Response (Pillar 4 \u0026amp; 5) Encryption Strategy: Detailed the necessary steps for encrypting data both in-transit and at-rest across core services (S3, EBS, RDS) using KMS (Key Management Service). Credential Hygiene: Stressed the need to eliminate hard-coded credentials by leveraging Secrets Manager and Parameter Store, including automated rotation mechanisms. Automated IR: The process of building structured IR Playbooks for common threats (e.g., compromised credentials, malware) and automating the response and isolation steps using services like Lambda and Step Functions. Key Takeaways The Zero Trust Model Identity as the Perimeter: In a cloud-native architecture, the security boundary shifts from the network edge to Identity, making it the most critical defense layer. The principle is clear: never trust any user or resource by default; always verify all access requests. Automation for Scale Manual security practices are unsustainable at cloud scale and speed. Implementing Detection-as-Code and Auto-remediation is mandatory to reduce the window of exposure to threats. Applying to Work IAM Policy Audit: Initiate a full audit of all IAM Users and their permissions, deprecating old keys and moving toward application-based IAM Roles. Enable Threat Intelligence: Activate GuardDuty globally across all organizational accounts/regions to benefit from continuous threat and anomaly detection. Secrets Management Adoption: Refactor applications to retrieve database credentials from Secrets Manager instead of storing them in configuration files. IR Readiness: Begin developing a formal Incident Response Playbook specifically for a \u0026ldquo;Compromised IAM Key\u0026rdquo; scenario, including a plan for rehearsing the automated response. Event Experience This workshop was a deep, technical dive into cloud security, providing the comprehensive knowledge required of a modern Cloud Engineer.\nStructured Learning Organizing the curriculum around the 5 Pillars successfully transformed scattered security concepts into a single, cohesive, and standard framework. The insight provided on Top threats specific to the local market context was immediately practical for risk mitigation. Practical Skill Development Demos, particularly those using the IAM Policy Validator and Access Analyzer, provided immediately useful skills for daily policy debugging and security hardening. The focus on Incident Response automation underscored the shift from mere monitoring to proactive, automated system defense. Some event photos "},{"uri":"https://thienluhoan.github.io/workshop-template/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at AWS FCJ Workforce from September 7, 2025 to November 25, 2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment. I participated in the 3-month intensive training program and completed multiple side projects/workshops focused on modern cloud architecture, through which I improved my skills in AWS architecture design, DevOps (CI/CD, IaC), Generative AI (RAG, Prompt Engineering), and Data Science/MLOps. In terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ☐ ✅ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ✅ ☐ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ☐ ✅ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ✅ ☐ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Improving team workflow scheduling and task prioritization for efficient project delivery. Optimizing the learning curve and knowledge acquisition speed for mastering new cloud services and features. Strengthening analytical and root-cause problem-solving skills when debugging complex architectural issues. Enhancing technical documentation consistency and maintainability for clearer project handovers. Deepening expertise in cloud cost optimization and resource governance within AWS architectures. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.7-backend-services/","title":"Backend Services","tags":[],"description":"","content":"Backend Services Implementation This section covers implementing the core backend Lambda functions for employee management, performance tracking, attendance, and AI chatbot.\nOverview We\u0026rsquo;ll implement four main services:\nEmployee Service - CRUD operations for employee records Performance Scores Service - Quarterly performance tracking Attendance Service - Check-in/check-out management Chatbot Service - AI-powered natural language queries Employee Service The employee service handles all employee-related operations with department and position filtering.\nKey Features:\nCRUD operations (Create, Read, Update, Delete) Department filtering (DEV, QA, DAT, SEC, AI) Position filtering (Junior, Mid, Senior, Lead, Manager) Status filtering (active, inactive) Search by name or employee ID Bulk import from CSV API Endpoints:\nGET /employees - List all employees GET /employees/{id} - Get employee by ID POST /employees - Create new employee PUT /employees/{id} - Update employee DELETE /employees/{id} - Delete employee POST /employees/bulk - Bulk import from CSV Performance Scores Service Manages quarterly performance scores with automatic calculation.\nKey Features:\nAutomatic score calculation (average of KPI, completed_task, feedback_360) Department and period filtering Role-based data access (Admin: all, Manager: department, Employee: own) Denormalized employee data for performance Bulk import from CSV API Endpoints:\nGET /performance-scores - List all scores GET /performance-scores/{id}/{period} - Get specific score POST /performance-scores - Create score PUT /performance-scores/{id}/{period} - Update score DELETE /performance-scores/{id}/{period} - Delete score POST /performance-scores/bulk - Bulk import Attendance Service Tracks employee attendance with check-in/check-out functionality.\nKey Features:\nReal-time check-in/check-out Date range filtering Department filtering Status tracking (present, absent, late, half-day) Bulk operations API Endpoints:\nGET /attendance - List attendance records GET /attendance/{id}/{date} - Get specific record POST /attendance/check-in - Check in POST /attendance/check-out - Check out PUT /attendance/{id}/{date} - Update record DELETE /attendance/{id}/{date} - Delete record Chatbot Service AI-powered chatbot using AWS Bedrock (Claude 3 Haiku).\nKey Features:\nAWS Bedrock integration (Claude 3 Haiku) Natural language query processing Context building from DynamoDB Role-based data filtering Cost-effective ($0.0004 per query) Supported Queries:\nEmployee information (\u0026ldquo;Who is DEV-001?\u0026rdquo;) Performance queries (\u0026ldquo;What\u0026rsquo;s the average score for Q1 2025?\u0026rdquo;) Department statistics (\u0026ldquo;Compare DEV and QA performance\u0026rdquo;) Trend analysis (\u0026ldquo;Performance trends over quarters\u0026rdquo;) API Endpoint:\nPOST /chatbot/query Implementation Pattern All services follow this pattern:\nRequest Validation - Validate input parameters Authorization - Check user permissions based on role DynamoDB Operations - Query or update data Response Formatting - Return standardized JSON response Error Handling - Catch and return appropriate errors Deployment Deploy all backend services using the Lambda deployment scripts provided in the workshop materials. Each service is packaged with its dependencies and deployed to AWS Lambda with appropriate environment variables.\nTesting Test each service using the provided test scripts or curl commands. Verify:\nCRUD operations work correctly Filtering and search functionality Role-based access control Error handling Performance and response times Next Steps With backend services implemented, proceed to Frontend Development to build the React application.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.7-event7/","title":"CloudThinker: Agentic AI &amp; Orchestration on AWS","tags":[],"description":"","content":"Event Objectives Conduct a deep-dive into the AWS Bedrock Agent Core and its functional capabilities. Explore practical use cases for developing complex Agentic Workflows. Analyze advanced technical concepts (L300) such as Agentic Orchestration and Context Optimization. Provide practical experience through the CloudThinker Hack workshop. Speakers Mr. Nguyen Gia Hung – Head of Solutions Architect, AWS Mr. Kien Nguyen – Solutions Architect, AWS Mr. Viet Pham – Founder \u0026amp; CEO Mr. Thang Ton – Co-founder \u0026amp; COO, CloudThinker Mr. Henry Bui – Head of Engineering, CloudThinker Mr. Kha Van – Workshop Facilitator Key Highlights 1. AWS Foundation Program Context: Mr. Nguyen Gia Hung introduced the strategic importance of Agentic AI within the modern cloud environment. Bedrock Agent Core Overview: Mr. Kien Nguyen provided a technical breakdown of how AWS Bedrock Agent Core simplifies the creation of agents that can plan tasks and execute them by invoking external APIs. 2. Practical Application \u0026amp; Use Cases Workflow Development: Mr. Viet Pham demonstrated the full lifecycle of designing and deploying a concrete agentic workflow on AWS. Ecosystem Introduction: Mr. Thang Ton presented the CloudThinker platform and its vision for leveraging AI in cloud solutions. 3. Deep Dive (Level 300) Technical Core: Mr. Henry Bui led the advanced session, focusing on intricate strategies for coordinating multiple agents (Agentic Orchestration) and fine-tuning prompts and context within Amazon Bedrock (Context Optimization) for high accuracy and cost efficiency in complex interactions. 4. Hands-on Experience CloudThinker Hack: Mr. Kha Van facilitated a 60-minute practical session where attendees prototyped an agent using the CloudThinker framework and various AWS services, reinforcing the theoretical lessons. Key Takeaways Evolution of AI From Chat to Action: The industry is evolving from simple, passive chatbots to active Agents capable of complex orchestration and performing actions via API calls. Cost \u0026amp; Accuracy: As AI complexity grows, sophisticated Context Optimization is vital to maintain high accuracy while efficiently managing token usage and associated costs. Architecture Orchestration Patterns: Building reliable multi-agent systems requires a robust orchestration layer to strategically manage the flow of user requests and distribute tasks among specialized agents. Applying to Work Agent Prototyping: I plan to use AWS Bedrock Agent to develop a simple internal tool that can execute an action via a company API (e.g., check server status or query an internal ledger). Research Context Management: I will study the advanced context optimization patterns shared to improve the efficiency and performance of our existing RAG implementations. Continuous Learning: Encourage the development team to participate in similar practical workshops and hackathons to stay current with the latest AWS innovations. Event Experience This event was highly technical and focused.\nTechnical Depth: The L300 discussion on Agentic Orchestration was particularly insightful, providing the architectural knowledge needed to scale complex AI solutions beyond simple demonstrations. Reinforcement: The \u0026ldquo;CloudThinker Hack\u0026rdquo; was highly effective, offering immediate, practical reinforcement of the technical concepts learned in the morning sessions. Some event photos "},{"uri":"https://thienluhoan.github.io/workshop-template/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment The working environment is very friendly, open, and kind. I quickly felt welcome and had the opportunity to make a lot of friends among my fellow interns.\n2. Support from Mentor / Team Admin The support from my mentors and the admin team was exceptional. My mentors were friendly, nice, and dedicated, providing very detailed guidance and teaching me a lot of critical cloud knowledge.\n3. Relevance of Work to Academic Major The tasks I was assigned align very well with my major in Artificial Intelligence (AI), particularly with the focus on Generative AI, MLOps, and Data Science on AWS. This allowed me to both strengthen my foundational knowledge from university and gain practical skills in new, cutting-edge areas I had never encountered before, like implementing RAG and CI/CD pipelines.\n4. Learning \u0026amp; Skill Development Opportunities The program provided immense opportunities for growth. I learned many new professional skills such as using project management tools and professional communication in a corporate environment.\n5. Company Culture \u0026amp; Team Spirit They\u0026rsquo;re fun and friendly. Also they are quick to answer questions when given, and to give guidance when it\u0026rsquo;s needed the most.\n6. Internship Policies / Benefits The company provides large working area with good conditions and a very flexible schedule for internship.\nAdditional Questions What did you find most satisfying during your internship? - Good cold air conditioning, very clean toilet, cleaner than my university\u0026rsquo;s.\nWhat do you think the company should improve for future interns? - Clearer rules and requirements.\nIf recommending to a friend, would you suggest they intern here? Why or why not? - Yeah I think I would. They\u0026rsquo;ll surely learn a lot about cloud computing like me.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience?\n- The mentor should give out candy more I like the candies the most.\nWould you like to continue this program in the future? - Highly likely, I\u0026rsquo;d like to become a Cloud Engineer.\nAny other comments (free sharing): - The buffet\u0026rsquo;s chicken is always cold.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.8-frontend/","title":"Frontend Development","tags":[],"description":"","content":"Frontend Development This section covers building the React frontend application for InsightHR.\nOverview The frontend is a Single Page Application (SPA) built with:\nReact 18 with TypeScript Vite 7.2 for fast builds Tailwind CSS 3.4 for styling (Frutiger Aero theme) Zustand 5.0 for state management React Hook Form + Zod for form validation Recharts 3.4 for data visualization React Router v7 for routing Project Structure insighthr-web/ ├── src/ │ ├── components/ │ │ ├── auth/ # Login, Register, ProtectedRoute │ │ ├── admin/ # User/Employee/Score Management │ │ ├── dashboard/ # Charts, Tables, Filters │ │ ├── attendance/ # Check-in/out, History │ │ ├── chatbot/ # AI Chat Interface │ │ ├── profile/ # User Profile │ │ ├── common/ # Reusable UI components │ │ └── layout/ # Header, Sidebar, Footer │ ├── pages/ # Page components │ ├── services/ # API service layer │ ├── store/ # Zustand state stores │ ├── types/ # TypeScript type definitions │ ├── utils/ # Utility functions │ └── styles/ # Global styles and theme ├── public/ # Static assets └── package.json # Dependencies Key Components Authentication Components LoginForm: Email/password and Google OAuth login RegisterForm: New user registration ProtectedRoute: Route guard for authenticated users ChangePasswordModal: Password change dialog Admin Panel Components UserManagement: User CRUD with role assignment EmployeeManagement: Employee CRUD with filters PerformanceScoreManagement: Score management with bulk import PasswordRequestsPanel: Password reset request handling Dashboard Components PerformanceDashboard: Main analytics dashboard BarChart: Department performance comparison LineChart: Trend analysis over time PieChart: Score distribution DataTable: Sortable, filterable data grid FilterPanel: Date range and department filters ExportButton: CSV/Excel export Chatbot Components MessageList: Conversation history display MessageInput: User input with send button TypingIndicator: Loading animation ChatbotInstructions: Usage guide Attendance Components CheckInCheckOut: Quick check-in/out buttons AttendanceManagement: Full attendance interface AttendanceCalendarView: Calendar visualization AttendanceRecordsList: History table State Management Using Zustand for global state:\n// Auth Store interface AuthState { user: User | null; tokens: Tokens | null; isAuthenticated: boolean; login: (email: string, password: string) =\u0026gt; Promise\u0026lt;void\u0026gt;; logout: () =\u0026gt; void; } // Employee Store interface EmployeeState { employees: Employee[]; loading: boolean; error: string | null; fetchEmployees: (filters?: Filters) =\u0026gt; Promise\u0026lt;void\u0026gt;; createEmployee: (data: EmployeeInput) =\u0026gt; Promise\u0026lt;void\u0026gt;; } API Integration All API calls go through a centralized service layer with axios interceptors for authentication:\nconst api = axios.create({ baseURL: import.meta.env.VITE_API_BASE_URL, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; } }); // Add auth token to all requests api.interceptors.request.use((config) =\u0026gt; { const token = localStorage.getItem(\u0026#39;idToken\u0026#39;); if (token) { config.headers.Authorization = `Bearer ${token}`; } return config; }); Routing React Router v7 handles navigation:\nconst router = createBrowserRouter([ { path: \u0026#39;/login\u0026#39;, element: \u0026lt;LoginPage /\u0026gt; }, { path: \u0026#39;/register\u0026#39;, element: \u0026lt;RegisterForm /\u0026gt; }, { path: \u0026#39;/\u0026#39;, element: \u0026lt;ProtectedRoute\u0026gt;\u0026lt;Layout /\u0026gt;\u0026lt;/ProtectedRoute\u0026gt;, children: [ { path: \u0026#39;/\u0026#39;, element: \u0026lt;DashboardPage /\u0026gt; }, { path: \u0026#39;/admin\u0026#39;, element: \u0026lt;AdminPage /\u0026gt; }, { path: \u0026#39;/employees\u0026#39;, element: \u0026lt;EmployeesPage /\u0026gt; }, { path: \u0026#39;/performance-scores\u0026#39;, element: \u0026lt;PerformanceScoresPage /\u0026gt; }, { path: \u0026#39;/attendance\u0026#39;, element: \u0026lt;AttendancePage /\u0026gt; }, { path: \u0026#39;/chatbot\u0026#39;, element: \u0026lt;ChatbotPage /\u0026gt; }, { path: \u0026#39;/profile\u0026#39;, element: \u0026lt;ProfilePage /\u0026gt; } ] } ]); Environment Configuration Create .env file:\nVITE_API_BASE_URL=https://your-api-id.execute-api.ap-southeast-1.amazonaws.com/dev VITE_GOOGLE_CLIENT_ID=your-google-client-id VITE_AWS_REGION=ap-southeast-1 VITE_COGNITO_USER_POOL_ID=your-user-pool-id VITE_COGNITO_CLIENT_ID=your-client-id Development # Install dependencies npm install # Start development server npm run dev # Build for production npm run build # Preview production build npm run preview Styling Tailwind CSS with custom Frutiger Aero theme provides a modern, clean interface with:\nGradient backgrounds Glassmorphism effects Smooth animations Responsive design Accessible components Testing The frontend includes:\nComponent unit tests Integration tests for user flows E2E tests with Playwright Accessibility testing Next Steps With the frontend built, proceed to Deployment to deploy the complete application to AWS.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.9-deployment/","title":"Deployment","tags":[],"description":"","content":"Deploying InsightHR to Production This section covers deploying the complete InsightHR application to AWS, including frontend deployment to S3/CloudFront and backend Lambda functions.\nDeployment Overview The deployment process consists of:\nFrontend Deployment: Build and deploy React app to S3 CloudFront Configuration: Set up CDN for global distribution Lambda Deployment: Deploy all backend functions API Gateway Configuration: Set up REST API endpoints Environment Configuration: Configure environment variables DNS Setup (Optional): Configure custom domain Prerequisites Before deploying, ensure you have:\n✅ All DynamoDB tables created ✅ Cognito User Pool configured ✅ IAM roles and policies set up ✅ AWS CLI configured ✅ Node.js and npm installed ✅ Python 3.11+ installed Step 1: Frontend Deployment Build the React Application Navigate to the frontend directory and build the production bundle:\ncd insighthr-web # Install dependencies npm install # Build production bundle npm run build This creates an optimized production build in the dist/ directory.\nBuild Output:\ndist/ ├── index.html ├── assets/ │ ├── index-[hash].js │ ├── index-[hash].css │ └── [other assets] └── [other files] Create S3 Bucket Create an S3 bucket for hosting the static website:\n# Create bucket aws s3 mb s3://insighthr-web-app-sg --region ap-southeast-1 # Enable static website hosting aws s3 website s3://insighthr-web-app-sg \\ --index-document index.html \\ --error-document index.html Configure Bucket Policy Create a bucket policy to allow CloudFront access:\nbucket-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::insighthr-web-app-sg/*\u0026#34; } ] } Apply the policy:\naws s3api put-bucket-policy \\ --bucket insighthr-web-app-sg \\ --policy file://bucket-policy.json Upload Files to S3 # Sync dist folder to S3 aws s3 sync dist/ s3://insighthr-web-app-sg \\ --region ap-southeast-1 \\ --delete # Verify upload aws s3 ls s3://insighthr-web-app-sg --recursive Step 2: CloudFront Configuration Create CloudFront Distribution Using AWS Console:\nNavigate to CloudFront\nClick \u0026ldquo;Create Distribution\u0026rdquo;\nConfigure origin:\nOrigin domain: insighthr-web-app-sg.s3.ap-southeast-1.amazonaws.com Origin path: (leave empty) Name: S3-insighthr-web-app Default cache behavior:\nViewer protocol policy: Redirect HTTP to HTTPS Allowed HTTP methods: GET, HEAD, OPTIONS Cache policy: CachingOptimized Settings:\nPrice class: Use all edge locations Alternate domain names (CNAMEs): insight-hr.io.vn, www.insight-hr.io.vn Custom SSL certificate: Request or import certificate Default root object: index.html Click \u0026ldquo;Create Distribution\u0026rdquo;\nUsing AWS CLI:\n# Create distribution configuration cat \u0026gt; cloudfront-config.json \u0026lt;\u0026lt; EOF { \u0026#34;CallerReference\u0026#34;: \u0026#34;insighthr-$(date +%s)\u0026#34;, \u0026#34;Comment\u0026#34;: \u0026#34;InsightHR CloudFront Distribution\u0026#34;, \u0026#34;DefaultRootObject\u0026#34;: \u0026#34;index.html\u0026#34;, \u0026#34;Origins\u0026#34;: { \u0026#34;Quantity\u0026#34;: 1, \u0026#34;Items\u0026#34;: [ { \u0026#34;Id\u0026#34;: \u0026#34;S3-insighthr-web-app\u0026#34;, \u0026#34;DomainName\u0026#34;: \u0026#34;insighthr-web-app-sg.s3.ap-southeast-1.amazonaws.com\u0026#34;, \u0026#34;S3OriginConfig\u0026#34;: { \u0026#34;OriginAccessIdentity\u0026#34;: \u0026#34;\u0026#34; } } ] }, \u0026#34;DefaultCacheBehavior\u0026#34;: { \u0026#34;TargetOriginId\u0026#34;: \u0026#34;S3-insighthr-web-app\u0026#34;, \u0026#34;ViewerProtocolPolicy\u0026#34;: \u0026#34;redirect-to-https\u0026#34;, \u0026#34;AllowedMethods\u0026#34;: { \u0026#34;Quantity\u0026#34;: 3, \u0026#34;Items\u0026#34;: [\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;, \u0026#34;OPTIONS\u0026#34;] }, \u0026#34;ForwardedValues\u0026#34;: { \u0026#34;QueryString\u0026#34;: false, \u0026#34;Cookies\u0026#34;: {\u0026#34;Forward\u0026#34;: \u0026#34;none\u0026#34;} }, \u0026#34;MinTTL\u0026#34;: 0, \u0026#34;DefaultTTL\u0026#34;: 86400, \u0026#34;MaxTTL\u0026#34;: 31536000 }, \u0026#34;Enabled\u0026#34;: true } EOF # Create distribution aws cloudfront create-distribution \\ --distribution-config file://cloudfront-config.json Configure Custom Error Pages Set up error pages for SPA routing:\nGo to CloudFront distribution\nNavigate to \u0026ldquo;Error Pages\u0026rdquo; tab\nCreate custom error response:\nHTTP error code: 403 Customize error response: Yes Response page path: /index.html HTTP response code: 200 Repeat for error code 404\nStep 3: Lambda Deployment Package Lambda Functions For each Lambda function, create a deployment package:\nExample: Auth Login Handler\ncd lambda/auth # Create deployment package mkdir -p package pip install -r requirements.txt -t package/ cd package zip -r ../auth-login-handler.zip . cd .. zip -g auth-login-handler.zip auth_login_handler.py # Deploy to Lambda aws lambda create-function \\ --function-name insighthr-auth-login-handler \\ --runtime python3.11 \\ --role arn:aws:iam::ACCOUNT_ID:role/insighthr-lambda-role \\ --handler auth_login_handler.lambda_handler \\ --zip-file fileb://auth-login-handler.zip \\ --timeout 30 \\ --memory-size 256 \\ --environment Variables=\u0026#34;{ USER_POOL_ID=ap-southeast-1_rzDtdAhvp, CLIENT_ID=6suhk5huhe40o6iuqgsnmuucj5, DYNAMODB_USERS_TABLE=insighthr-users-dev }\u0026#34; \\ --region ap-southeast-1 Deploy All Lambda Functions Create a deployment script:\ndeploy-all-lambdas.sh:\n#!/bin/bash FUNCTIONS=( \u0026#34;auth-login-handler\u0026#34; \u0026#34;auth-register-handler\u0026#34; \u0026#34;auth-google-handler\u0026#34; \u0026#34;employees-handler\u0026#34; \u0026#34;employees-bulk-handler\u0026#34; \u0026#34;performance-scores-handler\u0026#34; \u0026#34;chatbot-handler\u0026#34; \u0026#34;attendance-handler\u0026#34; ) for func in \u0026#34;${FUNCTIONS[@]}\u0026#34;; do echo \u0026#34;Deploying $func...\u0026#34; # Package and deploy logic here done Step 4: API Gateway Configuration Create REST API # Create API aws apigateway create-rest-api \\ --name \u0026#34;InsightHR API\u0026#34; \\ --description \u0026#34;InsightHR Backend API\u0026#34; \\ --region ap-southeast-1 Create Resources and Methods Example: Create /employees endpoint\n# Get API ID API_ID=$(aws apigateway get-rest-apis \\ --query \u0026#34;items[?name==\u0026#39;InsightHR API\u0026#39;].id\u0026#34; \\ --output text) # Get root resource ID ROOT_ID=$(aws apigateway get-resources \\ --rest-api-id $API_ID \\ --query \u0026#34;items[?path==\u0026#39;/\u0026#39;].id\u0026#34; \\ --output text) # Create /employees resource EMPLOYEES_ID=$(aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $ROOT_ID \\ --path-part employees \\ --query \u0026#39;id\u0026#39; \\ --output text) # Create GET method aws apigateway put-method \\ --rest-api-id $API_ID \\ --resource-id $EMPLOYEES_ID \\ --http-method GET \\ --authorization-type COGNITO_USER_POOLS \\ --authorizer-id $AUTHORIZER_ID # Integrate with Lambda aws apigateway put-integration \\ --rest-api-id $API_ID \\ --resource-id $EMPLOYEES_ID \\ --http-method GET \\ --type AWS_PROXY \\ --integration-http-method POST \\ --uri arn:aws:apigateway:ap-southeast-1:lambda:path/2015-03-31/functions/arn:aws:lambda:ap-southeast-1:ACCOUNT_ID:function:insighthr-employees-handler/invocations Deploy API # Create deployment aws apigateway create-deployment \\ --rest-api-id $API_ID \\ --stage-name dev \\ --description \u0026#34;Initial deployment\u0026#34; # Get API endpoint echo \u0026#34;API Endpoint: https://$API_ID.execute-api.ap-southeast-1.amazonaws.com/dev\u0026#34; Step 5: Environment Configuration Frontend Environment Variables Create .env.production file:\nVITE_API_BASE_URL=https://lqk4t6qzag.execute-api.ap-southeast-1.amazonaws.com/dev VITE_GOOGLE_CLIENT_ID=your-google-client-id VITE_AWS_REGION=ap-southeast-1 VITE_COGNITO_USER_POOL_ID=ap-southeast-1_rzDtdAhvp VITE_COGNITO_CLIENT_ID=6suhk5huhe40o6iuqgsnmuucj5 Rebuild and redeploy frontend:\nnpm run build aws s3 sync dist/ s3://insighthr-web-app-sg --delete Lambda Environment Variables Update Lambda functions with environment variables:\naws lambda update-function-configuration \\ --function-name insighthr-employees-handler \\ --environment Variables=\u0026#34;{ AWS_REGION=ap-southeast-1, EMPLOYEES_TABLE=insighthr-employees-dev, USERS_TABLE=insighthr-users-dev }\u0026#34; Step 6: CloudFront Cache Invalidation After deploying frontend changes, invalidate CloudFront cache:\n# Get distribution ID DIST_ID=$(aws cloudfront list-distributions \\ --query \u0026#34;DistributionList.Items[?Comment==\u0026#39;InsightHR CloudFront Distribution\u0026#39;].Id\u0026#34; \\ --output text) # Create invalidation aws cloudfront create-invalidation \\ --distribution-id $DIST_ID \\ --paths \u0026#34;/*\u0026#34; Step 7: DNS Configuration (Optional) If using a custom domain:\nRequest SSL Certificate # Request certificate in us-east-1 (required for CloudFront) aws acm request-certificate \\ --domain-name insight-hr.io.vn \\ --subject-alternative-names www.insight-hr.io.vn \\ --validation-method DNS \\ --region us-east-1 Configure Route53 # Create hosted zone aws route53 create-hosted-zone \\ --name insight-hr.io.vn \\ --caller-reference $(date +%s) # Create A record for CloudFront cat \u0026gt; change-batch.json \u0026lt;\u0026lt; EOF { \u0026#34;Changes\u0026#34;: [{ \u0026#34;Action\u0026#34;: \u0026#34;CREATE\u0026#34;, \u0026#34;ResourceRecordSet\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;insight-hr.io.vn\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;AliasTarget\u0026#34;: { \u0026#34;HostedZoneId\u0026#34;: \u0026#34;Z2FDTNDATAQYW2\u0026#34;, \u0026#34;DNSName\u0026#34;: \u0026#34;d2z6tht6rq32uy.cloudfront.net\u0026#34;, \u0026#34;EvaluateTargetHealth\u0026#34;: false } } }] } EOF aws route53 change-resource-record-sets \\ --hosted-zone-id ZONE_ID \\ --change-batch file://change-batch.json Deployment Checklist Before going live, verify:\nAll Lambda functions deployed and tested API Gateway endpoints configured DynamoDB tables populated with initial data Cognito User Pool configured Frontend built and deployed to S3 CloudFront distribution active SSL certificate validated (if using custom domain) DNS records configured (if using custom domain) Environment variables set correctly CORS configured on API Gateway CloudWatch logs enabled IAM roles and permissions verified Testing Deployment Test Frontend # Access CloudFront URL curl -I https://d2z6tht6rq32uy.cloudfront.net # Or custom domain curl -I https://insight-hr.io.vn Test API Endpoints # Test health check curl https://lqk4t6qzag.execute-api.ap-southeast-1.amazonaws.com/dev/health # Test authenticated endpoint (with token) curl -H \u0026#34;Authorization: Bearer YOUR_JWT_TOKEN\u0026#34; \\ https://lqk4t6qzag.execute-api.ap-southeast-1.amazonaws.com/dev/employees Continuous Deployment For automated deployments, consider:\nGitHub Actions: Automate build and deploy on push AWS CodePipeline: Full CI/CD pipeline AWS Amplify: Simplified frontend deployment Troubleshooting Frontend not loading:\nCheck S3 bucket policy Verify CloudFront distribution status Check browser console for errors API errors:\nVerify Lambda function logs in CloudWatch Check API Gateway configuration Verify Cognito authorizer setup CORS issues:\nConfigure CORS on API Gateway Check allowed origins match frontend domain Next Steps With deployment complete, proceed to Testing \u0026amp; Monitoring to set up CloudWatch monitoring and synthetic canaries.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.10-testing/","title":"Testing &amp; Monitoring","tags":[],"description":"","content":"Testing \u0026amp; Monitoring This section covers setting up comprehensive testing and monitoring for the InsightHR platform.\nOverview We\u0026rsquo;ll implement:\nCloudWatch Logs - Centralized logging CloudWatch Metrics - Performance monitoring CloudWatch Alarms - Automated alerting CloudWatch Synthetics - Automated testing X-Ray Tracing - Distributed tracing (optional) CloudWatch Logs All Lambda functions automatically log to CloudWatch Logs.\nView Logs:\n# List log groups aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/lambda/insighthr\u0026#34; \\ --region ap-southeast-1 # Tail logs for a function aws logs tail /aws/lambda/insighthr-auth-login-handler \\ --follow \\ --region ap-southeast-1 Log Insights Queries:\nFind errors:\nfields @timestamp, @message | filter @message like /ERROR/ | sort @timestamp desc | limit 20 Analyze response times:\nfields @timestamp, @duration | stats avg(@duration), max(@duration), min(@duration) CloudWatch Metrics Monitor key metrics:\nLambda Metrics:\nInvocations Duration Errors Throttles Concurrent executions DynamoDB Metrics:\nRead/Write capacity units Throttled requests System errors API Gateway Metrics:\nRequest count Latency 4XX/5XX errors View Metrics:\n# Lambda invocations aws cloudwatch get-metric-statistics \\ --namespace AWS/Lambda \\ --metric-name Invocations \\ --dimensions Name=FunctionName,Value=insighthr-auth-login-handler \\ --start-time 2025-12-09T00:00:00Z \\ --end-time 2025-12-09T23:59:59Z \\ --period 3600 \\ --statistics Sum \\ --region ap-southeast-1 CloudWatch Alarms Create alarms for critical metrics:\nHigh Error Rate Alarm:\naws cloudwatch put-metric-alarm \\ --alarm-name insighthr-high-error-rate \\ --alarm-description \u0026#34;Alert when Lambda error rate exceeds 5%\u0026#34; \\ --metric-name Errors \\ --namespace AWS/Lambda \\ --statistic Sum \\ --period 300 \\ --evaluation-periods 2 \\ --threshold 5 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=FunctionName,Value=insighthr-auth-login-handler \\ --region ap-southeast-1 High Latency Alarm:\naws cloudwatch put-metric-alarm \\ --alarm-name insighthr-high-latency \\ --alarm-description \u0026#34;Alert when API latency exceeds 2 seconds\u0026#34; \\ --metric-name Latency \\ --namespace AWS/ApiGateway \\ --statistic Average \\ --period 300 \\ --evaluation-periods 2 \\ --threshold 2000 \\ --comparison-operator GreaterThanThreshold \\ --region ap-southeast-1 CloudWatch Synthetics Canaries Automated testing with synthetic canaries:\n1. Login Canary - Tests login flow 2. Dashboard Canary - Tests dashboard loading 3. Chatbot Canary - Tests chatbot queries 4. Autoscoring Canary - Tests performance calculations\nCreate Login Canary:\n// login-canary.js const synthetics = require(\u0026#39;Synthetics\u0026#39;); const log = require(\u0026#39;SyntheticsLogger\u0026#39;); const loginFlow = async function () { const page = await synthetics.getPage(); // Navigate to login page await page.goto(\u0026#39;https://insight-hr.io.vn/login\u0026#39;, { waitUntil: \u0026#39;domcontentloaded\u0026#39;, timeout: 30000 }); // Fill login form await page.type(\u0026#39;#email\u0026#39;, \u0026#39;test@example.com\u0026#39;); await page.type(\u0026#39;#password\u0026#39;, \u0026#39;Test123!\u0026#39;); // Click login button await Promise.all([ page.waitForNavigation(), page.click(\u0026#39;button[type=\u0026#34;submit\u0026#34;]\u0026#39;) ]); // Verify successful login await page.waitForSelector(\u0026#39;.dashboard\u0026#39;, { timeout: 10000 }); log.info(\u0026#39;Login flow completed successfully\u0026#39;); }; exports.handler = async () =\u0026gt; { return await synthetics.executeStep(\u0026#39;LoginFlow\u0026#39;, loginFlow); }; Deploy Canary:\n# Create S3 bucket for canary artifacts aws s3 mb s3://insighthr-canary-artifacts --region ap-southeast-1 # Create canary aws synthetics create-canary \\ --name insighthr-login-canary \\ --artifact-s3-location s3://insighthr-canary-artifacts \\ --execution-role-arn arn:aws:iam::${ACCOUNT_ID}:role/CloudWatchSyntheticsRole \\ --schedule Expression=\u0026#34;rate(5 minutes)\u0026#34; \\ --runtime-version syn-nodejs-puppeteer-6.2 \\ --code file://login-canary.zip \\ --region ap-southeast-1 Dashboard Setup Create a CloudWatch Dashboard:\naws cloudwatch put-dashboard \\ --dashboard-name InsightHR-Dashboard \\ --dashboard-body file://dashboard-config.json \\ --region ap-southeast-1 dashboard-config.json:\n{ \u0026#34;widgets\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/Lambda\u0026#34;, \u0026#34;Invocations\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;Errors\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;Duration\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34;}] ], \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Lambda Metrics\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/ApiGateway\u0026#34;, \u0026#34;Count\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;4XXError\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;5XXError\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}] ], \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;API Gateway Metrics\u0026#34; } } ] } Testing Strategy Unit Tests:\nTest individual Lambda functions Mock DynamoDB and Cognito calls Verify business logic Integration Tests:\nTest API endpoints end-to-end Verify data flow through services Test authentication and authorization Load Tests:\nUse Artillery or k6 for load testing Test concurrent user scenarios Identify performance bottlenecks Example Load Test:\n# load-test.yml config: target: \u0026#39;https://your-api-id.execute-api.ap-southeast-1.amazonaws.com/dev\u0026#39; phases: - duration: 60 arrivalRate: 10 scenarios: - name: \u0026#34;Login and fetch employees\u0026#34; flow: - post: url: \u0026#34;/auth/login\u0026#34; json: email: \u0026#34;test@example.com\u0026#34; password: \u0026#34;Test123!\u0026#34; - get: url: \u0026#34;/employees\u0026#34; Best Practices ✅ Structured Logging: Use JSON format for logs ✅ Correlation IDs: Track requests across services ✅ Error Tracking: Log errors with context ✅ Performance Monitoring: Track response times ✅ Automated Testing: Run canaries regularly ✅ Alerting: Set up alarms for critical metrics ✅ Cost Monitoring: Track AWS costs\nTroubleshooting High Error Rates:\nCheck CloudWatch Logs for error details Verify IAM permissions Check DynamoDB capacity Review recent code changes High Latency:\nOptimize DynamoDB queries Add caching where appropriate Review Lambda memory allocation Check cold start times Failed Canaries:\nReview canary logs Check application availability Verify test credentials Update canary scripts if UI changed Monitoring Checklist CloudWatch Logs configured for all Lambda functions Key metrics tracked (invocations, errors, duration) Alarms set up for critical thresholds Synthetics canaries deployed and running Dashboard created for visualization Log retention policies configured Cost alerts configured On-call rotation established (for production) Next Steps With monitoring in place, proceed to Cleanup when you\u0026rsquo;re done with the workshop to avoid ongoing charges.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.11-cleanup/","title":"Cleanup","tags":[],"description":"","content":"Cleaning Up Resources To avoid ongoing charges, it\u0026rsquo;s important to delete all AWS resources created during this workshop. This section provides step-by-step instructions for cleaning up.\nImportant: Deleting resources is irreversible. Make sure you\u0026rsquo;ve backed up any data you want to keep before proceeding.\nCleanup Overview We\u0026rsquo;ll delete resources in the following order:\nCloudFront Distribution S3 Buckets API Gateway Lambda Functions DynamoDB Tables Cognito User Pool CloudWatch Logs IAM Roles and Policies Route53 (if configured) ACM Certificates (if created) Step 1: Delete CloudFront Distribution CloudFront distributions must be disabled before deletion.\nUsing AWS Console:\nNavigate to CloudFront Select the InsightHR distribution Click \u0026ldquo;Disable\u0026rdquo; Wait for status to change to \u0026ldquo;Disabled\u0026rdquo; (may take 15-20 minutes) Select the distribution again Click \u0026ldquo;Delete\u0026rdquo; Using AWS CLI:\n# Get distribution ID DIST_ID=$(aws cloudfront list-distributions \\ --query \u0026#34;DistributionList.Items[?Comment==\u0026#39;InsightHR CloudFront Distribution\u0026#39;].Id\u0026#34; \\ --output text) # Get ETag ETAG=$(aws cloudfront get-distribution --id $DIST_ID \\ --query \u0026#39;ETag\u0026#39; --output text) # Disable distribution aws cloudfront get-distribution-config --id $DIST_ID \u0026gt; dist-config.json # Edit dist-config.json: Set \u0026#34;Enabled\u0026#34;: false aws cloudfront update-distribution \\ --id $DIST_ID \\ --if-match $ETAG \\ --distribution-config file://dist-config.json # Wait for distribution to be disabled aws cloudfront wait distribution-deployed --id $DIST_ID # Delete distribution ETAG=$(aws cloudfront get-distribution --id $DIST_ID \\ --query \u0026#39;ETag\u0026#39; --output text) aws cloudfront delete-distribution --id $DIST_ID --if-match $ETAG Step 2: Delete S3 Buckets Empty and delete the web app bucket:\n# Empty bucket aws s3 rm s3://insighthr-web-app-sg --recursive # Delete bucket aws s3 rb s3://insighthr-web-app-sg --force Delete canary artifacts bucket (if created):\naws s3 rm s3://insighthr-canary-artifacts --recursive aws s3 rb s3://insighthr-canary-artifacts --force Step 3: Delete API Gateway Using AWS Console:\nNavigate to API Gateway Select \u0026ldquo;InsightHR API\u0026rdquo; Click \u0026ldquo;Actions\u0026rdquo; → \u0026ldquo;Delete API\u0026rdquo; Confirm deletion Using AWS CLI:\n# Get API ID API_ID=$(aws apigateway get-rest-apis \\ --query \u0026#34;items[?name==\u0026#39;InsightHR API\u0026#39;].id\u0026#34; \\ --output text \\ --region ap-southeast-1) # Delete API aws apigateway delete-rest-api \\ --rest-api-id $API_ID \\ --region ap-southeast-1 Step 4: Delete Lambda Functions Delete all Lambda functions:\n# List of functions to delete FUNCTIONS=( \u0026#34;insighthr-auth-login-handler\u0026#34; \u0026#34;insighthr-auth-register-handler\u0026#34; \u0026#34;insighthr-auth-google-handler\u0026#34; \u0026#34;insighthr-employees-handler\u0026#34; \u0026#34;insighthr-employees-bulk-handler\u0026#34; \u0026#34;insighthr-performance-scores-handler\u0026#34; \u0026#34;insighthr-chatbot-handler\u0026#34; \u0026#34;insighthr-attendance-handler\u0026#34; ) # Delete each function for func in \u0026#34;${FUNCTIONS[@]}\u0026#34;; do echo \u0026#34;Deleting $func...\u0026#34; aws lambda delete-function \\ --function-name $func \\ --region ap-southeast-1 done Step 5: Delete DynamoDB Tables Using AWS Console:\nNavigate to DynamoDB Select each table Click \u0026ldquo;Delete\u0026rdquo; Confirm by typing \u0026ldquo;delete\u0026rdquo; Using AWS CLI:\n# List of tables to delete TABLES=( \u0026#34;insighthr-users-dev\u0026#34; \u0026#34;insighthr-employees-dev\u0026#34; \u0026#34;insighthr-performance-scores-dev\u0026#34; \u0026#34;insighthr-attendance-history-dev\u0026#34; \u0026#34;insighthr-password-reset-requests-dev\u0026#34; \u0026#34;insighthr-kpis-dev\u0026#34; \u0026#34;insighthr-formulas-dev\u0026#34; \u0026#34;insighthr-data-tables-dev\u0026#34; \u0026#34;insighthr-notification-rules-dev\u0026#34; ) # Delete each table for table in \u0026#34;${TABLES[@]}\u0026#34;; do echo \u0026#34;Deleting $table...\u0026#34; aws dynamodb delete-table \\ --table-name $table \\ --region ap-southeast-1 done Step 6: Delete Cognito User Pool Using AWS Console:\nNavigate to Cognito Select \u0026ldquo;User Pools\u0026rdquo; Select the InsightHR user pool Click \u0026ldquo;Delete user pool\u0026rdquo; Type the pool name to confirm Using AWS CLI:\n# Get user pool ID USER_POOL_ID=$(aws cognito-idp list-user-pools \\ --max-results 10 \\ --query \u0026#34;UserPools[?Name==\u0026#39;insighthr-user-pool\u0026#39;].Id\u0026#34; \\ --output text \\ --region ap-southeast-1) # Delete user pool aws cognito-idp delete-user-pool \\ --user-pool-id $USER_POOL_ID \\ --region ap-southeast-1 Step 7: Delete CloudWatch Logs Delete log groups:\n# List log groups aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/lambda/insighthr\u0026#34; \\ --region ap-southeast-1 # Delete each log group LOG_GROUPS=$(aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/lambda/insighthr\u0026#34; \\ --query \u0026#39;logGroups[*].logGroupName\u0026#39; \\ --output text \\ --region ap-southeast-1) for log_group in $LOG_GROUPS; do echo \u0026#34;Deleting $log_group...\u0026#34; aws logs delete-log-group \\ --log-group-name $log_group \\ --region ap-southeast-1 done Delete CloudWatch Synthetics Canaries (if created):\n# List canaries aws synthetics describe-canaries \\ --region ap-southeast-1 # Delete each canary CANARIES=$(aws synthetics describe-canaries \\ --query \u0026#39;Canaries[?starts_with(Name, `insighthr`)].Name\u0026#39; \\ --output text \\ --region ap-southeast-1) for canary in $CANARIES; do echo \u0026#34;Deleting canary $canary...\u0026#34; aws synthetics delete-canary \\ --name $canary \\ --region ap-southeast-1 done Step 8: Delete IAM Roles and Policies Using AWS Console:\nNavigate to IAM Go to \u0026ldquo;Roles\u0026rdquo; Search for \u0026ldquo;insighthr\u0026rdquo; Select each role Detach policies Delete role Using AWS CLI:\n# List roles aws iam list-roles \\ --query \u0026#39;Roles[?starts_with(RoleName, `insighthr`)].RoleName\u0026#39; \\ --output text # For each role, detach policies and delete ROLES=$(aws iam list-roles \\ --query \u0026#39;Roles[?starts_with(RoleName, `insighthr`)].RoleName\u0026#39; \\ --output text) for role in $ROLES; do echo \u0026#34;Processing role $role...\u0026#34; # Detach managed policies POLICIES=$(aws iam list-attached-role-policies \\ --role-name $role \\ --query \u0026#39;AttachedPolicies[*].PolicyArn\u0026#39; \\ --output text) for policy in $POLICIES; do aws iam detach-role-policy \\ --role-name $role \\ --policy-arn $policy done # Delete inline policies INLINE_POLICIES=$(aws iam list-role-policies \\ --role-name $role \\ --query \u0026#39;PolicyNames[*]\u0026#39; \\ --output text) for policy in $INLINE_POLICIES; do aws iam delete-role-policy \\ --role-name $role \\ --policy-name $policy done # Delete role aws iam delete-role --role-name $role done Delete custom policies:\n# List custom policies aws iam list-policies \\ --scope Local \\ --query \u0026#39;Policies[?starts_with(PolicyName, `insighthr`)].Arn\u0026#39; \\ --output text # Delete each policy POLICIES=$(aws iam list-policies \\ --scope Local \\ --query \u0026#39;Policies[?starts_with(PolicyName, `insighthr`)].Arn\u0026#39; \\ --output text) for policy in $POLICIES; do echo \u0026#34;Deleting policy $policy...\u0026#34; # Delete all policy versions except default VERSIONS=$(aws iam list-policy-versions \\ --policy-arn $policy \\ --query \u0026#39;Versions[?!IsDefaultVersion].VersionId\u0026#39; \\ --output text) for version in $VERSIONS; do aws iam delete-policy-version \\ --policy-arn $policy \\ --version-id $version done # Delete policy aws iam delete-policy --policy-arn $policy done Step 9: Delete Route53 Resources (If Configured) Delete DNS records:\n# Get hosted zone ID ZONE_ID=$(aws route53 list-hosted-zones \\ --query \u0026#34;HostedZones[?Name==\u0026#39;insight-hr.io.vn.\u0026#39;].Id\u0026#34; \\ --output text) # List and delete records (except NS and SOA) aws route53 list-resource-record-sets \\ --hosted-zone-id $ZONE_ID # Delete A records for CloudFront cat \u0026gt; delete-records.json \u0026lt;\u0026lt; EOF { \u0026#34;Changes\u0026#34;: [{ \u0026#34;Action\u0026#34;: \u0026#34;DELETE\u0026#34;, \u0026#34;ResourceRecordSet\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;insight-hr.io.vn\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;AliasTarget\u0026#34;: { \u0026#34;HostedZoneId\u0026#34;: \u0026#34;Z2FDTNDATAQYW2\u0026#34;, \u0026#34;DNSName\u0026#34;: \u0026#34;d2z6tht6rq32uy.cloudfront.net\u0026#34;, \u0026#34;EvaluateTargetHealth\u0026#34;: false } } }] } EOF aws route53 change-resource-record-sets \\ --hosted-zone-id $ZONE_ID \\ --change-batch file://delete-records.json # Delete hosted zone aws route53 delete-hosted-zone --id $ZONE_ID Step 10: Delete ACM Certificates (If Created) Using AWS Console:\nNavigate to Certificate Manager (in us-east-1 region) Select the certificate Click \u0026ldquo;Delete\u0026rdquo; Using AWS CLI:\n# List certificates aws acm list-certificates \\ --region us-east-1 # Delete certificate CERT_ARN=$(aws acm list-certificates \\ --query \u0026#34;CertificateSummaryList[?DomainName==\u0026#39;insight-hr.io.vn\u0026#39;].CertificateArn\u0026#34; \\ --output text \\ --region us-east-1) aws acm delete-certificate \\ --certificate-arn $CERT_ARN \\ --region us-east-1 Automated Cleanup Script Create a comprehensive cleanup script:\ncleanup-all.sh:\n#!/bin/bash set -e echo \u0026#34;Starting InsightHR cleanup...\u0026#34; # 1. Disable and delete CloudFront echo \u0026#34;Step 1: CloudFront...\u0026#34; # (Add CloudFront cleanup commands) # 2. Delete S3 buckets echo \u0026#34;Step 2: S3 Buckets...\u0026#34; aws s3 rm s3://insighthr-web-app-sg --recursive aws s3 rb s3://insighthr-web-app-sg --force # 3. Delete API Gateway echo \u0026#34;Step 3: API Gateway...\u0026#34; # (Add API Gateway cleanup commands) # 4. Delete Lambda functions echo \u0026#34;Step 4: Lambda Functions...\u0026#34; # (Add Lambda cleanup commands) # 5. Delete DynamoDB tables echo \u0026#34;Step 5: DynamoDB Tables...\u0026#34; # (Add DynamoDB cleanup commands) # 6. Delete Cognito echo \u0026#34;Step 6: Cognito User Pool...\u0026#34; # (Add Cognito cleanup commands) # 7. Delete CloudWatch logs echo \u0026#34;Step 7: CloudWatch Logs...\u0026#34; # (Add CloudWatch cleanup commands) # 8. Delete IAM roles echo \u0026#34;Step 8: IAM Roles and Policies...\u0026#34; # (Add IAM cleanup commands) echo \u0026#34;Cleanup complete!\u0026#34; Verification After cleanup, verify all resources are deleted:\n# Check S3 buckets aws s3 ls | grep insighthr # Check Lambda functions aws lambda list-functions \\ --query \u0026#39;Functions[?starts_with(FunctionName, `insighthr`)].FunctionName\u0026#39; \\ --region ap-southeast-1 # Check DynamoDB tables aws dynamodb list-tables \\ --query \u0026#39;TableNames[?starts_with(@, `insighthr`)]\u0026#39; \\ --region ap-southeast-1 # Check CloudFront distributions aws cloudfront list-distributions \\ --query \u0026#34;DistributionList.Items[?Comment==\u0026#39;InsightHR CloudFront Distribution\u0026#39;].Id\u0026#34; # Check API Gateway aws apigateway get-rest-apis \\ --query \u0026#34;items[?name==\u0026#39;InsightHR API\u0026#39;].id\u0026#34; \\ --region ap-southeast-1 Cost Verification After cleanup, monitor your AWS billing:\nGo to AWS Billing Dashboard Check \u0026ldquo;Bills\u0026rdquo; for current month Verify no ongoing charges for deleted services Check \u0026ldquo;Cost Explorer\u0026rdquo; for trends Cleanup Checklist CloudFront distribution disabled and deleted S3 buckets emptied and deleted API Gateway deleted All Lambda functions deleted All DynamoDB tables deleted Cognito User Pool deleted CloudWatch log groups deleted CloudWatch Synthetics canaries deleted IAM roles and policies deleted Route53 hosted zone deleted (if created) ACM certificates deleted (if created) Billing dashboard checked for ongoing charges Troubleshooting Cleanup CloudFront won\u0026rsquo;t delete:\nEnsure distribution is fully disabled Wait 15-20 minutes after disabling Check for associated resources S3 bucket won\u0026rsquo;t delete:\nEnsure bucket is completely empty Check for versioned objects Disable versioning before deleting IAM role won\u0026rsquo;t delete:\nDetach all managed policies first Delete all inline policies Check for service-linked roles DynamoDB table deletion fails:\nWait for table to be in ACTIVE state Check for ongoing operations Verify IAM permissions Final Notes Best Practice: Always clean up resources after completing a workshop to avoid unexpected charges. Set up billing alerts to notify you of any ongoing costs.\nCongratulations! You\u0026rsquo;ve successfully completed the InsightHR workshop and cleaned up all resources. You\u0026rsquo;ve learned:\n✅ Serverless architecture design ✅ AWS Lambda and API Gateway ✅ DynamoDB data modeling ✅ Cognito authentication ✅ AWS Bedrock AI integration ✅ CloudFront CDN deployment ✅ Infrastructure as Code principles ✅ Cost optimization strategies Thank you for participating in this workshop!\n"},{"uri":"https://thienluhoan.github.io/workshop-template/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://thienluhoan.github.io/workshop-template/tags/","title":"Tags","tags":[],"description":"","content":""}]